{"docstore/metadata": {"38253cc8-12dd-4b35-8f2a-b9f2e9c40c69": {"doc_hash": "41acafbd57109a1524a7f0a59b2b00437ff802f58d9e100c61d60c4d20e9a422"}, "66db197a-f5f5-4026-96c4-a651c6c1ca97": {"doc_hash": "e7f0e661dd61eb0e5a6e38c758b581c9230ac6717d939638ddc9104ae113726a"}, "4f2b6468-fca3-4e90-a794-75fbd0ccfa6f": {"doc_hash": "d4f70bd51ab9daee4a5b2652d295b080fdac3d720b383021c9f1dad5247fac5c"}, "a802dfb1-a085-4e15-84e5-a84ddae7da05": {"doc_hash": "cace5c24cbd35e1e745213e116e14f6b5f2abf1b3b172f9d1fd46e3b8cd71f13"}, "ff022234-d71d-46ad-956b-0a001e38d97a": {"doc_hash": "fe4b35ef9dc0f409fcb29c4b38914566b04c92d4c3023988ee3206a6eb09ee56"}, "6ef09ca3-d0d7-4d56-904b-77675ada0e6c": {"doc_hash": "360e9eb10a04bdc75ce22222575bf038918b74ce18164b8bdb7a1cf8aceccd0a"}, "e34fe21a-43b6-4938-a0da-94ce8e6fe887": {"doc_hash": "ef051310bd2a66bc6f27c9d8908ba7804134f25036a68b7ccccf92e7fa66dff0"}, "7c5eec50-2964-4601-ad30-745f2dadf028": {"doc_hash": "a0fd2042eca7ef63831b40f43c23a6222d2870f09edcd0aae1f1b712bbf4dc2e"}, "7385d65d-a81a-4ee3-ab0d-801ee406e901": {"doc_hash": "11469c9b63f6b1fbe6f23ab9ed7f63920e6511ea08ff3cc4015bea32e04d016c"}, "d3c395ca-e222-4561-ab7a-274b7cb5d76d": {"doc_hash": "d3ce912d9d58cac8e946c939598ffb61a3c73f00c1d91f0737909d7a76b9cee7"}, "35d71bcd-d63b-4bd6-9b22-72c0dd3093ae": {"doc_hash": "d1836592af2222d2f3493290942b64fa792bbefda43b0444b49929fc785db324"}, "f66c3f8b-0a4d-4cfd-a4b8-f323ab565cec": {"doc_hash": "51720e99b3d2a89a60599ec086aaf403e002facfc46929b16b85fb9c08ac6eee"}, "6c9c9fbe-b13d-40e4-b964-83cc4cdffdbe": {"doc_hash": "7e1a92993e001c0c90a2be04ca4e50ec0ea1f7a25334dc535f9b7698f061eb56"}, "658b3e2b-bd12-4119-9520-c8386c71a9c9": {"doc_hash": "fdede87fbeebb677d1dddedec8993a4746bc1e2c61312b11ae46d688eadd0cc6"}, "9e70d970-f163-4450-a05b-569fd6cda981": {"doc_hash": "d606ecbe8a24b27a797bbac84e1236667b3de5c33b2df5636ae268c48048114e"}, "0352b9e5-6c5f-4e76-8f61-8bed337644a8": {"doc_hash": "53dbea82b98557eb08dc620b5ae4e3384a52d5bcde7643f69485509e99ce6826"}, "f4024f7c-c873-4d85-9473-f4a3020695b4": {"doc_hash": "6cba803e3b32b7a892e966ee891181e18e58d95e66ed78d24179ce7b7aae810c"}, "529319ff-9d54-42a7-8896-5b102213bf49": {"doc_hash": "6922b816ca54549450e1f7a66257b5965c22b95edb0e184bf0c75747bc357fe9"}, "1da7cd2f-71b1-4b5c-be5c-7494f7c964d6": {"doc_hash": "92250e2742349fe52f810211aae79a2c342dd0e4f9ad0656a6d828a93522cd55"}, "6cc5d296-aa67-4c8e-849d-31c835174760": {"doc_hash": "98dd7f93dfd3792da4d3b849f9799304eac2a57f082d84a3d6e3faae54b5242b"}, "591d6c26-7b4a-46cf-a55e-e0a3853009b0": {"doc_hash": "cb27cbbc743c985099738c6e060f3fa01a681cb7419f8b65c1c3d3ff26e806dc"}, "90b3f1ff-bfc5-4c52-aecf-6719d5d37e5d": {"doc_hash": "aa697e530e9d51a9956fbd1548c8dac253111878d3c8eb4741d65f3896529a37"}, "f270085a-4b2d-4cd3-a38d-94e567ba30ba": {"doc_hash": "497a81cb40ae1efef258e92b1d55990357d4fed7e10d53e63e0f32731e3b9d4b"}, "33f29cd2-df22-4a07-964c-35e7dffb43bd": {"doc_hash": "2d49e5c562c87c082e8dcd053d1537963754c73a068bddfa098e9860700bf6b1"}, "64bed31e-7c59-4b52-b7c9-3d8eb6171e69": {"doc_hash": "02efc94b5df8c8f1fe9f63556885e2102c3a738fee0d6d96a19e7087231c4c48"}, "6b30c2db-168b-4d8b-b7ea-82a8f4f27eca": {"doc_hash": "d30037dc8e1cbc0481435627ba49b39e0475e6ab6c22e5c0c3b961f73d68a66c"}, "d013c5ef-a363-4978-8dbe-b07fe972bf3f": {"doc_hash": "78247e9ca2174cd9893e00acfa08b2bbc266a12ed8e7233b37383c00844e9133"}, "768dec39-fe48-4c1d-a18a-e7f054e32739": {"doc_hash": "5d546cbf3a1beba0824ee3915e787ca4a581a84b28dc7979f5eb6e070c7c1827"}, "2473d46c-0ccc-4fc6-ae14-ea0a15e51b6b": {"doc_hash": "afa488ad6e97574f87bb67b3802b01315e1ebd5e4cbf87cd328ef7c17abfd3ea"}, "65409667-83da-4b4c-8725-3c8393996070": {"doc_hash": "db29abf1cd48762b90fbc0f137ff3cec85075c51c7c9b3bc45eeeedbbe61e0f2"}, "1a43a6eb-1ccb-4acc-bc84-6e65487ec16a": {"doc_hash": "19d0e6b6668f8e2df3435668e72d81af7766cd30bb43eba4026841c5c3a5a16c"}, "2f9fa58d-29eb-4248-b6cb-52465ee67e91": {"doc_hash": "2821bb0bcfe6f290c0e290cbf0dca14242cccade32643b28b2945da097d38b56"}, "93cdd84e-6b13-4405-adcc-134add3cbe11": {"doc_hash": "cf26736eb97f66588b4b968e3b6ba5a8d75489f962449f2982060f6abb4d911e"}, "a2a18fb6-3415-4745-9f4b-b263304d32d4": {"doc_hash": "93bd99c7eaef9b3c01e5f03d59d44fa9c5faa626a9d845296a9ceacb6a657995"}, "1eee1785-8b22-4586-bd15-a49a1e1c88cb": {"doc_hash": "d664f1bd323a09f511d43b052931ca440349c5313ab52106f45c7af3233b6e57"}, "fee773e4-ee3a-4447-bc94-b604cf9231b9": {"doc_hash": "44b29f2d54962f0df650f38ce70f0b9e3bdd80cfb4bad9908375efc4d28291b4"}, "295d2839-53d5-44dc-a40b-4fdcff61c08d": {"doc_hash": "6e6b6d67611c6c503db8f49125ca86e4f529847a998fb6a82c277812277ce60f"}, "05861ae7-c95f-4e12-aa50-ec6f2863cfa7": {"doc_hash": "948005c5160e89e345fa7209781b967cf8b3a92a47d0c4865f493ab84f4a094e"}, "d4dbac9a-42a7-4e29-8602-358df8e42956": {"doc_hash": "b8c2a68f76cd4073f82c04ef553904000ed6666bd1b64b65df54c9ecfa38208e"}, "1b35a746-2334-4af3-b700-9e7451d7e037": {"doc_hash": "4807ca8889f732a8aa62c267babc13a72019deed926030c892dff34edf99a458"}, "c4650e72-2e50-44fa-8278-a4e922d08f99": {"doc_hash": "95a7decef3be7043c8675572aede2b359afdc8385e1f76fca96db67a16126d20"}, "69c3976a-7737-4d6e-a73b-2ee14795a4d6": {"doc_hash": "6b01349a8a599348ca9a6c4dd08c0092424162afec08028744263d04384db4cc"}, "bb70198e-5d74-49ba-9662-c7cb492efe26": {"doc_hash": "2cdb788e4f904d79698e558b62023d602434cf1caa0ad36bd81948fcc49c7a65"}, "036cc20e-6afc-45ea-95aa-e2dc7a370631": {"doc_hash": "c873ed4ea17b29b71707b9f71bbf33c65f813f7de9224536fc24e769d4cbf7c2"}, "ca0e92d5-47c1-4dc2-b585-920412c51542": {"doc_hash": "ba09750dffd7ff47ef3961e2a4d1dfa537b57f2da3f5fa0b928d34cff404c91a"}, "aff38af1-2b57-4a15-9ad4-916f9a964e9e": {"doc_hash": "9f87e54b97bd6d7a3f920c76f724dd40cecdce92289e44109efcaec31d0d5b3e"}, "170a97eb-81bd-449d-be31-fdb5631aef90": {"doc_hash": "a7b91ffb760718b16231c894241de8404bf2cd515f85c235fbc764e84a29ea15"}, "aec0222c-6689-4c17-843a-09c2360240a6": {"doc_hash": "d7d668ebadd5fb37a85796f95e99f3531215ee725abaffffc1374301b3688014"}, "6f6dae7e-5f00-4deb-958f-07a241f3289b": {"doc_hash": "03a483c5fd3e4df925d0ee70afce1d8d59ffc7f2cab2661550c0d984749ee462"}, "822deb9f-11f3-49cc-9315-71846cfc6731": {"doc_hash": "59ab949be7a51f6fcff2d6b40af3cfc2c1ce53993a12516265ee0e09d65fb9ac"}, "04fb9861-1f52-4be8-ada0-e4da2c5dd23c": {"doc_hash": "6c9ccf6bd24b21c0cd88554b466f7e2d66116e136f5c832c3a231c7de35fbef6"}, "8e97222e-ee39-45c2-b060-6e5ca5995f56": {"doc_hash": "a0e56bd98834c2075c8c73bd60ad098c363be3308d3086c1079a4e28a8b3721f"}, "41f7e5a5-5ced-458c-8ef5-498839cafa82": {"doc_hash": "9da70c1c3cfa61b9b83f542426ef4fb931318e30eb911c6b7dbe1b4b6efbda20", "ref_doc_id": "38253cc8-12dd-4b35-8f2a-b9f2e9c40c69"}, "36bf1abd-a5cd-4473-8ef3-63a224053636": {"doc_hash": "33b92c6e385815ee2957f6ef73f208d9ff5767c467765230b306a8908b787c53", "ref_doc_id": "66db197a-f5f5-4026-96c4-a651c6c1ca97"}, "e67d7290-6bf9-49e2-8451-74d2ca337e1c": {"doc_hash": "582f9b409d35299012d033868597aa5d2c208ce45026e2a02d3f9cb7a5367c91", "ref_doc_id": "4f2b6468-fca3-4e90-a794-75fbd0ccfa6f"}, "b72d55ca-44aa-4650-914f-3ec67ad30e38": {"doc_hash": "f86e075c4805beda4c520c0fc9f6679b9995d40830b2640be9e1c45c1200f6b8", "ref_doc_id": "4f2b6468-fca3-4e90-a794-75fbd0ccfa6f"}, "fe3f1d63-9531-4c4f-967a-fb91d26d77e5": {"doc_hash": "871a4b3fb42bae3349398b4f2df08ebc86214a3c9d299106386e1d282b8787be", "ref_doc_id": "4f2b6468-fca3-4e90-a794-75fbd0ccfa6f"}, "a2aa4a30-0e28-411e-9ccb-e5fc09bfde24": {"doc_hash": "0c14e3f68135911b40dbe67cd2b776c65981665400110b8cd0bf93c3b3452c1e", "ref_doc_id": "a802dfb1-a085-4e15-84e5-a84ddae7da05"}, "020a0fe4-9d3a-4b2a-a15b-2470686c646e": {"doc_hash": "a444e464e3bff7da0aaa2da5a92b4c0e802561a44f40623aa9609ebc088b0a87", "ref_doc_id": "a802dfb1-a085-4e15-84e5-a84ddae7da05"}, "1e2febac-0ca8-409e-aa17-26a274a3716a": {"doc_hash": "071a7f38ae2c1685b8c8d5f31246ff88a1986b5480c580713914720a4bea7022", "ref_doc_id": "a802dfb1-a085-4e15-84e5-a84ddae7da05"}, "9833fb8d-412e-4fa5-9c5f-9416401c8c55": {"doc_hash": "a466bbfb70b5f1584814672a661f455d4655e88a3819ecfec1c14a48a11dcbab", "ref_doc_id": "ff022234-d71d-46ad-956b-0a001e38d97a"}, "876129a9-00b7-489b-9b74-e4d49a211895": {"doc_hash": "e34ff3b6fd3083209f6c920abdc6d9beeed88c82571bc3d3d4267e95456f85eb", "ref_doc_id": "6ef09ca3-d0d7-4d56-904b-77675ada0e6c"}, "fb50f1df-d98b-4a8c-8c47-f427c196cc65": {"doc_hash": "be6b33e2d28255da98841cd5e995aa32a4fc139b17f18109f6423d66313fa9e1", "ref_doc_id": "6ef09ca3-d0d7-4d56-904b-77675ada0e6c"}, "4ac2c396-47c3-482f-8ef5-a38859334be4": {"doc_hash": "15112e330de837cd4a2806ee42f30c170a2a7deb00eb460fa88bea3617caa9c0", "ref_doc_id": "6ef09ca3-d0d7-4d56-904b-77675ada0e6c"}, "02921ceb-9fc4-4d70-8ccb-a0c9566b6a05": {"doc_hash": "36ad9873f6a4cb596d625dc53f874aba178079606049e63d48dc5291ebb41d8a", "ref_doc_id": "6ef09ca3-d0d7-4d56-904b-77675ada0e6c"}, "103355cf-a015-4bdd-94d3-aa7bb6572b16": {"doc_hash": "825602d0dbeeb30b084440e5d9bf09f79f86d5cb992a566c45a1eadfe67af592", "ref_doc_id": "e34fe21a-43b6-4938-a0da-94ce8e6fe887"}, "ebaf87ff-868a-4436-a1ae-0c3f6f586cc2": {"doc_hash": "15cb24b38852337397948df8a96f674e699492ef10971e7cd405c94f1d98c773", "ref_doc_id": "e34fe21a-43b6-4938-a0da-94ce8e6fe887"}, "52d09477-5764-4389-a48b-7de925680984": {"doc_hash": "8751fbf0b7611ddc0c03234238972bd1fc245e5324035f64e00c762893dbd113", "ref_doc_id": "e34fe21a-43b6-4938-a0da-94ce8e6fe887"}, "e22931d7-ca68-493d-9d64-d7513b071a20": {"doc_hash": "fabf8f06db975dc9653c9421ead68abdc8dc384a38d2419b18d052946fa8e5ec", "ref_doc_id": "7c5eec50-2964-4601-ad30-745f2dadf028"}, "dbcb7956-3782-4047-8b85-912f13830ed6": {"doc_hash": "4f582439b44df37520297c612b7f38510d1a3005daff96913ba542a4fa2e6e62", "ref_doc_id": "7c5eec50-2964-4601-ad30-745f2dadf028"}, "790bf101-ca54-43a5-a74e-797c33ad0195": {"doc_hash": "af5f40731b262181237ac76deaa82e178836be2c2f4f90e911fb460cb0b996ff", "ref_doc_id": "7385d65d-a81a-4ee3-ab0d-801ee406e901"}, "25595cc1-a897-4c2d-aaee-10dd5090f624": {"doc_hash": "d06bc28854a70231ebfed6668e39250725f32918194427b2c48140977ae4a18c", "ref_doc_id": "7385d65d-a81a-4ee3-ab0d-801ee406e901"}, "eef8d306-c1d5-49e2-9b6a-301798ea4063": {"doc_hash": "c10e6e6ca28fb78cbc187d083a3acd556efdc994494ac3c801ef7e630711fcfd", "ref_doc_id": "7385d65d-a81a-4ee3-ab0d-801ee406e901"}, "1687731c-baa4-4e78-a2ea-03fe2b4a854d": {"doc_hash": "c67bf9b2d505bf046afd6ba263a9655a1601ce08c7b5f7c3f1ed01992a8a9d7c", "ref_doc_id": "7385d65d-a81a-4ee3-ab0d-801ee406e901"}, "a60f2398-83d0-4654-92fe-d209101d02f3": {"doc_hash": "310b2af8ab3a9691f39278aceb1e2ed8270e9437d4bef5c6e0993703ac7dd4b3", "ref_doc_id": "7385d65d-a81a-4ee3-ab0d-801ee406e901"}, "e80e809a-89a9-49c5-8db6-ecaf3c77894e": {"doc_hash": "46b7d470e3f7562c05a624b79168d0b87ff9dff1839748e632892dc98cbddc1f", "ref_doc_id": "7385d65d-a81a-4ee3-ab0d-801ee406e901"}, "46a6286b-8c49-4d46-856c-ecb89c16bb59": {"doc_hash": "ba48b43c880171b9b736f7a2cdac310a36a4061cfa87677c09af9b5e9cbbd6a7", "ref_doc_id": "d3c395ca-e222-4561-ab7a-274b7cb5d76d"}, "892ca795-9525-4782-b037-3c622852c571": {"doc_hash": "c3bd1a515859755343494bb3c282ee02728e8825b58dd09cdb5d9bb3f0bb5c47", "ref_doc_id": "d3c395ca-e222-4561-ab7a-274b7cb5d76d"}, "db911fa6-8dbe-44cf-804f-f102990d879a": {"doc_hash": "dd5bd2f37675d630681523e4345edbb4e873862b0d4cc2c0068bda3085194016", "ref_doc_id": "35d71bcd-d63b-4bd6-9b22-72c0dd3093ae"}, "b1779f68-717f-429c-8002-e0586889db7b": {"doc_hash": "d1344f8e901f4c386b1b49e9a3c61f9ac07314481e81c4f63a923f1295ac235c", "ref_doc_id": "35d71bcd-d63b-4bd6-9b22-72c0dd3093ae"}, "3a79e45b-7652-45a5-b905-fcc898844cbf": {"doc_hash": "e884ccc308a0422ede8dbadf642e844a89c771c55629d685ef3895946336db96", "ref_doc_id": "f66c3f8b-0a4d-4cfd-a4b8-f323ab565cec"}, "00acb849-90b0-4d58-bc95-4d544b6fc949": {"doc_hash": "9b78afd5ba7e2d9b613ebd7e8996a5f022128ea16071fe7f010768dbb0b95ff5", "ref_doc_id": "f66c3f8b-0a4d-4cfd-a4b8-f323ab565cec"}, "a936117a-b9af-45d5-b7a4-17ac50e9d0fb": {"doc_hash": "266160ce16530c1c4d028b5f0ba4f4e8c50844090a8a9482644d31a1fb1ea7cc", "ref_doc_id": "f66c3f8b-0a4d-4cfd-a4b8-f323ab565cec"}, "f3c4c837-50cc-4896-9123-106298446286": {"doc_hash": "77422305f3ef7b17687df17585c42495ab8c19c95edef88c84e520d694806bdb", "ref_doc_id": "f66c3f8b-0a4d-4cfd-a4b8-f323ab565cec"}, "ddc4174b-3544-4928-a3b1-8882fa9e9cfd": {"doc_hash": "051e89319ed16f654e73bdf0fb2e4b7e6a3739142bd83eebfc14c57b96d579b0", "ref_doc_id": "6c9c9fbe-b13d-40e4-b964-83cc4cdffdbe"}, "eed170e3-014d-46e1-9cb2-1150e23634b3": {"doc_hash": "ba027ee577dcb41ecf1197c98e8494156fa012995629a71b315709b09947acf1", "ref_doc_id": "6c9c9fbe-b13d-40e4-b964-83cc4cdffdbe"}, "83a93cdf-15ed-4c10-b34d-7df7b67b373a": {"doc_hash": "0adbe6d7868a43663ad211f3d0cba8a6d215f44ed6aff0cb699368722112aa20", "ref_doc_id": "658b3e2b-bd12-4119-9520-c8386c71a9c9"}, "1e19efa8-f10c-4f24-9936-3988ecbd728f": {"doc_hash": "f49caade165d0e77bd4b2ecdec63dd01369dd9ad7be145930f9c1d52a96da415", "ref_doc_id": "658b3e2b-bd12-4119-9520-c8386c71a9c9"}, "67c27eb4-00b7-40ab-9b44-65b72798cca1": {"doc_hash": "95034995580cc4ca03eaa2e31c5b5ec82af5db4f7a4b32198f566d1842be9ba7", "ref_doc_id": "658b3e2b-bd12-4119-9520-c8386c71a9c9"}, "77883c74-6a43-4770-99ad-15367ee48e74": {"doc_hash": "0ee5f92abd275721249953e04957712aab509b29bf24ae795a810c320327fbd7", "ref_doc_id": "658b3e2b-bd12-4119-9520-c8386c71a9c9"}, "b31ec050-c902-491c-9258-6bb677d5271a": {"doc_hash": "9e04172f0ac70f2d8242f1e12e12b7535043b471ebaa5e95cb5aa83c2a1564ca", "ref_doc_id": "9e70d970-f163-4450-a05b-569fd6cda981"}, "fb621787-34b2-4d44-9b43-40ae4d51ef6f": {"doc_hash": "8c98ae31bcb7e825671193ae091130cdfa335daa724a03686e34306c2ff2049e", "ref_doc_id": "9e70d970-f163-4450-a05b-569fd6cda981"}, "2a37b826-09f0-42ce-885b-bd38d77ff3e3": {"doc_hash": "269c42e82b4a55483c695934616e0fd9c81abaf49801366d69a7ccd1d26e69a8", "ref_doc_id": "9e70d970-f163-4450-a05b-569fd6cda981"}, "e5daa82f-1059-48b0-b69e-5474ea40a678": {"doc_hash": "5ac7709e0750ca519a0b79eb5060ae864bdab8ecae7b4865117840d0b4ec4680", "ref_doc_id": "9e70d970-f163-4450-a05b-569fd6cda981"}, "a977913b-9ed7-4a01-b05a-04cd810a45ab": {"doc_hash": "09e813fef30aee33ff722ce94689ec5e5759ed45865cc293f7c2ed8fdfda32cc", "ref_doc_id": "0352b9e5-6c5f-4e76-8f61-8bed337644a8"}, "5f4e4fd6-4fc1-43b9-9c97-2ab0690244c0": {"doc_hash": "ed365ac78c69dbfc2f10a3b448789fe1602339d78bcb3e211377fd3f2760bcd7", "ref_doc_id": "0352b9e5-6c5f-4e76-8f61-8bed337644a8"}, "12a34e79-6b58-4d1a-88ba-b9366185df8d": {"doc_hash": "898d3bcf5b9239b2045bf008782bdceb1280cc1ff246530caac4301d46bf9bb9", "ref_doc_id": "f4024f7c-c873-4d85-9473-f4a3020695b4"}, "e6f4ae6f-a0f7-4fed-b655-6fc05df310f0": {"doc_hash": "5db5229ea2c48b97a7262778bd4deb25bbb54400b255113d6520ffd64f04d30a", "ref_doc_id": "529319ff-9d54-42a7-8896-5b102213bf49"}, "e964de9c-1efd-430b-a3f1-6217c7ce2962": {"doc_hash": "d7a438dcd45f661a9057c412bb26b90789501bdb3fb33ad19c51221eecea6eea", "ref_doc_id": "1da7cd2f-71b1-4b5c-be5c-7494f7c964d6"}, "39523747-413c-449d-bd8d-ad6ccf5f30c5": {"doc_hash": "79e86a592842647b83ef6948d6ba909e1ec13157d9974d14df6ac5f6865475d8", "ref_doc_id": "1da7cd2f-71b1-4b5c-be5c-7494f7c964d6"}, "62fd2cc8-8280-4ce1-9c3b-35523d3250ac": {"doc_hash": "7f41b049aee4e78285bd3ef9406577a40cdeced1090f3984a8292e85d14f7439", "ref_doc_id": "1da7cd2f-71b1-4b5c-be5c-7494f7c964d6"}, "19b3d750-66b7-41ef-aa1e-1fcc6d1384e7": {"doc_hash": "521f737638b25d5e29ae3d93c4c787557d8d46ed9050f014cc6295d61e87ba74", "ref_doc_id": "6cc5d296-aa67-4c8e-849d-31c835174760"}, "bf154297-c15d-4b70-ad17-950be1b71696": {"doc_hash": "129160022a24838bc1fe3b3bf2f790b74df5e26364907bade7ca05a20d08f641", "ref_doc_id": "6cc5d296-aa67-4c8e-849d-31c835174760"}, "3aae7b83-4bc0-426c-9a88-dad4e183565a": {"doc_hash": "992d6a79c140f20bb236ab34e3ed97821c2e0065afc3e964fad45e7353b86e55", "ref_doc_id": "6cc5d296-aa67-4c8e-849d-31c835174760"}, "6bbe09c1-6aff-4f7a-af38-59c0a67c3b69": {"doc_hash": "dc0b24457c802e78a707d166bce64e2d935e7951933c9804d7cf726b4d860198", "ref_doc_id": "591d6c26-7b4a-46cf-a55e-e0a3853009b0"}, "620e52d2-d58f-4b9b-80dc-3cf4a505692f": {"doc_hash": "0c2a99fff6a72c5ee947f949b359aa7dc7d26d00c64d5538e4f1800e55f84659", "ref_doc_id": "591d6c26-7b4a-46cf-a55e-e0a3853009b0"}, "ad43e8ca-78d4-4915-87e2-1684d61c49b9": {"doc_hash": "364cf2b4ea29f02876ba89ea6d5d586bf81840e5eb80362eb53d3a266c3795c8", "ref_doc_id": "90b3f1ff-bfc5-4c52-aecf-6719d5d37e5d"}, "2223a2d0-86e0-41ed-b897-2b41bb92c1b7": {"doc_hash": "322062e97b9f6a22a9f074bc47d6acb44db93a60dddac688c29444892782ad22", "ref_doc_id": "f270085a-4b2d-4cd3-a38d-94e567ba30ba"}, "68ac249e-80c8-451f-84b4-56b62e56769e": {"doc_hash": "49581352f750b263f2befbaad51529d308a391e0398173bc53cfe32829cc4715", "ref_doc_id": "f270085a-4b2d-4cd3-a38d-94e567ba30ba"}, "6049579a-0074-42a5-908f-ed134ceb3e0d": {"doc_hash": "d6a1d27daa32a3b9ad78b15efca8c60981276cc87e8a81d2d7100b4288c84a9e", "ref_doc_id": "33f29cd2-df22-4a07-964c-35e7dffb43bd"}, "5ce87ded-fe42-44cf-a315-2559355c3dfb": {"doc_hash": "a5a67953a23b24f1687959603fd874f70aac8616c85efa4ef9de4d65e4c47400", "ref_doc_id": "64bed31e-7c59-4b52-b7c9-3d8eb6171e69"}, "ab4be978-4237-47a1-b9c9-a53015888493": {"doc_hash": "e3fccbf683c3efa426eac96c7e0d346a623edc6e65c31bc0324c13f53c49d3a5", "ref_doc_id": "64bed31e-7c59-4b52-b7c9-3d8eb6171e69"}, "23dbfd41-80ec-47ea-9de7-a08155e311b8": {"doc_hash": "2f5128c9bd40445b1eff306d0b84af6066e8815b83bef1fa88802eac1d11443c", "ref_doc_id": "64bed31e-7c59-4b52-b7c9-3d8eb6171e69"}, "b0e4086e-400f-4e65-8532-c7fd7353cd8f": {"doc_hash": "56610c36aa1c213217538e3716eef1c16fb28fde02fd946e71155f1df1e24d67", "ref_doc_id": "6b30c2db-168b-4d8b-b7ea-82a8f4f27eca"}, "c48ccc50-dfc0-4e9e-b942-debce010f5a3": {"doc_hash": "e88fa52a49b60892f0aebbfaa0608b4cecb14669bd73d34c1ead27b8651aa17f", "ref_doc_id": "d013c5ef-a363-4978-8dbe-b07fe972bf3f"}, "ccb20e4d-511e-4573-9e0c-f87f3c45c42d": {"doc_hash": "a80c6f38e4cba3e8ac6d48883d1aecfc515b0e8478c4f372725b932d73fb48db", "ref_doc_id": "d013c5ef-a363-4978-8dbe-b07fe972bf3f"}, "822d6083-6ad0-4204-a82d-f62393a59aa1": {"doc_hash": "7de4b34caedcbb0239b1af6dce2be34ba79aa84f6c453ee6e20799eae2f523c3", "ref_doc_id": "768dec39-fe48-4c1d-a18a-e7f054e32739"}, "cc748c2e-0c29-4b64-b923-0f81adad1429": {"doc_hash": "0765e6d70f806a4ea652cee24ad666dc8545b4a7233e5290a37f895c5bb4e9b7", "ref_doc_id": "768dec39-fe48-4c1d-a18a-e7f054e32739"}, "d279576f-f8a2-4c88-a2c4-5cc635028747": {"doc_hash": "22510de4ac256c9a096a2e4feef3b78fe28927f97f048fa24c555b9e9d228e36", "ref_doc_id": "2473d46c-0ccc-4fc6-ae14-ea0a15e51b6b"}, "fab507da-6367-4727-906c-c0f3c6588536": {"doc_hash": "01ebbe52058eb32a9a63155dc9776d2bed4fd399fcd27472f453182265e281b0", "ref_doc_id": "2473d46c-0ccc-4fc6-ae14-ea0a15e51b6b"}, "60466d64-81fb-46a4-a9f9-105ee1f4c9da": {"doc_hash": "fe91018d3e49697abbbac67b7c5e624b4919e58636fd059418514acb1f17f5e0", "ref_doc_id": "65409667-83da-4b4c-8725-3c8393996070"}, "649a9591-69f7-49da-96cc-d5db61df64e4": {"doc_hash": "f4efcf2fff18c9ba659e8eded4bf8fb336ec1de60ef75ce2faa654042e282ecc", "ref_doc_id": "65409667-83da-4b4c-8725-3c8393996070"}, "c5364800-f3bd-4e89-8e38-0333a123361e": {"doc_hash": "07f408eb8d5a29c9163d240a49b151ba5d601cfd7fac342520670d5f69ae978c", "ref_doc_id": "65409667-83da-4b4c-8725-3c8393996070"}, "35df53c4-4d0e-4233-9fe7-93a81839b6d6": {"doc_hash": "2312e270eb576ce6ddfcb28c0904c239ff10ad863ce64a194d7dd38535d17bfb", "ref_doc_id": "1a43a6eb-1ccb-4acc-bc84-6e65487ec16a"}, "c20ae551-1854-4eb4-821b-e2bd868bcb0d": {"doc_hash": "2160d5d94a9a4ff2330b654aa99d9a80022826d2e1193b2a6cba98c47d80e11f", "ref_doc_id": "1a43a6eb-1ccb-4acc-bc84-6e65487ec16a"}, "8c570dc5-7ecf-40a6-99ec-219181965149": {"doc_hash": "dbb5bd85edf5552ba5d442cf444d55b5117309735e3c43c3ccaa11eb24c20797", "ref_doc_id": "2f9fa58d-29eb-4248-b6cb-52465ee67e91"}, "c48cc975-3ac0-4be5-83e4-c519dab13668": {"doc_hash": "ca88e1c77e86294b059a9cb6f750a120dede93b6cf13547ae65543bac4c32e67", "ref_doc_id": "93cdd84e-6b13-4405-adcc-134add3cbe11"}, "70ce7632-c91d-4431-a3f5-68397739189e": {"doc_hash": "eb956a0c87b7cd37fbe87e7a6b93b7da0c4725d7f34085d572824d4fdc5b19cc", "ref_doc_id": "a2a18fb6-3415-4745-9f4b-b263304d32d4"}, "4f40aae8-2ca5-4c8b-8e30-1eb25c595274": {"doc_hash": "88d306ffeb818abd2424a84aa6a6ed3436e9ab10977deed853bef04d2d6c0597", "ref_doc_id": "a2a18fb6-3415-4745-9f4b-b263304d32d4"}, "b60a9dcf-19b3-4df1-ad5b-228c50b8032c": {"doc_hash": "683f936b2c3f6e07ad83c9eadc352a458fd470188900e0f8e7e0312f8ce9b18c", "ref_doc_id": "a2a18fb6-3415-4745-9f4b-b263304d32d4"}, "697fc1ef-e443-472a-9121-97a2a620f624": {"doc_hash": "f58deea3ec2d3d72ce078a38ef18ec45a33cc4ad8fa05ee3eec29f306c12f7ae", "ref_doc_id": "1eee1785-8b22-4586-bd15-a49a1e1c88cb"}, "3238c40c-e598-4c35-b4bc-a5b780a73d63": {"doc_hash": "ff4294d707c61bcce599a9624315880517ed7ffb5d84071a27b047dbe797c8ee", "ref_doc_id": "1eee1785-8b22-4586-bd15-a49a1e1c88cb"}, "eee5711e-eb8c-4895-a2a3-31315a11ae0f": {"doc_hash": "f6e63ba79983e7a3af6824f9792b5972d6fab9dddb4e51efb48491863630f2a5", "ref_doc_id": "fee773e4-ee3a-4447-bc94-b604cf9231b9"}, "af026887-3667-4b25-a09b-a89a11e5ca0a": {"doc_hash": "a683940b7ac749b7a88b42b7cba1594ec7d828b036442e754a523dcede3955de", "ref_doc_id": "fee773e4-ee3a-4447-bc94-b604cf9231b9"}, "5bdf1930-b446-42d2-b806-4396d68a5c2c": {"doc_hash": "d494e2c99b97a9a6c17dda6dd75b0f9251cca3dba4ca2474b485367a3c07e295", "ref_doc_id": "295d2839-53d5-44dc-a40b-4fdcff61c08d"}, "551b68f8-c29f-4abf-9b6e-ab446c536438": {"doc_hash": "3bc5484e388497d4f93446d5cfc721e4f3cae4247d3478e016f0b202de8ab4c3", "ref_doc_id": "295d2839-53d5-44dc-a40b-4fdcff61c08d"}, "8de519fa-e912-479f-936d-1e0df2cad1f9": {"doc_hash": "764f7120d2662e94a0d975b9da01dfc3d3e89e889b5d1478728f9bb94925e0f3", "ref_doc_id": "295d2839-53d5-44dc-a40b-4fdcff61c08d"}, "d69a9195-cfb0-4605-8d1c-4cb506908f46": {"doc_hash": "33f63a52cfdd262c3b1e2b7d29472643651a9b29d9aa43b15f4b5041b9d2a4a6", "ref_doc_id": "05861ae7-c95f-4e12-aa50-ec6f2863cfa7"}, "21a243d7-fae5-49e5-8236-c36ad36971b5": {"doc_hash": "1502ebce179f546f0d8aa9004250e0efe79527e26599462a48863dca34b8b41a", "ref_doc_id": "d4dbac9a-42a7-4e29-8602-358df8e42956"}, "4060ea8c-9755-4421-9c1c-e6e2f13ba1a3": {"doc_hash": "f614f766a1c42ab27eea924cba4f6d4da3b06fda23193112172f8d582a862886", "ref_doc_id": "1b35a746-2334-4af3-b700-9e7451d7e037"}, "e7c31deb-e039-4a95-a056-61f085f667ae": {"doc_hash": "029a57eb2bdbe00448087c37950b9574d76c170bdea619266c4628c122acd353", "ref_doc_id": "c4650e72-2e50-44fa-8278-a4e922d08f99"}, "ab5ee918-c0b4-4cf7-8b11-d92b6c737281": {"doc_hash": "c0f11a78612eee9223c6204d508a7b04cd6af83914894d357cb4d3e3803e7559", "ref_doc_id": "c4650e72-2e50-44fa-8278-a4e922d08f99"}, "433da125-97f8-4ad6-9806-4592a1ae5936": {"doc_hash": "ab0b4bcd3c3e5b2de7e451270d13ea5fe24019fbf5d4dcba51b7ba4aaf7ed482", "ref_doc_id": "c4650e72-2e50-44fa-8278-a4e922d08f99"}, "93d5cda4-eb80-49f3-b3a3-7b709e8ad9cb": {"doc_hash": "55dd050410ffb82ff87cfdf02ba4dfd6ee9e52f592d99967bd778ef14069d030", "ref_doc_id": "69c3976a-7737-4d6e-a73b-2ee14795a4d6"}, "ddd9c354-aaee-4172-85d5-49d8297500bc": {"doc_hash": "9cfe5c07e5c4c873475d765ad30d63348dd362f89443e837d4d7053b97727c1b", "ref_doc_id": "69c3976a-7737-4d6e-a73b-2ee14795a4d6"}, "03ffeae0-4aac-4f98-b2dc-68d972f3bb7d": {"doc_hash": "8bddfde18cb879f794a3ab0ee5f94a63d04202064d5dfffab7d76e8ba4237a2e", "ref_doc_id": "bb70198e-5d74-49ba-9662-c7cb492efe26"}, "c6fcd670-40e9-49de-bc71-e4da4c99b2b1": {"doc_hash": "cdacc1ba5f9c178a10c734271f9e8f3ad819f1b73bc5ee79d9158285404139ee", "ref_doc_id": "036cc20e-6afc-45ea-95aa-e2dc7a370631"}, "6b83140e-8002-480d-9201-fc1432699b27": {"doc_hash": "27d17518a47bba2f2508260985b4c1ef8b9165228c6c52f74f20e22562ca7ad3", "ref_doc_id": "036cc20e-6afc-45ea-95aa-e2dc7a370631"}, "5f832ed9-9715-4e6c-a976-836c387b101a": {"doc_hash": "47421a2e5a6c04ef48f5d4c17034104453f3392de9b05a7a535f6cf0fe5b9adb", "ref_doc_id": "036cc20e-6afc-45ea-95aa-e2dc7a370631"}, "e64faa6d-6cfe-4fab-8fdb-7661991005b3": {"doc_hash": "1d8fdce97883f0fea9881457131461885e226f984ded57b95d830c48097382f5", "ref_doc_id": "036cc20e-6afc-45ea-95aa-e2dc7a370631"}, "d88b7964-bcd2-4799-879a-d5def701f526": {"doc_hash": "ce801d543df0912f07bc96053ca03ae006be428f3297946a01b931feabb6295b", "ref_doc_id": "036cc20e-6afc-45ea-95aa-e2dc7a370631"}, "2bdf1086-5b5e-487f-af65-985cd564a3d8": {"doc_hash": "91bb3d881643fe0f27bc3c9c985527db30c8a40877c7b14cc993560bb5c11b0b", "ref_doc_id": "036cc20e-6afc-45ea-95aa-e2dc7a370631"}, "7a588886-6fd7-47b7-9c4f-bed2282189b8": {"doc_hash": "b974da409a217e8567290659deb7ba0f438a7975205ade536a3426614ad33a2e", "ref_doc_id": "036cc20e-6afc-45ea-95aa-e2dc7a370631"}, "cb4b951c-f4ea-4d8e-b3e4-0a3ca0e988db": {"doc_hash": "d00c02822e092d53c8508bdd4b2891e97cfb1f7a021c4823616c865c1c6a9a0d", "ref_doc_id": "ca0e92d5-47c1-4dc2-b585-920412c51542"}, "b430f89c-e810-4466-b7e9-8dfaf4cc8064": {"doc_hash": "a6452d020af232299de008a73679c3371963a89d72d8f4811c6bf3453fe6d51c", "ref_doc_id": "ca0e92d5-47c1-4dc2-b585-920412c51542"}, "1876db37-d180-46c0-8bc7-4397949ae4bd": {"doc_hash": "78e9942924b1c1dd11025e8008120f6851a8628c70116e9280c15e560691b56a", "ref_doc_id": "aff38af1-2b57-4a15-9ad4-916f9a964e9e"}, "dae2bd5e-f99f-4fac-9202-d719e49a56f5": {"doc_hash": "4f2ff19d1ca1b9c329fdf3f5aa2b79da50a068c676142bd4184384a4d1805e50", "ref_doc_id": "aff38af1-2b57-4a15-9ad4-916f9a964e9e"}, "0a9ef591-d1a1-492c-8fc5-36cc23356bce": {"doc_hash": "ce005dc37f7309859717977bc7e2f3efe455713789ea3144430bb7c8ed92acdc", "ref_doc_id": "aff38af1-2b57-4a15-9ad4-916f9a964e9e"}, "8075beab-b865-454f-b9ec-9d7e69c7757d": {"doc_hash": "483d8fc30347e69334a17c92dc92f227e5770095959cc44ae0f061d04f89224f", "ref_doc_id": "aff38af1-2b57-4a15-9ad4-916f9a964e9e"}, "f458fd93-2978-449c-a835-48d81edb0d57": {"doc_hash": "355af0e37c70dcdb040e4fd0e05b0ccd3eb84ef337f862b7eb665443dc2d3e59", "ref_doc_id": "170a97eb-81bd-449d-be31-fdb5631aef90"}, "76b4e9d6-cbad-4dfe-b089-76a8af22d836": {"doc_hash": "237a31ee69a2836186b459766a74d5c873d0281e74b40f819c897dce838f7bf3", "ref_doc_id": "aec0222c-6689-4c17-843a-09c2360240a6"}, "81bdae04-6371-4ce8-bd3f-5ef4d3ee23ea": {"doc_hash": "6187ab6f4d22c3e10c1c75c52bc6f549522df6e6ce5a8aa11d8d904353506d32", "ref_doc_id": "6f6dae7e-5f00-4deb-958f-07a241f3289b"}, "33fe22bc-688d-46a6-9ca5-d9f97976c601": {"doc_hash": "625bcecaa09e529505219b6d643fce025aa1d92381a0e65232e41c7dbda024e5", "ref_doc_id": "822deb9f-11f3-49cc-9315-71846cfc6731"}, "320fcc8a-c626-4a66-927a-b524902e31dc": {"doc_hash": "16b0504428b40de9d7a91dacf3e2d0b1f9db92364a3afbc2f5f50a09254a6f98", "ref_doc_id": "822deb9f-11f3-49cc-9315-71846cfc6731"}, "d8977978-ee4d-4897-bebb-3eebf6a37e6f": {"doc_hash": "1cff9b00003dd2f452743425a6c2254df45f95a3b4342fd82d96059d0743049e", "ref_doc_id": "04fb9861-1f52-4be8-ada0-e4da2c5dd23c"}, "6726c641-6646-40a8-9de2-594a14a1f26c": {"doc_hash": "ec3e767544cc07a6a197aeae39cc25ae3a65938df118af7a834faffecc59ae8e", "ref_doc_id": "04fb9861-1f52-4be8-ada0-e4da2c5dd23c"}, "bf29aa66-8795-4f15-974b-7bd09d40d29c": {"doc_hash": "047e916604382ef4f7cb5c8d4231da34abd7e17e3dada49f4063998a9bd3be3f", "ref_doc_id": "8e97222e-ee39-45c2-b060-6e5ca5995f56"}, "49b17c5e-9050-4dc1-9ca5-bd1e1ebded20": {"doc_hash": "557c006c64e5525a82748483bd199bc5253ef6179180f529caa9e2df99fa8ceb", "ref_doc_id": "8e97222e-ee39-45c2-b060-6e5ca5995f56"}}, "docstore/data": {"41f7e5a5-5ced-458c-8ef5-498839cafa82": {"__data__": {"id_": "41f7e5a5-5ced-458c-8ef5-498839cafa82", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_7-24-release-notes_.txt", "file_name": "blog.langchain.dev_7-24-release-notes_.txt", "file_type": "text/plain", "file_size": 2719, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38253cc8-12dd-4b35-8f2a-b9f2e9c40c69", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_7-24-release-notes_.txt", "file_name": "blog.langchain.dev_7-24-release-notes_.txt", "file_type": "text/plain", "file_size": 2719, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "41acafbd57109a1524a7f0a59b2b00437ff802f58d9e100c61d60c4d20e9a422", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "36bf1abd-a5cd-4473-8ef3-63a224053636", "node_type": "1", "metadata": {}, "hash": "33b92c6e385815ee2957f6ef73f208d9ff5767c467765230b306a8908b787c53", "class_name": "RelatedNodeInfo"}}, "hash": "9da70c1c3cfa61b9b83f542426ef4fb931318e30eb911c6b7dbe1b4b6efbda20", "text": "URL: https://blog.langchain.dev/7-24-release-notes/\nTitle: [Week of 7/24] LangChain Release Notes\n\nNew in LangSmith\n\nStreaming in the playground: The playground now supports streaming! You can enter the playground by clicking on the \u201cOpen in Playground\u201d button on the upper right hand corner of select runs.\n\nThe playground now supports streaming! You can enter the playground by clicking on the \u201cOpen in Playground\u201d button on the upper right hand corner of select runs. Infrastructure changes to support more users: We\u2019ve beefed up our backend. That means more of you off the waitlist soon!\n\nWe\u2019ve beefed up our backend. That means more of you off the waitlist soon! UI and navigation enhancements: We\u2019ve made a lot of quality-of-life improvements to the UI, mainly around the run details page. Biggest change is the trace tree is now on the left hand side. See the screenshot below for an up-to-date picture.\n\nNew in Open Source\n\nLangChain Experimental : We\u2019re moving select chains into a separate package to make LangChain leaner, more focused, and safer. This does induce some backwards incompatible changes. Read the GitHub discussion here.\n\n: We\u2019re moving select chains into a separate package to make LangChain leaner, more focused, and safer. This does induce some backwards incompatible changes. Read the GitHub discussion here. Docs restructures: We\u2019ve made a lot of improvements to docs. First, we ported over the skeleton from Python JS, so they are matching. Second, we restructured docs to better distinguish between modules and use cases.\n\nWe\u2019ve made a lot of improvements to docs. First, we ported over the skeleton from Python JS, so they are matching. Second, we restructured docs to better distinguish between modules and use cases. Web retriever : We went deep on a particular use case\u2013a web research assistant. Turned into an interesting and novel retriever. Source code here.\n\nWe went deep on a particular use case\u2013a web research assistant. Turned into an interesting and novel retriever. Source code here. Llama 2 Support: We quickly added support and examples for Llama 2, including some Llama 2 specific prompts.\n\nIn case you missed it\n\nComing soon\n\nSupport for teams and organizations to share work: Bring colleagues together in LangSmith.\n\nBring colleagues together in LangSmith. Increasing the composability of chains: We\u2019re working on some syntax to allow for chains and components to be truly composable.\n\nWe\u2019re working on some syntax to allow for chains and components to be truly composable. Better use case sections in the documentation: All use case sections will be getting a face lift.\n\nUse-cases we love\n\nWant more of these in your inbox? Sign up here.", "start_char_idx": 0, "end_char_idx": 2689, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "36bf1abd-a5cd-4473-8ef3-63a224053636": {"__data__": {"id_": "36bf1abd-a5cd-4473-8ef3-63a224053636", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_about_.txt", "file_name": "blog.langchain.dev_about_.txt", "file_type": "text/plain", "file_size": 426, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "66db197a-f5f5-4026-96c4-a651c6c1ca97", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_about_.txt", "file_name": "blog.langchain.dev_about_.txt", "file_type": "text/plain", "file_size": 426, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "e7f0e661dd61eb0e5a6e38c758b581c9230ac6717d939638ddc9104ae113726a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41f7e5a5-5ced-458c-8ef5-498839cafa82", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_7-24-release-notes_.txt", "file_name": "blog.langchain.dev_7-24-release-notes_.txt", "file_type": "text/plain", "file_size": 2719, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "9da70c1c3cfa61b9b83f542426ef4fb931318e30eb911c6b7dbe1b4b6efbda20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e67d7290-6bf9-49e2-8451-74d2ca337e1c", "node_type": "1", "metadata": {}, "hash": "582f9b409d35299012d033868597aa5d2c208ce45026e2a02d3f9cb7a5367c91", "class_name": "RelatedNodeInfo"}}, "hash": "33b92c6e385815ee2957f6ef73f208d9ff5767c467765230b306a8908b787c53", "text": "URL: https://blog.langchain.dev/about/\nTitle: Write with us\n\nIf you would like to publish a guest post on our blog, say hey and send a draft of your post to hello@langchain.dev.\n\nWe are particularly enthusiastic about publishing: 1-technical deep-dives about building with LangChain/LangSmith 2-interesting LLM use-cases with LangChain/LangSmith under the hood!\n\nWe can't wait to see what you're working on and thinking about!", "start_char_idx": 0, "end_char_idx": 426, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e67d7290-6bf9-49e2-8451-74d2ca337e1c": {"__data__": {"id_": "e67d7290-6bf9-49e2-8451-74d2ca337e1c", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_announcing-langsmith_.txt", "file_name": "blog.langchain.dev_announcing-langsmith_.txt", "file_type": "text/plain", "file_size": 11758, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f2b6468-fca3-4e90-a794-75fbd0ccfa6f", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_announcing-langsmith_.txt", "file_name": "blog.langchain.dev_announcing-langsmith_.txt", "file_type": "text/plain", "file_size": 11758, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "d4f70bd51ab9daee4a5b2652d295b080fdac3d720b383021c9f1dad5247fac5c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "36bf1abd-a5cd-4473-8ef3-63a224053636", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_about_.txt", "file_name": "blog.langchain.dev_about_.txt", "file_type": "text/plain", "file_size": 426, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "33b92c6e385815ee2957f6ef73f208d9ff5767c467765230b306a8908b787c53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b72d55ca-44aa-4650-914f-3ec67ad30e38", "node_type": "1", "metadata": {}, "hash": "f86e075c4805beda4c520c0fc9f6679b9995d40830b2640be9e1c45c1200f6b8", "class_name": "RelatedNodeInfo"}}, "hash": "582f9b409d35299012d033868597aa5d2c208ce45026e2a02d3f9cb7a5367c91", "text": "URL: https://blog.langchain.dev/announcing-langsmith/\nTitle: Announcing LangSmith, a unified platform for debugging, testing, evaluating, and monitoring your LLM applications\n\nLangChain exists to make it as easy as possible to develop LLM-powered applications.\n\nWe started with an open-source Python package when the main blocker for building LLM-powered applications was getting a simple prototype working. We remember seeing Nat Friedman tweet in late 2022 that there was \u201cnot enough tinkering happening.\u201d The LangChain open-source packages are aimed at addressing this and we see lots of tinkering happening now (Nat agrees)\u2013people are building everything from chatbots over internal company documents to an AI dungeon master for a Dungeons and Dragons game.\n\nThe blocker has now changed. While it\u2019s easy to build a prototype of an application in ~5 lines of LangChain code, it\u2019s still deceptively hard to take an application from prototype to production. The main issue that we see today is application performance\u2013something that works ~30% of the time is good enough for a Twitter demo, but not nearly good enough for production.\n\nToday, we\u2019re introducing LangSmith, a platform to help developers close the gap between prototype and production. It\u2019s designed for building and iterating on products that can harness the power\u2013and wrangle the complexity\u2013of LLMs.\n\nLangSmith is now in closed beta. So if you\u2019re looking for a robust, unified, system for debugging, testing, evaluating, and monitoring your LLM applications, sign up here.\n\n\n\nHow did we get here?\n\nGiven the stochastic nature of LLMs, it is not easy\u2013and there\u2019s currently no straightforward way\u2013to answer the simple question of \u201cwhat\u2019s happening in these models?,\u201d let alone getting them to work reliably. The builders we hear from are running into the same roadblocks (and it\u2019s true for our team, too):\n\nUnderstanding what exactly the final prompt to the LLM call is (after all the prompt template formatting, this final prompt can be long and obfuscated)\n\nUnderstanding what exactly is returned from the LLM call at each step (before it is post-processed or transformed in any way)\n\nUnderstanding the exact sequence of calls to LLM (or other resources), and how they are chained together\n\nTracking token usage\n\nManaging costs\n\nTracking (and debugging) latency\n\nNot having a good dataset to evaluate their application over\n\nNot having good metrics with which to evaluate their application\n\nUnderstanding how users are interacting with the product\n\nAll of these problems have parallels in traditional software engineering. And, in response, a set of practices and tools for debugging, testing, logging, monitoring, etc. has emerged to help developers abstract away common infrastructure and focus on what really matters - building their applications. LLM application developers deserve the same.\n\nLangSmith aspires to be that platform. Over the last few months, we\u2019ve been working directly with some early design partners and testing it on our own internal workflows, and we\u2019ve found LangSmith helps teams in 5 core ways:\n\nDebugging\n\nLangSmith gives you full visibility into model inputs and output of every step in the chain of events. This makes it easy for teams to experiment with new chains and prompt templates, and spot the source of unexpected results, errors, or latency issues. We\u2019ll also expose latency and token usage so that you can identify which calls are causing issues.\n\nWe\u2019ve also made it easy to change and rerun examples from the UI. We added this feature after seeing teams take logs of bad examples and copy-paste into the OpenAI playground to tweak the prompt until they got a good result. We wanted to eliminate that friction, and now with the click of a button, you can go from a log to a playground where you can actively edit. This is currently supported for both OpenAI and Anthropic models, with support for more coming soon. We\u2019re also working on supporting this for chains in general.\n\nThis deep visibility into model performance has been particularly helpful for teams developing complex applications. LangSmith helped Streamlit and Snowflake implement agents that could intelligently and reliably answer questions about their data.\n\n\"LangChain has been instrumental in helping us prototype intelligent agents at Snowflake,\u201d said Adrien Treuille, Director of Product at Snowflake. \u201cLangSmith was easy to integrate, and the agnostic open source API made it very flexible to adapt to our implementation,\u201d tacked on Richard Meng, Senior Software Engineer at Snowflake.\n\nBoston Consulting Group also built a highly-customized, and highly performant, series of applications on top of LangChain\u2019s framework by relying on this same infrastructure.\n\n\u201cWe are proud of being one of the early LangChain design partners and users of LangSmith,\u201d Said Dan Sack, Managing Director and Partner at BCG. \u201cThe use of LangSmith has been key to bringing production-ready LLM applications to our clients. LangSmith's ease of integration and intuitive UI enabled us to have an evaluation pipeline up", "start_char_idx": 0, "end_char_idx": 5074, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b72d55ca-44aa-4650-914f-3ec67ad30e38": {"__data__": {"id_": "b72d55ca-44aa-4650-914f-3ec67ad30e38", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_announcing-langsmith_.txt", "file_name": "blog.langchain.dev_announcing-langsmith_.txt", "file_type": "text/plain", "file_size": 11758, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f2b6468-fca3-4e90-a794-75fbd0ccfa6f", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_announcing-langsmith_.txt", "file_name": "blog.langchain.dev_announcing-langsmith_.txt", "file_type": "text/plain", "file_size": 11758, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "d4f70bd51ab9daee4a5b2652d295b080fdac3d720b383021c9f1dad5247fac5c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e67d7290-6bf9-49e2-8451-74d2ca337e1c", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_announcing-langsmith_.txt", "file_name": "blog.langchain.dev_announcing-langsmith_.txt", "file_type": "text/plain", "file_size": 11758, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "582f9b409d35299012d033868597aa5d2c208ce45026e2a02d3f9cb7a5367c91", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe3f1d63-9531-4c4f-967a-fb91d26d77e5", "node_type": "1", "metadata": {}, "hash": "871a4b3fb42bae3349398b4f2df08ebc86214a3c9d299106386e1d282b8787be", "class_name": "RelatedNodeInfo"}}, "hash": "f86e075c4805beda4c520c0fc9f6679b9995d40830b2640be9e1c45c1200f6b8", "text": "our clients. LangSmith's ease of integration and intuitive UI enabled us to have an evaluation pipeline up and running very quickly. Additionally, tracing and evaluating the complex agent prompt chains is much easier, reducing the time required to debug and refine our prompts, and giving us the confidence to move to deployment.\u201d\n\nIn another example of debugging in action, we partnered with DeepLearningAI to equip learners in the recently-released LangChain courses with access to LangSmith. This allowed students to easily visualize the exact sequence of calls, and the inputs and outputs at each step in the chain with precision. Students can understand exactly what the chains, prompts, and LLMs were doing, which helps build intuition as they learn to create new and more sophisticated applications.\n\n\n\nTesting\n\nOne of the main questions we see developers grapple with is: \u201cIf I change this chain/prompt, how does that affect my outputs?\u201d The most effective way to answer this question is to curate a dataset of examples that you care about, and then run any changed prompts/chains over this dataset. LangSmith first makes it easy to create these datasets from traces or by uploading datasets you\u2019ve curated manually. You can then easily run chains and prompts over those data sets.\n\nThe first helpful step is simply manually looking at the new inputs and outputs. Although this may seem unsatisfyingly basic, it actually has some benefits - many of the companies we\u2019ve spoken to actually like some manual touch points because it allows them to develop better intuition about how to interact with LLMs. This intuition can prove incredibly valuable when trying to think about how to improve the application. The main unlock we hope to provide is a clear interface for letting developers easily see the inputs and outputs for each data point, as without that visibility they cannot build up that intuition.\n\nToday, we primarily hear from teams that want to bring their prototype into production, and are narrowing in on specific prompts they\u2019d like to improve. Klarna is building industry-leading AI integrations that go beyond a simple call to a language model, and instead rely on a series of calls. As they focus on a specific section, LangSmith has provided the tools and data they need to ensure no regressions occur.\n\nIn parallel, we\u2019re starting to hear from more and more ambitious teams that are striving for a more effective approach.\n\nEvaluating\n\nLangSmith integrates seamlessly with our open source collection of evaluation modules. These modules have two main types of evaluation: heuristics and LLMs. Heuristic Evaluations will use logic like regexes to evaluate the correctness of an answer. LLM evaluations will use LLMs to evaluate themselves.\n\nWe are extremely bullish on LLM assisted evaluation over the long term. Critics of this approach will say that it\u2019s conceptually shaky and practically costly (time and money). But, we\u2019ve been seeing some very compelling evidence come out of top labs that this is a viable strategy. And, as we collectively make improvements to these models\u2013both private and open source\u2013and usage becomes more ubiquitous, we expect costs to come down considerably.\n\nMonitoring\n\nWhile debugging, testing, and evaluating can help you get from prototype to production, the work doesn\u2019t stop once you ship. Developers need to actively track performance, and ideally, optimize that performance based on feedback. We consistently see developers relying on LangSmith to track the system-level performance of their application (like latency and cost), track the model/chain performance (through associating feedback with runs), debug issues (diving into a particular run that went wrong), and establish a broad understanding of how users are interacting with their application and what their experience is like.\n\nAmbitious startups like Mendable, Multi-On and Quivr, who are already serving thousands of users are actively using LangSmith to not only monitor overall usage, but also use those insights to take action on critical issues.\n\n\u201cThanks to Langchain smith we were able to analyze our LLM calls, understand the performance of the different chain methods ( stuff vs reduce) for QA and improve upon it. It even helped us debug and understand errors we made. We are consistently using it to improve our prompt engineering and look forward to the new features,\u201d said Stan Girard, Head of GenAI at Theodo and creator of Quivr.\n\n\n\nA unified platform\n\nWhile each of these product areas provide unique value, often at a specific point in time in the development process, we believe a great deal of the long term impact of LangSmith will come from having a single, fully-integrated hub to do this work from. We see teams with all kinds of Rube Goldberg-machine-like processes for managing their LLM applications, and we want to make that a thing of the past.\n\nAs a very simple example, we considered it to be table stakes for LangSmith to help users easily create datasets from existing logs and use them immediately for testing and evaluation, seamlessly connecting the logging/debugging workflows to the testing/evaluation ones.\n\n\n\nFintual, a Latin American startup with", "start_char_idx": 4968, "end_char_idx": 10179, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe3f1d63-9531-4c4f-967a-fb91d26d77e5": {"__data__": {"id_": "fe3f1d63-9531-4c4f-967a-fb91d26d77e5", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_announcing-langsmith_.txt", "file_name": "blog.langchain.dev_announcing-langsmith_.txt", "file_type": "text/plain", "file_size": 11758, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f2b6468-fca3-4e90-a794-75fbd0ccfa6f", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_announcing-langsmith_.txt", "file_name": "blog.langchain.dev_announcing-langsmith_.txt", "file_type": "text/plain", "file_size": 11758, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "d4f70bd51ab9daee4a5b2652d295b080fdac3d720b383021c9f1dad5247fac5c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b72d55ca-44aa-4650-914f-3ec67ad30e38", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_announcing-langsmith_.txt", "file_name": "blog.langchain.dev_announcing-langsmith_.txt", "file_type": "text/plain", "file_size": 11758, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "f86e075c4805beda4c520c0fc9f6679b9995d40830b2640be9e1c45c1200f6b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2aa4a30-0e28-411e-9ccb-e5fc09bfde24", "node_type": "1", "metadata": {}, "hash": "0c14e3f68135911b40dbe67cd2b776c65981665400110b8cd0bf93c3b3452c1e", "class_name": "RelatedNodeInfo"}}, "hash": "871a4b3fb42bae3349398b4f2df08ebc86214a3c9d299106386e1d282b8787be", "text": "logging/debugging workflows to the testing/evaluation ones.\n\n\n\nFintual, a Latin American startup with big dreams to help their citizens build wealth through a personalized financial advisor, found LangSmith early in their LLM development journey. \u201cAs soon as we heard about LangSmith, we moved our entire development stack onto it. We could build the evaluation, testing, and monitoring tools we needed in-house. But it would be 1000x worse, take us 10x longer, and require a team 2x the size,\u201d said Fintual leader Jose Pena.\n\n\u201cBecause we are building financial products, the bar for accuracy, personalization, and security is particularly high. LangSmith helps us build products we are confident putting in front of users.\u201d\n\nWe can\u2019t wait to bring these benefits to more teams. And we\u2019ve got a long list of features on the roadmap like analytics, playgrounds, collaboration, in-context learning, prompt creation, and more.\n\nFinally, we recognize that we cannot build ALL the functionality you will need to make it easy to make your applications production ready today. We\u2019ve made it possible to export datasets in the format OpenAI evals expects so you can contribute them there. This data can be also used directly to fine tune your models on the Fireworks platform (and we aim to make it easy to plug into other fine-tuning systems as well). Finally, we\u2019ve made logs exportable in a generic format and worked with teams like Context to ensure that you can load them into their analytics engine and run analytics over them in there.\n\n\n\nWe can\u2019t wait to see what you build.", "start_char_idx": 10078, "end_char_idx": 11652, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2aa4a30-0e28-411e-9ccb-e5fc09bfde24": {"__data__": {"id_": "a2aa4a30-0e28-411e-9ccb-e5fc09bfde24", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_announcing-langsmith_?ref=commandbar.ghost.io.txt", "file_name": "blog.langchain.dev_announcing-langsmith_?ref=commandbar.ghost.io.txt", "file_type": "text/plain", "file_size": 11782, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a802dfb1-a085-4e15-84e5-a84ddae7da05", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_announcing-langsmith_?ref=commandbar.ghost.io.txt", "file_name": "blog.langchain.dev_announcing-langsmith_?ref=commandbar.ghost.io.txt", "file_type": "text/plain", "file_size": 11782, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "cace5c24cbd35e1e745213e116e14f6b5f2abf1b3b172f9d1fd46e3b8cd71f13", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe3f1d63-9531-4c4f-967a-fb91d26d77e5", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_announcing-langsmith_.txt", "file_name": "blog.langchain.dev_announcing-langsmith_.txt", "file_type": "text/plain", "file_size": 11758, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "871a4b3fb42bae3349398b4f2df08ebc86214a3c9d299106386e1d282b8787be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "020a0fe4-9d3a-4b2a-a15b-2470686c646e", "node_type": "1", "metadata": {}, "hash": "a444e464e3bff7da0aaa2da5a92b4c0e802561a44f40623aa9609ebc088b0a87", "class_name": "RelatedNodeInfo"}}, "hash": "0c14e3f68135911b40dbe67cd2b776c65981665400110b8cd0bf93c3b3452c1e", "text": "URL: https://blog.langchain.dev/announcing-langsmith/?ref=commandbar.ghost.io\nTitle: Announcing LangSmith, a unified platform for debugging, testing, evaluating, and monitoring your LLM applications\n\nLangChain exists to make it as easy as possible to develop LLM-powered applications.\n\nWe started with an open-source Python package when the main blocker for building LLM-powered applications was getting a simple prototype working. We remember seeing Nat Friedman tweet in late 2022 that there was \u201cnot enough tinkering happening.\u201d The LangChain open-source packages are aimed at addressing this and we see lots of tinkering happening now (Nat agrees)\u2013people are building everything from chatbots over internal company documents to an AI dungeon master for a Dungeons and Dragons game.\n\nThe blocker has now changed. While it\u2019s easy to build a prototype of an application in ~5 lines of LangChain code, it\u2019s still deceptively hard to take an application from prototype to production. The main issue that we see today is application performance\u2013something that works ~30% of the time is good enough for a Twitter demo, but not nearly good enough for production.\n\nToday, we\u2019re introducing LangSmith, a platform to help developers close the gap between prototype and production. It\u2019s designed for building and iterating on products that can harness the power\u2013and wrangle the complexity\u2013of LLMs.\n\nLangSmith is now in closed beta. So if you\u2019re looking for a robust, unified, system for debugging, testing, evaluating, and monitoring your LLM applications, sign up here.\n\n\n\nHow did we get here?\n\nGiven the stochastic nature of LLMs, it is not easy\u2013and there\u2019s currently no straightforward way\u2013to answer the simple question of \u201cwhat\u2019s happening in these models?,\u201d let alone getting them to work reliably. The builders we hear from are running into the same roadblocks (and it\u2019s true for our team, too):\n\nUnderstanding what exactly the final prompt to the LLM call is (after all the prompt template formatting, this final prompt can be long and obfuscated)\n\nUnderstanding what exactly is returned from the LLM call at each step (before it is post-processed or transformed in any way)\n\nUnderstanding the exact sequence of calls to LLM (or other resources), and how they are chained together\n\nTracking token usage\n\nManaging costs\n\nTracking (and debugging) latency\n\nNot having a good dataset to evaluate their application over\n\nNot having good metrics with which to evaluate their application\n\nUnderstanding how users are interacting with the product\n\nAll of these problems have parallels in traditional software engineering. And, in response, a set of practices and tools for debugging, testing, logging, monitoring, etc. has emerged to help developers abstract away common infrastructure and focus on what really matters - building their applications. LLM application developers deserve the same.\n\nLangSmith aspires to be that platform. Over the last few months, we\u2019ve been working directly with some early design partners and testing it on our own internal workflows, and we\u2019ve found LangSmith helps teams in 5 core ways:\n\nDebugging\n\nLangSmith gives you full visibility into model inputs and output of every step in the chain of events. This makes it easy for teams to experiment with new chains and prompt templates, and spot the source of unexpected results, errors, or latency issues. We\u2019ll also expose latency and token usage so that you can identify which calls are causing issues.\n\nWe\u2019ve also made it easy to change and rerun examples from the UI. We added this feature after seeing teams take logs of bad examples and copy-paste into the OpenAI playground to tweak the prompt until they got a good result. We wanted to eliminate that friction, and now with the click of a button, you can go from a log to a playground where you can actively edit. This is currently supported for both OpenAI and Anthropic models, with support for more coming soon. We\u2019re also working on supporting this for chains in general.\n\nThis deep visibility into model performance has been particularly helpful for teams developing complex applications. LangSmith helped Streamlit and Snowflake implement agents that could intelligently and reliably answer questions about their data.\n\n\"LangChain has been instrumental in helping us prototype intelligent agents at Snowflake,\u201d said Adrien Treuille, Director of Product at Snowflake. \u201cLangSmith was easy to integrate, and the agnostic open source API made it very flexible to adapt to our implementation,\u201d tacked on Richard Meng, Senior Software Engineer at Snowflake.\n\nBoston Consulting Group also built a highly-customized, and highly performant, series of applications on top of LangChain\u2019s framework by relying on this same infrastructure.\n\n\u201cWe are proud of being one of the early LangChain design partners and users of LangSmith,\u201d Said Dan Sack, Managing Director and Partner at BCG. \u201cThe use of LangSmith has been key to bringing production-ready LLM applications to our clients.", "start_char_idx": 0, "end_char_idx": 5004, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "020a0fe4-9d3a-4b2a-a15b-2470686c646e": {"__data__": {"id_": "020a0fe4-9d3a-4b2a-a15b-2470686c646e", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_announcing-langsmith_?ref=commandbar.ghost.io.txt", "file_name": "blog.langchain.dev_announcing-langsmith_?ref=commandbar.ghost.io.txt", "file_type": "text/plain", "file_size": 11782, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a802dfb1-a085-4e15-84e5-a84ddae7da05", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_announcing-langsmith_?ref=commandbar.ghost.io.txt", "file_name": "blog.langchain.dev_announcing-langsmith_?ref=commandbar.ghost.io.txt", "file_type": "text/plain", "file_size": 11782, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "cace5c24cbd35e1e745213e116e14f6b5f2abf1b3b172f9d1fd46e3b8cd71f13", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2aa4a30-0e28-411e-9ccb-e5fc09bfde24", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_announcing-langsmith_?ref=commandbar.ghost.io.txt", "file_name": "blog.langchain.dev_announcing-langsmith_?ref=commandbar.ghost.io.txt", "file_type": "text/plain", "file_size": 11782, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "0c14e3f68135911b40dbe67cd2b776c65981665400110b8cd0bf93c3b3452c1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e2febac-0ca8-409e-aa17-26a274a3716a", "node_type": "1", "metadata": {}, "hash": "071a7f38ae2c1685b8c8d5f31246ff88a1986b5480c580713914720a4bea7022", "class_name": "RelatedNodeInfo"}}, "hash": "a444e464e3bff7da0aaa2da5a92b4c0e802561a44f40623aa9609ebc088b0a87", "text": "\u201cThe use of LangSmith has been key to bringing production-ready LLM applications to our clients. LangSmith's ease of integration and intuitive UI enabled us to have an evaluation pipeline up and running very quickly. Additionally, tracing and evaluating the complex agent prompt chains is much easier, reducing the time required to debug and refine our prompts, and giving us the confidence to move to deployment.\u201d\n\nIn another example of debugging in action, we partnered with DeepLearningAI to equip learners in the recently-released LangChain courses with access to LangSmith. This allowed students to easily visualize the exact sequence of calls, and the inputs and outputs at each step in the chain with precision. Students can understand exactly what the chains, prompts, and LLMs were doing, which helps build intuition as they learn to create new and more sophisticated applications.\n\n\n\nTesting\n\nOne of the main questions we see developers grapple with is: \u201cIf I change this chain/prompt, how does that affect my outputs?\u201d The most effective way to answer this question is to curate a dataset of examples that you care about, and then run any changed prompts/chains over this dataset. LangSmith first makes it easy to create these datasets from traces or by uploading datasets you\u2019ve curated manually. You can then easily run chains and prompts over those data sets.\n\nThe first helpful step is simply manually looking at the new inputs and outputs. Although this may seem unsatisfyingly basic, it actually has some benefits - many of the companies we\u2019ve spoken to actually like some manual touch points because it allows them to develop better intuition about how to interact with LLMs. This intuition can prove incredibly valuable when trying to think about how to improve the application. The main unlock we hope to provide is a clear interface for letting developers easily see the inputs and outputs for each data point, as without that visibility they cannot build up that intuition.\n\nToday, we primarily hear from teams that want to bring their prototype into production, and are narrowing in on specific prompts they\u2019d like to improve. Klarna is building industry-leading AI integrations that go beyond a simple call to a language model, and instead rely on a series of calls. As they focus on a specific section, LangSmith has provided the tools and data they need to ensure no regressions occur.\n\nIn parallel, we\u2019re starting to hear from more and more ambitious teams that are striving for a more effective approach.\n\nEvaluating\n\nLangSmith integrates seamlessly with our open source collection of evaluation modules. These modules have two main types of evaluation: heuristics and LLMs. Heuristic Evaluations will use logic like regexes to evaluate the correctness of an answer. LLM evaluations will use LLMs to evaluate themselves.\n\nWe are extremely bullish on LLM assisted evaluation over the long term. Critics of this approach will say that it\u2019s conceptually shaky and practically costly (time and money). But, we\u2019ve been seeing some very compelling evidence come out of top labs that this is a viable strategy. And, as we collectively make improvements to these models\u2013both private and open source\u2013and usage becomes more ubiquitous, we expect costs to come down considerably.\n\nMonitoring\n\nWhile debugging, testing, and evaluating can help you get from prototype to production, the work doesn\u2019t stop once you ship. Developers need to actively track performance, and ideally, optimize that performance based on feedback. We consistently see developers relying on LangSmith to track the system-level performance of their application (like latency and cost), track the model/chain performance (through associating feedback with runs), debug issues (diving into a particular run that went wrong), and establish a broad understanding of how users are interacting with their application and what their experience is like.\n\nAmbitious startups like Mendable, Multi-On and Quivr, who are already serving thousands of users are actively using LangSmith to not only monitor overall usage, but also use those insights to take action on critical issues.\n\n\u201cThanks to Langchain smith we were able to analyze our LLM calls, understand the performance of the different chain methods ( stuff vs reduce) for QA and improve upon it. It even helped us debug and understand errors we made. We are consistently using it to improve our prompt engineering and look forward to the new features,\u201d said Stan Girard, Head of GenAI at Theodo and creator of Quivr.\n\n\n\nA unified platform\n\nWhile each of these product areas provide unique value, often at a specific point in time in the development process, we believe a great deal of the long term impact of LangSmith will come from having a single, fully-integrated hub to do this work from. We see teams with all kinds of Rube Goldberg-machine-like processes for managing their LLM applications, and we want to make that a thing of the past.\n\nAs a very simple example, we considered it to be table stakes for LangSmith to help users easily create datasets from existing logs and use them immediately for testing and", "start_char_idx": 4908, "end_char_idx": 10063, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e2febac-0ca8-409e-aa17-26a274a3716a": {"__data__": {"id_": "1e2febac-0ca8-409e-aa17-26a274a3716a", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_announcing-langsmith_?ref=commandbar.ghost.io.txt", "file_name": "blog.langchain.dev_announcing-langsmith_?ref=commandbar.ghost.io.txt", "file_type": "text/plain", "file_size": 11782, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a802dfb1-a085-4e15-84e5-a84ddae7da05", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_announcing-langsmith_?ref=commandbar.ghost.io.txt", "file_name": "blog.langchain.dev_announcing-langsmith_?ref=commandbar.ghost.io.txt", "file_type": "text/plain", "file_size": 11782, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "cace5c24cbd35e1e745213e116e14f6b5f2abf1b3b172f9d1fd46e3b8cd71f13", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "020a0fe4-9d3a-4b2a-a15b-2470686c646e", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_announcing-langsmith_?ref=commandbar.ghost.io.txt", "file_name": "blog.langchain.dev_announcing-langsmith_?ref=commandbar.ghost.io.txt", "file_type": "text/plain", "file_size": 11782, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "a444e464e3bff7da0aaa2da5a92b4c0e802561a44f40623aa9609ebc088b0a87", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9833fb8d-412e-4fa5-9c5f-9416401c8c55", "node_type": "1", "metadata": {}, "hash": "a466bbfb70b5f1584814672a661f455d4655e88a3819ecfec1c14a48a11dcbab", "class_name": "RelatedNodeInfo"}}, "hash": "071a7f38ae2c1685b8c8d5f31246ff88a1986b5480c580713914720a4bea7022", "text": "stakes for LangSmith to help users easily create datasets from existing logs and use them immediately for testing and evaluation, seamlessly connecting the logging/debugging workflows to the testing/evaluation ones.\n\n\n\nFintual, a Latin American startup with big dreams to help their citizens build wealth through a personalized financial advisor, found LangSmith early in their LLM development journey. \u201cAs soon as we heard about LangSmith, we moved our entire development stack onto it. We could build the evaluation, testing, and monitoring tools we needed in-house. But it would be 1000x worse, take us 10x longer, and require a team 2x the size,\u201d said Fintual leader Jose Pena.\n\n\u201cBecause we are building financial products, the bar for accuracy, personalization, and security is particularly high. LangSmith helps us build products we are confident putting in front of users.\u201d\n\nWe can\u2019t wait to bring these benefits to more teams. And we\u2019ve got a long list of features on the roadmap like analytics, playgrounds, collaboration, in-context learning, prompt creation, and more.\n\nFinally, we recognize that we cannot build ALL the functionality you will need to make it easy to make your applications production ready today. We\u2019ve made it possible to export datasets in the format OpenAI evals expects so you can contribute them there. This data can be also used directly to fine tune your models on the Fireworks platform (and we aim to make it easy to plug into other fine-tuning systems as well). Finally, we\u2019ve made logs exportable in a generic format and worked with teams like Context to ensure that you can load them into their analytics engine and run analytics over them in there.\n\n\n\nWe can\u2019t wait to see what you build.", "start_char_idx": 9946, "end_char_idx": 11676, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9833fb8d-412e-4fa5-9c5f-9416401c8c55": {"__data__": {"id_": "9833fb8d-412e-4fa5-9c5f-9416401c8c55", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_automating-web-research_.txt", "file_name": "blog.langchain.dev_automating-web-research_.txt", "file_type": "text/plain", "file_size": 4728, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ff022234-d71d-46ad-956b-0a001e38d97a", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_automating-web-research_.txt", "file_name": "blog.langchain.dev_automating-web-research_.txt", "file_type": "text/plain", "file_size": 4728, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "fe4b35ef9dc0f409fcb29c4b38914566b04c92d4c3023988ee3206a6eb09ee56", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e2febac-0ca8-409e-aa17-26a274a3716a", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_announcing-langsmith_?ref=commandbar.ghost.io.txt", "file_name": "blog.langchain.dev_announcing-langsmith_?ref=commandbar.ghost.io.txt", "file_type": "text/plain", "file_size": 11782, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "071a7f38ae2c1685b8c8d5f31246ff88a1986b5480c580713914720a4bea7022", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "876129a9-00b7-489b-9b74-e4d49a211895", "node_type": "1", "metadata": {}, "hash": "e34ff3b6fd3083209f6c920abdc6d9beeed88c82571bc3d3d4267e95456f85eb", "class_name": "RelatedNodeInfo"}}, "hash": "a466bbfb70b5f1584814672a661f455d4655e88a3819ecfec1c14a48a11dcbab", "text": "URL: https://blog.langchain.dev/automating-web-research/\nTitle: Automating Web Research\n\nMotivation\n\nWeb research is one of the killer LLM applications: Greg Kamradt highlighted it as one of his top desired AI tools and OSS repos like gpt-researcher are growing in popularity. We decided to take a stab at it, initially setting out like many others to build a web research agent. But, we landed somewhere different: a fairly simple retriever proved to be effective and easily configurable (e.g., to run in private mode as popularized by projects like PrivateGPT) . In this blog we talk about our exploration and thought process, how we built it, and the next steps.\n\nExploration\n\nAbovementioned projects like gpt-researcher and AI search engines (perplexity.ai) offer an early glimpse into how web research may be re-imagined. Like many, we first devised an agent that could be given a prompt, a set of tools, and then would set forth to scour the web autonomously! For this, it clearly needed tools to:\n\nSearch and return pages\n\nScrape the full content of the pages returned\n\nExtract relevant information from the pages\n\nWith those tools, the agent could approximate what a human does: search a topic, choose selected links, skim the link for useful pieces of information, and return to the search in an iterative exploration. We made an agent, gave it these tools ... but found it slowly fumbled thought the iterative search process, much like a human!\n\nImprovements\n\nWe noticed a central advantage that AIs can uniquely exploit: kick off many searches in parallel and, in turn, \"read\" many pages in parallel. Of course, this risks inefficiency if the first article in a sequential search has all of the necessary information. But for complex questions that warrant an AI researcher, this risk is somewhat mitigated. We added some basic tools to support this process.\n\nWith a heap of information collected in parallel from a set of pages, it seemed reasonable to fetch the most relevant chunks from each page and load them into the context window of an LLM for synthesis. Of course, at this point we realized that our agent was morphing into a retriever! (NOTE: we still think that agentic properties can further benefit this retriever, as discussed at the end.)\n\nRetrieval\n\nWhat exactly would this retriever do under the hood? Our thinking was:\n\nUse an LLM to generate multiple relevant search queries (one LLM call)\n\nExecute a search for each query\n\nChoose the top K links per query (multiple search calls in parallel)\n\nLoad the information from all chosen links (scrape pages in parallel)\n\nIndex those documents into a vectorstore\n\nFind the most relevant documents for each original generated search query\n\nCollectively, these steps fall into the flow used for retrieval augmented generation:\n\nAnd yet the logic is similar to the agentic architecture for gpt-researcher:\n\nEven though this isn't an agent, the similarity in logic is a useful sanity check on the approach. We created a new LangChain retriever and provide documentation on usage with configurations. For an example question (How do LLM Powered Autonomous Agents work?), we can use LangSmith to visualize and validate the process (see trace here), observing that the retriever loads and retrieves chunks from a reasonable source (Lilian Weng's blog post on agents):\n\nAs noted in the documentation, the same process can be trivially configured to run it \"private\" mode using, for example, LlamaV2 and GPT4all embeddings (below is a trace from a run executed on my Mac M2 Max GPU ~50 tok / sec):\n\nApplication\n\nWe wrapped the retriever with a simple Streamlit IU (only ~50 lines of code here) that can be configured with any LLM, vectorstore, and search tool of choice.\n\nConclusion\n\nWhat started as an attempt to build an autonomous web research agent, evolved into a fairly simple / efficient and customizable retriever. Still, this was just a first step. This project could benefit from adding in many agentic properties, such as:\n\nAsking an LLM if more information is needed after the initial search\n\nUsing multiple \"write\" and \"revision\" agents to construct the final answer\n\nIf any of those additions sound interesting, please open a PR against the base repo and we'll work with you to get them in!\n\nWhile hosted AI search from large models like Bard or Perplexity.ai are extremely performant, smaller lightweight tools for web research also have important merits such as privacy (e.g., the ability to run locally on your laptop without sharing any data externally), configurability (e.g., the ability to select the specific open source components to use), and observability (e.g., peer into what is happening \"under the hood\" using tools such as LangSmith).", "start_char_idx": 0, "end_char_idx": 4728, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "876129a9-00b7-489b-9b74-e4d49a211895": {"__data__": {"id_": "876129a9-00b7-489b-9b74-e4d49a211895", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt", "file_name": "blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt", "file_type": "text/plain", "file_size": 18365, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ef09ca3-d0d7-4d56-904b-77675ada0e6c", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt", "file_name": "blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt", "file_type": "text/plain", "file_size": 18365, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "360e9eb10a04bdc75ce22222575bf038918b74ce18164b8bdb7a1cf8aceccd0a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9833fb8d-412e-4fa5-9c5f-9416401c8c55", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_automating-web-research_.txt", "file_name": "blog.langchain.dev_automating-web-research_.txt", "file_type": "text/plain", "file_size": 4728, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "a466bbfb70b5f1584814672a661f455d4655e88a3819ecfec1c14a48a11dcbab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb50f1df-d98b-4a8c-8c47-f427c196cc65", "node_type": "1", "metadata": {}, "hash": "be6b33e2d28255da98841cd5e995aa32a4fc139b17f18109f6423d66313fa9e1", "class_name": "RelatedNodeInfo"}}, "hash": "e34ff3b6fd3083209f6c920abdc6d9beeed88c82571bc3d3d4267e95456f85eb", "text": "URL: https://blog.langchain.dev/benchmarking-question-answering-over-csv-data/\nTitle: Benchmarking Question/Answering Over CSV Data\n\nThis is a bit of a longer post. It's a deep dive on question-answering over tabular data. We discuss (and use) CSV data in this post, but a lot of the same ideas apply to SQL data. It covers:\n\nBackground Motivation: why this is an interesting task\n\nInitial Application: how we set up a simple Streamlit app in order to gather a good distribution of real questions\n\nInitial Solution: our initial solution and some conceptual considerations\n\nDebugging with LangSmith: what we saw people asking, and what issues our initial solution had\n\nEvaluation Setup: how we evaluated solutions\n\nImproved Solution: the final improved solution we arrived at\n\nAs a sneak preview, the improved solution we arrived at was a custom agent that used OpenAI functions and had access to two tools: a Python REPL and a retriever.\n\nWe've open-sourced everything - the app we used to gather feedback, the dataset, the eval script - at this repo. We've also made a YouTube video walking through the content in this blog, if that's more your style.\n\nBackground Motivation\n\nThere's a pretty standard recipe for question over text data at this point. On the other hand, one area where we've heard consistent asks for improvement is with regards to tabular (CSV) data. Lots of enterprise data is contained in CSVs, and exposing a natural language interface over it can enable easy insights. The problem is that it's far less clear how to accomplish this.\n\nA few weeks ago we decided to focus on this for a bit but quickly ran into an issue\u2013we didn't really know what types of questions people expected to be able ask CSV data, and we didn't have any good way to evaluate this applications.\n\n\ud83d\udca1 Evaluation of LLM applications is often hard because of a lack of data and a lack of metrics.\n\nIn traditional machine learning you usually start with a dataset of inputs and outputs, and you use this to train and then evaluate your model. However, because LLMs are fantastic zero shot learners, it is now possible to use a prompt to quickly build an application based on just an idea, and no data. While this is incredibly powerful in terms of enabling developers to build new applications quickly, it leads to difficulty in evaluating because you lack that data. This is why we've built LangSmith in a way where constructing datasets is as easy as possible.\n\nLikewise, there's often not great metrics for evaluating LLM applications. The outputs are often natural language, and traditional NLP metrics like BLEU and ROUGE aren't great. But what is good at understanding natural language? LLMs! We're pretty bullish on LLM assisted evaluation and have invested in building a bunch of evaluators that use LLMs to do the evaluation.\n\nSo how did we apply these ideas to our task of creating a better application for answering questions over tabular data? We'll dive into these in more detail in the next sections, but at a high level we:\n\nUsed LangSmith to flag interesting datapoints and used that to construct a dataset of examples\n\nUsed LLMs to evaluate correctness\n\nInitial Application\n\nFirst, we set about creating a dataset of questions and ground truth answers. Part of the issue here was didn't even know what type of questions people would want to ask of their tabular data. We could have made some educated guesses, or tried to generate synthetic questions to ask. But we wanted to optimize instead for real questions, as we also wanted to do a bit of exploration here into what types of questions real users would want to ask.\n\n\ud83d\udca1 Before you launch an app it can be tough to guess how users may interact with it. Rather than guessing, one strategy is launch quickly and early and gather real data.\n\nIn order to do this we decided to spin up a quick demo application and put that out in the wild. We would then log actual user questions along with any feedback about the answers that they gave us. To gather feedback we added a simple \"thumps up\"/\"thumbs down\" button to the application. We would use LangSmith to monitor all the interactions and feedback, and then we would manually review the interactions and create a dataset consisting of any interesting ones. This is done easily from the LangSmith UI - there is an \"Add to Dataset\" button on all logs.\n\nThere's also the question of what type of data we wanted to gather. We considered two approaches: (1) let users upload their own CSV and ask questions of that, (2) fix the CSV and gather questions over that. We opted for (2) for a few reasons. First - it would make it simpler for people to play around with, likely leading to more responses. Second - it would probably make it easier", "start_char_idx": 0, "end_char_idx": 4743, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb50f1df-d98b-4a8c-8c47-f427c196cc65": {"__data__": {"id_": "fb50f1df-d98b-4a8c-8c47-f427c196cc65", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt", "file_name": "blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt", "file_type": "text/plain", "file_size": 18365, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ef09ca3-d0d7-4d56-904b-77675ada0e6c", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt", "file_name": "blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt", "file_type": "text/plain", "file_size": 18365, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "360e9eb10a04bdc75ce22222575bf038918b74ce18164b8bdb7a1cf8aceccd0a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "876129a9-00b7-489b-9b74-e4d49a211895", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt", "file_name": "blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt", "file_type": "text/plain", "file_size": 18365, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "e34ff3b6fd3083209f6c920abdc6d9beeed88c82571bc3d3d4267e95456f85eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ac2c396-47c3-482f-8ef5-a38859334be4", "node_type": "1", "metadata": {}, "hash": "15112e330de837cd4a2806ee42f30c170a2a7deb00eb460fa88bea3617caa9c0", "class_name": "RelatedNodeInfo"}}, "hash": "be6b33e2d28255da98841cd5e995aa32a4fc139b17f18109f6423d66313fa9e1", "text": "people to play around with, likely leading to more responses. Second - it would probably make it easier to evaluate. Third - we specifically wanted to be logging and looking at user questions, and we didn't want to do this over any confidential CSV that someone might upload. However, this does have several downsides. We would have to choose a CSV to use, and this CSV may not be representative of other CSVs - both in the size and shape of the data, as well as the questions people may want to ask of it.\n\nFor our example application we chose the classic Titanic dataset - a record of all passengers on the Titanic and whether the survived, often used for example data science projects. We created this simple application in Streamlit, put it out in the world, and asked people to give feedback. You can view the hosted app here, and the source code here.\n\nThrough this, we gathered ~400 interactions. Of those, about 200 had some form of feedback. Using LangSmith, we drilled into datapoints with bad feedback (and some with good) and manually labeled them and added them to a dataset we created. We did this until we had about 50 datapoints.\n\nNow it was time to improve our system! Before talking about how we improved, let's first discuss (1) what the initial system was (2) what issues it had, and (3) how we would evaluate the system to measure any improvements.\n\nInitial Solution\n\nThe Titanic dataset has a mix of columns in it. Some of them are numeric (age, number of siblings, fare), some of them are categorical (station embarked, cabin) and there's one text column (name).\n\nWhile a person's name isn't super text heavy, it is still text heavy enough to cause some issues. For example if a question is asked about \"John Smith\", there a bunch of variants of how that name could be represented: Mr. John Smith (title), Smith, John (order), Jon Smith (typo), John Jacob Smith (middle name), etc. This can make it tricky to filter rows exactly by name, or even do lookups. Therefor, from the start we knew we had to include some more fuzzy based functionality. However, we also guessed that people would want to ask some questions about aggregations (\"who paid the most for their fare\") or the like, and so we probably need some functionality to do that.\n\n\ud83d\udca1 Tabular data that contains text can be particularly tough to deal with, as retrieval is likely needed in some form, but pure retrieval probably isn't enough.\n\nRetrieval\n\nFor the natural language bit, we wanted to use a traditional retrieval system. We weren't going to get too fancy, so we just wanted to use a simple vectorstore and look up results based on cosine similarity with the input question.\n\nIn order to do this we needed to load a CSV into a vectorstore. We did this using the logic of our CSVLoader. What this does under the hood is:\n\nLoad each row as its own document Represent the text of each document as a list of Column: value pairs, each on their own line.\n\nDigging into point (2) a bit more, there's a few ways you could represent a row of CSV as a document. You could represent it as JSON, as a CSV, or - as we ended up doing - a formatted piece of text. Very concretely, if you had a CSV row with the following values: {\"col1\": \"foo\", \"col2\": \"bar\"} what this ends up looking like after you format it is:\n\ncol1: foo col2: bar\n\nWhile this may not seem all that interesting, a BIG part of LLM applications is proper data engineering to communicate data to the LLM most effectively. Anecdotally, we've found this representation of tabular (and also JSON) data to be most efficient when the values could contain textual values.\n\nQuery Language\n\nAside from retrieval, we also figured people would want to ask questions that required some type of query language. For example - \"who paid the most for their fare\". There are two approaches we considered here.\n\nFirst, we considered using a Python REPL and asking the language model to write code to help answer the user's question. This has the benefit of being very flexible. This also has the downside of maybe being TOO flexible - it could enable execution of arbitrary code.\n\nSecond, we considered using kork to give access to a predetermined set of functions. kork is a library that basically whitelists a set of functions that can be used. It's less general - you have to declare all functions that can be run - but it's safer.\n\nTo start, we went with kork . We weren't entirely sure about what people would ask, so we defined a few functions (filter, sum, contains) and gave it access to that.\n\nOur", "start_char_idx": 4640, "end_char_idx": 9175, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ac2c396-47c3-482f-8ef5-a38859334be4": {"__data__": {"id_": "4ac2c396-47c3-482f-8ef5-a38859334be4", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt", "file_name": "blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt", "file_type": "text/plain", "file_size": 18365, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ef09ca3-d0d7-4d56-904b-77675ada0e6c", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt", "file_name": "blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt", "file_type": "text/plain", "file_size": 18365, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "360e9eb10a04bdc75ce22222575bf038918b74ce18164b8bdb7a1cf8aceccd0a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb50f1df-d98b-4a8c-8c47-f427c196cc65", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt", "file_name": "blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt", "file_type": "text/plain", "file_size": 18365, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "be6b33e2d28255da98841cd5e995aa32a4fc139b17f18109f6423d66313fa9e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02921ceb-9fc4-4d70-8ccb-a0c9566b6a05", "node_type": "1", "metadata": {}, "hash": "36ad9873f6a4cb596d625dc53f874aba178079606049e63d48dc5291ebb41d8a", "class_name": "RelatedNodeInfo"}}, "hash": "15112e330de837cd4a2806ee42f30c170a2a7deb00eb460fa88bea3617caa9c0", "text": "we defined a few functions (filter, sum, contains) and gave it access to that.\n\nOur first solution ran retrieval and kork in parallel, and then combined the answers.\n\nDebugging with LangSmith\n\nPeople started asking questions and the feedback starting rolling in. Only about 1/3 of feedback was positive. What was going wrong? There was two main sources of errors:\n\nData Formatting\n\nA lot of the functions we wrote for kork would return a dataframe. This dataframe was then inserted into a prompt and passed to the language model. There was then a question of how that dataframe was formatted as a string to be passed to the language model.\n\nThis was important for answering questions like Who was in cabin C128 . The returned dataframe would have hopefully filtered to the correct row and be returning all relevant information. Before we launched the app, we tested questions like this and it was working fine. However, after we launched the app and started to look at the responses we noticed it was failing terribly at a large number of these types of questions.\n\nWe used LangSmith to inspect the traces to try to get a sense of what was going on. We could see that the correct query was being generated... but when that dataframe was passed into the prompt the formatting was being messed up. We expected it look something like:\n\nBut instead it was looking something like:\n\nAfter some more debugging, we discovered that how a dataframe is represented as string may change depending on what platform you are on. In this case, it was being represented differently locally compared to Streamlit cloud. After some more debugging, we figured out that we could fix that inconsistency by specifying some parameters:\n\npd.set_option('display.max_rows', 20) pd.set_option('display.max_columns', 20)\n\nDoing this fixed a lot of our issues! It also shows how LangSmith can be extremely helpful in debugging LLM issues. The main parts of bringing an LLM application from prototype to production are prompt engineering and data engineering. Understand what exactly the data looks like when you are passing it to an LLM is crucial for debugging performance issues. We've heard from several users of LangSmith who have found these types of data engineering issues only after using LangSmith to inspect the exact inputs to LLMs more carefully.\n\n\ud83d\udca1 If data is not passed to the language model in a clear way, it will make it really tricky for the language model to reason about it correcting. Using LangSmith to make sure the final text looks reasonable, and debug any data processing steps, is a great way to catch any bugs here.\n\nLimited kork Functionality\n\nIt turns out the set of functions we gave to kork was not NEARLY enough to cover the long tail of questions users would ask.\n\nThere are two potential fixes to this. One, we could try to add more functions to kork . Second, we could revert to using a Python REPL.\n\nEvaluation Setup\n\nSo we've now constructed our dataset of real world examples. We've also done some manual debugging and identified some areas of errors and have some ideas for how to improve. How exactly do we go about measuring whether we've improved?\n\nFor an example of why this is non-trivial, let's consider the question Who is in cabin C128 . The correct answer in the CSV is Williams-Lambert, Mr. Fletcher Fellows . But there are a LOT of ways a language model could respond that should be considered \"correct\":\n\nMr. Fletcher Fellows Williams-Lambert\n\nThe person in cabin C128 was Mr. Fletcher Fellows Williams-Lambert.\n\nFletcher Williams-Lambert\n\nMr. Williams-Lambert was in that cabin\n\nIn order to properly evaluate these natural language answers... we turned to a language model. We decided to use our standard qa evaluator, which takes as input:\n\nThe input\n\nThe ground truth answer\n\nA predicted answer\n\nFrom there, it formats a prompt template with those values and passes that to a language model to get back a response.\n\nEven still, this is NOT perfect. For example, one of the questions we evaluated on was male to female ratio? . It's pretty unclear what the answer to that question should be. We had labelled the answer as There were 577 males and 314 females, for a ratio of 1.84 . In one test run, the language model responded The ratio of males to females in the dataframe is approximately 0.65 to 0.35. This means that there are about 65% males and 35% females . Our LLM evaluator marked that answer as INCORRECT, even though it probably likely correct.\n\nDoes this mean there is no use for an LLM evaluator? We do not believe so. Rather, we believe that LLM evaluators are still useful. For starters, they are markedly better than other \"general\" evaluation methods we've tried. Secondly, even if occasionally correct that can be totally fine", "start_char_idx": 9092, "end_char_idx": 13861, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02921ceb-9fc4-4d70-8ccb-a0c9566b6a05": {"__data__": {"id_": "02921ceb-9fc4-4d70-8ccb-a0c9566b6a05", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt", "file_name": "blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt", "file_type": "text/plain", "file_size": 18365, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ef09ca3-d0d7-4d56-904b-77675ada0e6c", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt", "file_name": "blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt", "file_type": "text/plain", "file_size": 18365, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "360e9eb10a04bdc75ce22222575bf038918b74ce18164b8bdb7a1cf8aceccd0a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ac2c396-47c3-482f-8ef5-a38859334be4", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt", "file_name": "blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt", "file_type": "text/plain", "file_size": 18365, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "15112e330de837cd4a2806ee42f30c170a2a7deb00eb460fa88bea3617caa9c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "103355cf-a015-4bdd-94d3-aa7bb6572b16", "node_type": "1", "metadata": {}, "hash": "825602d0dbeeb30b084440e5d9bf09f79f86d5cb992a566c45a1eadfe67af592", "class_name": "RelatedNodeInfo"}}, "hash": "36ad9873f6a4cb596d625dc53f874aba178079606049e63d48dc5291ebb41d8a", "text": "\"general\" evaluation methods we've tried. Secondly, even if occasionally correct that can be totally fine if you're not treating the grades as gospel. For example - don't blindly accept the LLM scores, but rather treat them as indications of where it may be worth looking. Even if you still need to do human evaluation on some data points, using LLM assisted evaluation can help guide you to the most interesting datapoint to look at.\n\n\ud83d\udca1 Evaluating LLM output using LLMs is NOT perfect, but we think this is currently the best available solution and are bullish on it in the long run.\n\nImproved Solution\n\nFinally, we arrive at the exciting part of the blog. Did we manage to improve our solution? And how did we do so?\n\nOur final solution is:\n\nAn agent powered by OpenAIFunctions ( OpenAIFunctionsAgent )\n\n) GPT-4\n\nTwo tools: a Python REPL and a retriever\n\nA custom prompt with custom instructions on how to think about when to use the Python REPL vs the retriever\n\nThis provides several benefits. First, by giving it access to a Python REPL we give it the ability to do all sorts of queries and analysis. However, as we'll see in some of the comparisons below, the Python REPL can have issues when dealing with text data - in this case the Name column. That is where the retriever can come in handy.\n\n\ud83d\udca1 Our final solution is an agent with two tools: a Python REPL and a retriever. This allows it to answer questions about the unstructured text, but also perform more traditional data analysis operations.\n\nNote that we do include some instructions in the prompt specific to the Titanic dataset. Specifically, we tell it that it should try to use the retriever for the Name column and the Python REPL for most other things. We did this because with generic wording it was having some trouble reasoning about when to use it. This does mean that comparing to generic solutions (as we do below) is a bit unfair. As a follow up, we would love to see a more generic prompt presented that does not include dataset specific logic. However, we also believe that in order to really improve the performance of your application you will likely need to use a custom prompt and NOT rely on generic defaults.\n\nNow let's look at some results and compare to other methods. First, we compare to our standard Pandas Agent (using both GPT-3.5 as well as GPT-4). Next, we compare to PandasAI - one of the top open source libraries for interacting with Pandas DataFrames. A table of performance is below. Again, this is over 50 datapoints and some of the evaluations may not be 100% accurate, so we'll also present some concrete examples after the fact.\n\nNote: these were all run on LangSmith. We're working on making evaluation runs publicly sharable.\n\nThe Pandas Agent and PandasAI performed roughly the same. They struggled on questions involving people's names. For example, for the following question:\n\nHow many siblings does Carrie Johnston have?\n\nThe code generate is:\n\n# First, we need to find the row for Carrie Johnston carrie_johnston = df[df['Name'].str.contains('Carrie Johnston')] # Then, we can find the number of siblings she has num_siblings = carrie_johnston['SibSp'].values[0] num_siblings\n\nHowever, df[df['Name'].str.contains('Carrie Johnston')] does not return any rows because her name appears as Johnston, Miss. Catherine Helen \"Carrie\"\n\nLooking at the four example our custom agent gets wrong, we can see that a lot of the mistakes aren't that bad.\n\nIn one case it filters based on age (the ground truth answer we added had no filtering - maybe there should have been?)\n\nIn another case it stops listing after 10 - this is actually because the DataFrame when printed out didn't actually show the whole contents.\n\nIn a third case it just has a different interpretation of how to respond (but the facts look correct)\n\nAnd finally, it messes up because it uses the retriever to search for names, and the retriever is limited to four responses.\n\nConclusion\n\nWe're pretty satisfied with the final solution we arrived at - and most of the feedback had been positive as well. We're also pretty happy with the dataset we've put together and think it can be useful in evaluating these types of applications.\n\nAt the same time, we recognize that there is always room for improvements - on both fronts. The dataset can be improved/added to, the evaluators can likely be improved, and we're especially excited to see more solutions to this problem of question-answering over CSV data!\n\nWe've open-sourced everything - the app we used to gather feedback, the dataset, the eval script - at this repo.", "start_char_idx": 13756, "end_char_idx": 18345, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "103355cf-a015-4bdd-94d3-aa7bb6572b16": {"__data__": {"id_": "103355cf-a015-4bdd-94d3-aa7bb6572b16", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_building-chat-langchain-2_.txt", "file_name": "blog.langchain.dev_building-chat-langchain-2_.txt", "file_type": "text/plain", "file_size": 11423, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e34fe21a-43b6-4938-a0da-94ce8e6fe887", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_building-chat-langchain-2_.txt", "file_name": "blog.langchain.dev_building-chat-langchain-2_.txt", "file_type": "text/plain", "file_size": 11423, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "ef051310bd2a66bc6f27c9d8908ba7804134f25036a68b7ccccf92e7fa66dff0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02921ceb-9fc4-4d70-8ccb-a0c9566b6a05", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt", "file_name": "blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt", "file_type": "text/plain", "file_size": 18365, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "36ad9873f6a4cb596d625dc53f874aba178079606049e63d48dc5291ebb41d8a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ebaf87ff-868a-4436-a1ae-0c3f6f586cc2", "node_type": "1", "metadata": {}, "hash": "15cb24b38852337397948df8a96f674e699492ef10971e7cd405c94f1d98c773", "class_name": "RelatedNodeInfo"}}, "hash": "825602d0dbeeb30b084440e5d9bf09f79f86d5cb992a566c45a1eadfe67af592", "text": "URL: https://blog.langchain.dev/building-chat-langchain-2/\nTitle: Building Chat LangChain\n\nHosted: https://chat.langchain.com\n\nRepo: https://github.com/langchain-ai/chat-langchain\n\nIntro\n\nLangChain packs the power of large language models and an entire ecosystem of tooling around them into a single package. This consolidation ultimately simplifies building LLM applications, but it does mean that there are a lot of features to learn about.\n\nTo help folks navigate LangChain, we decided to use LangChain to explain LangChain.\n\nIn this post, we'll build a chatbot that answers questions about LangChain by indexing and searching through the Python docs and API reference. We call this bot Chat LangChain. In explaining the architecture we'll touch on how to:\n\nUse the Indexing API to continuously sync a vector store to data sources\n\nDefine a RAG chain with LangChain Expression Language (LCEL)\n\nEvaluate an LLM application\n\nDeploy a LangChain application\n\nMonitor a LangChain application\n\nBy the end, you'll see how easy it is to bootstrap an intelligent chatbot from scratch. The process can be adapted for other knowledge bases, too. Let's dive in!\n\nArchitecture\n\nIngestion\n\nTo perform RAG, we need to create an index over some source of information about LangChain that can be queried at runtime. Ingestion refers to the process of loading, transforming and indexing the relevant data source(s).\n\nSee https://python.langchain.com/docs/modules/data_connection for more on the building blocks of retrieval\n\nTo start we tried indexing the Python docs, API Reference and Python repo. We found that retrieving chunks of code directly was often less effective than more contextualized and verbose sources like the docs, so we dropped the codebase from our retrieval sources.\n\nThe final ingestion pipeline looks like this:\n\nLoad: SitemapLoader + RecursiveURLLoader \u2014 We first load in our docs by scraping the relevant web pages. We do this instead of loading directly from the repo because 1) parts of our docs and API reference are autogenerated from code and notebooks, and 2) this is a more generalizable approach.\n\nTo load the Python docs we used the SitemapLoader , which finds all the relevant links to scrape from a sitemap XML file. A lot of the legwork here is done by the langchain_docs_extractor method, which is an awesome custom HTML -> text parser contributed by Jes\u00fas V\u00e9lez Santiago:\n\ndocs = SitemapLoader( \"https://python.langchain.com/sitemap.xml\", filter_urls=[\"https://python.langchain.com/\"], parsing_function=langchain_docs_extractor, default_parser=\"lxml\", bs_kwargs={ \"parse_only\": SoupStrainer( name=(\"article\", \"title\", \"html\", \"lang\", \"content\") ), }, meta_function=metadata_extractor, ).load()\n\nTo load the API Reference (which doesn't have a very useful sitemap) we use a RecursiveUrlLoader , which recursively loads sublinks from a page up to a certain depth.\n\napi_ref = RecursiveUrlLoader( \"https://api.python.langchain.com/en/latest/\", max_depth=8, extractor=simple_extractor, prevent_outside=True, use_async=True, timeout=600, check_response_status=True, exclude_dirs=( \"https://api.python.langchain.com/en/latest/_sources\", \"https://api.python.langchain.com/en/latest/_modules\", ), ).load()\n\nTransform: RecursiveCharacterTextSplitter \u2014 By the time our documents are loaded, we've already done a good amount of HTML to text and metadata parsing. Some of the pages we've loaded are quite long, so we'll want to chunk them. This is important because 1) it can improve retrieval performance; similarity search might miss relevant documents if they also contain a lot of irrelevant information, 2) saves us having to worry about retrieved documents fitting in a model's context window.\n\nWe use a simple RecursiveCharacterTextSplitter to partition the content into approximately equally-sized chunks:\n\ntransformed_docs = RecursiveCharacterTextSplitter( chunk_size=4000, chunk_overlap=200, ).split_documents(docs + api_ref)\n\nEmbed + Store: OpenAIEmbeddings, Weaviate \u2014 To make sense of this textual data and enable effective retrieval, we leveraged OpenAI's embeddings. These embeddings allowed us to represent each chunk as a vector in the Weaviate vector store, creating a structured knowledge repository ready for retrieval.\n\nclient = weaviate.Client( url=WEAVIATE_URL, auth_client_secret=weaviate.AuthApiKey(api_key=WEAVIATE_API_KEY), ) embedding = OpenAIEmbeddings(chunk_size=200) vectorstore = Weaviate( client=client, index_name=WEAVIATE_DOCS_INDEX_NAME, text_key=\"text\", embedding=embedding, by_text=False, attributes=[\"source\", \"title\"], )\n\nIndexing + Record Management: SQLRecordManager \u2014", "start_char_idx": 0, "end_char_idx": 4622, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ebaf87ff-868a-4436-a1ae-0c3f6f586cc2": {"__data__": {"id_": "ebaf87ff-868a-4436-a1ae-0c3f6f586cc2", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_building-chat-langchain-2_.txt", "file_name": "blog.langchain.dev_building-chat-langchain-2_.txt", "file_type": "text/plain", "file_size": 11423, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e34fe21a-43b6-4938-a0da-94ce8e6fe887", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_building-chat-langchain-2_.txt", "file_name": "blog.langchain.dev_building-chat-langchain-2_.txt", "file_type": "text/plain", "file_size": 11423, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "ef051310bd2a66bc6f27c9d8908ba7804134f25036a68b7ccccf92e7fa66dff0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "103355cf-a015-4bdd-94d3-aa7bb6572b16", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_building-chat-langchain-2_.txt", "file_name": "blog.langchain.dev_building-chat-langchain-2_.txt", "file_type": "text/plain", "file_size": 11423, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "825602d0dbeeb30b084440e5d9bf09f79f86d5cb992a566c45a1eadfe67af592", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52d09477-5764-4389-a48b-7de925680984", "node_type": "1", "metadata": {}, "hash": "8751fbf0b7611ddc0c03234238972bd1fc245e5324035f64e00c762893dbd113", "class_name": "RelatedNodeInfo"}}, "hash": "15cb24b38852337397948df8a96f674e699492ef10971e7cd405c94f1d98c773", "text": "attributes=[\"source\", \"title\"], )\n\nIndexing + Record Management: SQLRecordManager \u2014 We want to be able to re-run our ingestion pipeline to keep the chatbot up-to-date with new LangChain releases and documentation. We also want to be able to improve our ingestion logic over time. To do this without having to re-index all of our documents from scratch every time, we use the LangChain Indexing API. This uses a RecordManager to track writes to any vector store and handles deduplication and cleanup of documents from the same source. For our purposes, we used a Supabase PostgreSQL-backed record manager:\n\nrecord_manager = SQLRecordManager( f\"weaviate/{WEAVIATE_DOCS_INDEX_NAME}\", db_url=RECORD_MANAGER_DB_URL ) record_manager.create_schema() indexing_stats = index( transformed_docs, record_manager, vectorstore, cleanup=\"full\", source_id_key=\"source\", )\n\nAnd voila! We've now created a query-able vector store index of our docs and API reference.\n\nContinuous ingestion\n\nWe add and improve LangChain features pretty regularly. To make sure our chatbot is up-to-date with the latest and greatest that LangChain has to offer, we need to regularly re-index the docs. To do this we added a scheduled Github Action to the repo that runs the ingestion pipeline daily: check it out here.\n\nQuestion-Answering\n\nOur question-answering chain has two simple components. First, we take the question, combine it with the past messages from the current chat session, and write a standalone search query. So if a user asks \u201cHow do I use the Anthropic LLM\u201d and follows up with \u201cHow about VertexAI\u201d, the chatbot might rewrite the last question as \u201cHow do I use the VertexAI LLM\u201d and use that to query the retriever instead of \u201cHow about VertexAI\u201d. You can see the prompt we use for rephrasing a question here.\n\ncondense_question_chain = ( PromptTemplate.from_template(REPHRASE_TEMPLATE) | llm | StrOutputParser() ).with_config( run_name=\"CondenseQuestion\", ) retriever_chain = condense_question_chain | retriever\n\nOnce we\u2019ve formulated the search query and retrieved relevant documents, we pass the original question, the chat history and the retrieved context to a model using this prompt. Notice the prompt instructs the model to cite its sources. This is done 1) to try and mitigate hallucinations, and 2) to make it easy for the user to explore the relevant documentation themselves.\n\n_context = RunnableMap( { \"context\": retriever_chain | format_docs, \"question\": itemgetter(\"question\"), \"chat_history\": itemgetter(\"chat_history\"), } ).with_config(run_name=\"RetrieveDocs\") prompt = ChatPromptTemplate.from_messages( [ (\"system\", RESPONSE_TEMPLATE), MessagesPlaceholder(variable_name=\"chat_history\"), (\"human\", \"{question}\"), ] ) response_synthesizer = (prompt | llm | StrOutputParser()).with_config( run_name=\"GenerateResponse\", ) answer_chain = _context | response_synthesizer\n\nEvaluation\n\nBuilding a reliable chatbot took many adjustments. Our initial prototype left a lot to be desired. It confidently hallucinated content that was irrelevant to the LangChain context or it was unable to respond to many common questions. By using LangSmith throughout the development process, we could rapidly iterate on the different steps in our pipeline, quickly identifying the weakest links in our chain. Our typical process would be to interact with the app, view the trace for bad examples, assign blame to the different components (retriever, response generator, etc.) so we know what to update, and repeat. Whenever the bot failed on one of our questions, we would add the run to a dataset, along with the answer we expected to receive. This quickly got us a working V0 and gave us benchmark dataset \"for free\", which we could use to check our performance whenever we make further changes to our chain structure.\n\nWe took our dataset of questions and hand-written answers and used LangSmith to benchmark each version of our bot. For tasks like question-answering, where the responses can sometimes be long or contain code, selecting the right metrics can be a challenge. When checking the evaluation results for embedding or string distance evaluators, neither adequately matched what we would expect for grading, so used LangChain\u2019s QA evaluator, along with a custom \u201cHallucination\u201d evaluator that compares the response with the documents to see if there is content that is not grounded in the retrieved docs. Doing so let us confidently make changes to the bot\u2019s prompt, retriever, and overall architecture.\n\nChat Application\n\nWith any chat application, the \u201ctime to first token\u201d needs to be minimal, meaning end-to-end asynchronous streaming support is a must. For our application, we also want to stream the reference knowledge so the user can see the source docs for more", "start_char_idx": 4539, "end_char_idx": 9299, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52d09477-5764-4389-a48b-7de925680984": {"__data__": {"id_": "52d09477-5764-4389-a48b-7de925680984", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_building-chat-langchain-2_.txt", "file_name": "blog.langchain.dev_building-chat-langchain-2_.txt", "file_type": "text/plain", "file_size": 11423, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e34fe21a-43b6-4938-a0da-94ce8e6fe887", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_building-chat-langchain-2_.txt", "file_name": "blog.langchain.dev_building-chat-langchain-2_.txt", "file_type": "text/plain", "file_size": 11423, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "ef051310bd2a66bc6f27c9d8908ba7804134f25036a68b7ccccf92e7fa66dff0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ebaf87ff-868a-4436-a1ae-0c3f6f586cc2", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_building-chat-langchain-2_.txt", "file_name": "blog.langchain.dev_building-chat-langchain-2_.txt", "file_type": "text/plain", "file_size": 11423, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "15cb24b38852337397948df8a96f674e699492ef10971e7cd405c94f1d98c773", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e22931d7-ca68-493d-9d64-d7513b071a20", "node_type": "1", "metadata": {}, "hash": "fabf8f06db975dc9653c9421ead68abdc8dc384a38d2419b18d052946fa8e5ec", "class_name": "RelatedNodeInfo"}}, "hash": "8751fbf0b7611ddc0c03234238972bd1fc245e5324035f64e00c762893dbd113", "text": "application, we also want to stream the reference knowledge so the user can see the source docs for more information. Since we\u2019ve built our chat bot using LangChain Runnables, we gets all of this for free.\n\nOur chatbot uses the astream_log method to asynchronously stream the responses from the retriever and the response generation chain to the web client.\n\nstream = answer_chain.astream_log( { \"question\": question, \"chat_history\": converted_chat_history, }, config={\"metadata\": metadata}, include_names=[\"FindDocs\"], include_tags=[\"FindDocs\"], )\n\nThis creates a generator that emits operations from the selected that we can easily separate into:\n\nResponse content (the answer)\n\nRetrieved document metadata (for citations)\n\nThe run ID for the LangSmith trace (for feedback)\n\nFrom these, the client can render the document cards as soon as they are available, stream the chat response, and capture the run ID to use for logging user feedback.\n\nLogging User Feedback\n\nThe feedback endpoint is a simple line that takes the Run ID and score from the web client and logs the values to LangSmith:\n\nclient.create_feedback(run_id, \"user_score\", score=score)\n\nMonitoring\n\nWith the chat bot in production, LangSmith makes it easy to aggregate and monitor relevant metrics so we can track how well the application is behaving. For instance, we can check the time-to-first-token metric, which captures the delay between when the query is sent to the chat bot and when the first response token is sent back to the user.\n\nTime to first token (TTFT) chart\n\nOr we can monitor when there are any errors in the trace:\n\nSuccess rate chart\n\nWe can also track the user feedback metrics, token counts, and all kinds of other analytics to make sure our chat bot is performing as expected. All this information helps us detect, filter, and improve the bot over time.\n\nConclusion\n\nHead to https://chat.langchain.com to play around with the deployed version. To dive deeper, check out the source code, clone it, and incorporate your own documents to further explore its capabilities.\n\nIf you are exploring building apps and want to chat, I would be happy to make that happen :) DM me on Twitter @mollycantillon", "start_char_idx": 9195, "end_char_idx": 11381, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e22931d7-ca68-493d-9d64-d7513b071a20": {"__data__": {"id_": "e22931d7-ca68-493d-9d64-d7513b071a20", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt", "file_name": "blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt", "file_type": "text/plain", "file_size": 4707, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7c5eec50-2964-4601-ad30-745f2dadf028", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt", "file_name": "blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt", "file_type": "text/plain", "file_size": 4707, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "a0fd2042eca7ef63831b40f43c23a6222d2870f09edcd0aae1f1b712bbf4dc2e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52d09477-5764-4389-a48b-7de925680984", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_building-chat-langchain-2_.txt", "file_name": "blog.langchain.dev_building-chat-langchain-2_.txt", "file_type": "text/plain", "file_size": 11423, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "8751fbf0b7611ddc0c03234238972bd1fc245e5324035f64e00c762893dbd113", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dbcb7956-3782-4047-8b85-912f13830ed6", "node_type": "1", "metadata": {}, "hash": "4f582439b44df37520297c612b7f38510d1a3005daff96913ba542a4fa2e6e62", "class_name": "RelatedNodeInfo"}}, "hash": "fabf8f06db975dc9653c9421ead68abdc8dc384a38d2419b18d052946fa8e5ec", "text": "URL: https://blog.langchain.dev/chat-loaders-finetune-a-chatmodel-in-your-voice/\nTitle: Chat Loaders: Fine-tune a ChatModel in your Voice\n\nSummary\n\nWe are adding a new integration type, ChatLoaders, to make it easier to fine-tune models on your own unique writing style. These utilities help convert data from popular messaging platforms to chat messages compatible with fine-tuning formats like that supported by OpenAI.\n\nThank you to Greg Kamradt for Misbah Syed for their thought leadership on this.\n\nImportant Links:\n\nContext\n\nOn Tuesday, OpenAI announced improved fine-tuning support, extending the service to larger chat models like GPT-3.5-turbo. This enables anyone to customize these larger, more capable models for their own use cases. They also teased support for fine-tuning GPT-4 later this year.\n\nWhile fine-tuning is typically not advised for teaching an LLM substantially new knowledge or for factual recall; it is good for style transfer.\n\nWe've had a lot of community members ask about the best ways to get ChatGPT to respond \"in your own voice\" - fine-tuning is an excellent way to do so!\n\nGreat people on Twitter like Greg Kamdrat have also been bullish on this use case:\n\nSetting the tone/style of the output is top of the list for me\n\n\n\nFine-tuning as a service to businesses that matches their tone\n\n\n\nCurrently investigating...will report back https://t.co/235WSJzxet pic.twitter.com/KDzMrdqccv \u2014 Greg Kamradt (@GregKamradt) August 22, 2023\n\nFine-tuning on your communications could be useful for a variety of applications, such as responding to customers in your brand's voice, generating content that is more aware of your team's unique jargon, chatting reliably in a target language, or just for fun!\n\nWhy is this better than direct instructions? Style and tone can be hard to describe! Most of us don't write like ChatGPT, and it can sometimes be frustratingly difficult to get the LLM to consistently respond in a particular voice (especially over longer conversations).\n\nWhy is this better than few-shot examples? It can be challenging to capture your voice in only a few concise snippets! Fine-tuning lets you provide a larger number of examples the model can learn from without having to see them every time you want to query the model.\n\nChatLoaders\n\nAt LangChain, we want to make it as easy as possible for you to take advantage of this improved fine-tuning support. To make it simple to adapt a model to your voice, we're adding a new integration type: ChatLoaders .\n\nThese utilities take data exported from popular messaging platforms and convert them to LangChain message objects, which you can then easily convert platform-agnostic message formats, such as OpenAI, Llama 2, and others. This training data can be used directly for fine-tuning a model.\n\nWe've added loaders for the following popular messaging platforms so far:\n\nFacebook Messenger\n\nSlack\n\nTelegram\n\nWhatsApp\n\nWe have also added a recipe on how to do so for Discord and Twitter (using Apify) and plan to integrate additional chat loaders in the near future. If you have a favorite messaging platform you'd like to support, we'd love to help you land a PR!\n\nTo get you started, we've added an end-to-end example notebook to the LangChain documentation showing how to fine-tune gpt-3.5-turbo (the model behind ChatGPT) on an example set of Facebook messages.\n\n\u2757 Please ensure all participants of your conversations support the decision to train a model on the chat data before proceeding.\n\nOnce you have your fine-tuned model, you can use the model name directly in LangChain's ChatOpenAI class:\n\nfrom langchain.chat_models import ChatOpenAI llm = ChatOpenAI(model=\"ft:gpt-3.5-turbo-0613:{openaiOrg}::{modelId}\") llm.predict(\"What classes are you taking this year?\")\n\nThen you can plug this into any other LangChain component!\n\nEnd-to-End Example\n\nWe've also created an end-to-end example of finetuning a model based on Elon Musk's tweets. This uses Apify to load data. Note that it's less than 100 examples so results may not be the most amazing they could be.\n\nWe open-sourced this example at the GitHub repo here. We also hosted it on Streamlit app so you can easily play around with it here.\n\nWebinar\n\nThere is a lot more to discuss on this topic. What types of messages are best for finetuning? What others sources of data exist for this? How many points do you need?\n\nWe'll be discussing this and more next week in a webinar with Greg Kamradt.", "start_char_idx": 0, "end_char_idx": 4448, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dbcb7956-3782-4047-8b85-912f13830ed6": {"__data__": {"id_": "dbcb7956-3782-4047-8b85-912f13830ed6", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt", "file_name": "blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt", "file_type": "text/plain", "file_size": 4707, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7c5eec50-2964-4601-ad30-745f2dadf028", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt", "file_name": "blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt", "file_type": "text/plain", "file_size": 4707, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "a0fd2042eca7ef63831b40f43c23a6222d2870f09edcd0aae1f1b712bbf4dc2e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e22931d7-ca68-493d-9d64-d7513b071a20", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt", "file_name": "blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt", "file_type": "text/plain", "file_size": 4707, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "fabf8f06db975dc9653c9421ead68abdc8dc384a38d2419b18d052946fa8e5ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "790bf101-ca54-43a5-a74e-797c33ad0195", "node_type": "1", "metadata": {}, "hash": "af5f40731b262181237ac76deaa82e178836be2c2f4f90e911fb460cb0b996ff", "class_name": "RelatedNodeInfo"}}, "hash": "4f582439b44df37520297c612b7f38510d1a3005daff96913ba542a4fa2e6e62", "text": "need?\n\nWe'll be discussing this and more next week in a webinar with Greg Kamradt. Come join!\n\nConclusion\n\nWe're excited to see all the creative applications fine-tuning unlocks. We have implemented a few ChatLoaders already, but we need your help to make it easier to create your own personalized model. Help us create more ChatLoaders!", "start_char_idx": 4366, "end_char_idx": 4703, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "790bf101-ca54-43a5-a74e-797c33ad0195": {"__data__": {"id_": "790bf101-ca54-43a5-a74e-797c33ad0195", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_name": "blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_type": "text/plain", "file_size": 24234, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7385d65d-a81a-4ee3-ab0d-801ee406e901", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_name": "blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_type": "text/plain", "file_size": 24234, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "11469c9b63f6b1fbe6f23ab9ed7f63920e6511ea08ff3cc4015bea32e04d016c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dbcb7956-3782-4047-8b85-912f13830ed6", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt", "file_name": "blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt", "file_type": "text/plain", "file_size": 4707, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "4f582439b44df37520297c612b7f38510d1a3005daff96913ba542a4fa2e6e62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25595cc1-a897-4c2d-aaee-10dd5090f624", "node_type": "1", "metadata": {}, "hash": "d06bc28854a70231ebfed6668e39250725f32918194427b2c48140977ae4a18c", "class_name": "RelatedNodeInfo"}}, "hash": "af5f40731b262181237ac76deaa82e178836be2c2f4f90e911fb460cb0b996ff", "text": "URL: https://blog.langchain.dev/chat-with-your-data-using-openai-pinecone-airbyte-langchain/\nTitle: Chat with your data using OpenAI, Pinecone, Airbyte and Langchain\n\nEditor\u2019s Note: This blog post was written in collaboration with Airbyte. Their new vector database destination makes it really easy for data to retrieve relevant context for question answering use cases via LangChain. We're seeing more and more teams seek ways to integrate diverse data sources\u2013and keep them up-to-date automatically\u2013and this is a fantastic way to do it!\n\nAside from the specific use case highlighted here, we're also very excited about this integration in general. It combines the hundreds of sources in AirByte with their robust scheduling and orchestration framework, and leverages the advanced transformation logic in LangChain along with LangChain's 50+ embedding provider integrations and 50+ vectorstore integrations.\n\nLearn how to build a connector development support bot for Slack that knows all your APIs, open feature requests and previous Slack conversations by heart\n\nIn a previous article, we explained how Dagster and Airbyte can be leveraged to power LLM-supported use cases. Our newly introduced vector database destination makes this even easier as it removes the need to orchestrate chunking and embedding manually - instead the sources can be directly connected to the vector database through an Airbyte connection.\n\nThis tutorial walks you through a real-world use case of how to leverage vector databases and LLMs to make sense out of your unstructured data. By the end of this, you will:\n\nKnow how to extract unstructured data from a variety of sources using Airbyte\n\nKnow how to use Airbyte to efficiently load data into a vector database, preparing the data for LLM usage along the way\n\nKnow how to integrate a vector database into your LLM to ask questions about your proprietary data\n\nWhat we will build\n\nTo better illustrate how this can look in practice, let\u2019s use something that\u2019s relevant for Airbyte itself.\n\nAirbyte is a highly extensible system that allows users to develop their own connectors to extract data from any API or internal systems. Helpful information for connector developers can be found in different places:\n\nThe official connector development documentation website\n\nGithub issues documenting existing feature requests, known bugs and work in progress\n\nThe community Slack help channel\n\nThis article describes how to tie together all of these diverse sources to offer a single chat interface to access information about connector development - a bot that can answer questions in plain english about the code base, documentation and reference previous conversations:\n\n\n\nIn these examples, information from the documentation website and existing Github issues is combined in a single answer.\n\nPrerequisites\n\nFor following through the whole process, you will need the following accounts. However, you can also work with your own custom sources and use a local vector store to avoid all but the OpenAI account:\n\nSource-specific accounts\n\nApify account\n\nGithub account\n\nSlack account\n\nDestination-specific accounts\n\nOpenAI account\n\nPinecone account\n\nAirbyte instance (local or cloud)\n\nStep 1 - Fetch Github issues\n\nAirbyte\u2019s feature and bug tracking is handled by the Github issue tracker of the Airbyte open source repository. These issues contain important information people need to look up regularly.\n\nTo fetch Github issues, create a new source using the Github connector.\n\nIf you are using Airbyte Cloud , you can easily authenticate using the \u201cAuthenticate your GitHub account\u201d, otherwise follow the instructions in the documentation on the right side of how to set up a personal access token in the Github UI.\n\nNext, configure a cutoff date for issues and specify the repositories that should be synced. In this case I\u2019m going with \u201c2023-07-01T00:00:00Z\u201d and \u201cairbytehq/airbyte\u201d to sync recent issues from the main Airbyte repository:\n\nStep 2 - Load into vector database\n\nNow we have our first source ready, but Airbyte doesn\u2019t know yet where to put the data. The next step is to configure the destination. To do so, pick the \u201cVector Database (powered by LangChain)\u201d. There is some preprocessing that Airbyte is doing for you so that the data is vector ready:\n\nSeparating text and metadata fields and splitting up records into multiple documents to keep each document focused on a single topic and to make sure the text fits into the context window of the LLM that\u2019s going to be used for question answering\n\nEmbedding the text of every document using the configured embedding service, turning the text into a vector to do similarity search on\n\nIndexing the documents into the vector database (uploading the vector from the embedding service along with the metadata object)\n\nThe vector database destination currently supports two different vector databases (with more to come) - Pinecone, which is a hosted service with a free tier and Chroma which stores the vector database in a local file.\n\nFor using Pinecone, sign up for a free trial account and", "start_char_idx": 0, "end_char_idx": 5087, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25595cc1-a897-4c2d-aaee-10dd5090f624": {"__data__": {"id_": "25595cc1-a897-4c2d-aaee-10dd5090f624", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_name": "blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_type": "text/plain", "file_size": 24234, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7385d65d-a81a-4ee3-ab0d-801ee406e901", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_name": "blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_type": "text/plain", "file_size": 24234, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "11469c9b63f6b1fbe6f23ab9ed7f63920e6511ea08ff3cc4015bea32e04d016c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "790bf101-ca54-43a5-a74e-797c33ad0195", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_name": "blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_type": "text/plain", "file_size": 24234, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "af5f40731b262181237ac76deaa82e178836be2c2f4f90e911fb460cb0b996ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eef8d306-c1d5-49e2-9b6a-301798ea4063", "node_type": "1", "metadata": {}, "hash": "c10e6e6ca28fb78cbc187d083a3acd556efdc994494ac3c801ef7e630711fcfd", "class_name": "RelatedNodeInfo"}}, "hash": "d06bc28854a70231ebfed6668e39250725f32918194427b2c48140977ae4a18c", "text": "vector database in a local file.\n\nFor using Pinecone, sign up for a free trial account and create an index using a starter pod. Set the dimensions to 1536 as that\u2019s the size of the OpenAI embeddings we will be using\n\nOnce the index is ready, configure the vector database destination in Airbyte:\n\nSet chunk size to 1000 (the size refers to number of tokens, not characters, so this is roughly 4KB of text. The best chunking is dependent on the data you are dealing with)\n\n(the size refers to number of tokens, not characters, so this is roughly 4KB of text. The best chunking is dependent on the data you are dealing with) Configure the records fields to treat as text fields which will be embedded. All other fields will be handled as metadata. For now, set it \u201c title \u201d and \u201c body \u201d as these are the relevant feels in the issue stream of the Github source\n\n\u201d and \u201c \u201d as these are the relevant feels in the issue stream of the Github source Set your OpenAI api key for powering the embedding service. You can find your API key in the API keys section of the platform.openai.com/account page\n\nFor the indexing step, copy over index, environment and api key from the Pinecone UI. You can find the API key and the environment in the \u201cAPI Keys\u201d section in the UI\n\nStep 3 - Create a connection\n\nOnce the destination is set up successfully, set up a connection from the Github source to the vector database destination. In the configuration flow, pick the existing source and destination. When configuring the connection, make sure to only use the \u201cissues\u201d stream, as this is the one we are interested in.\n\nSide note: Airbyte allows to make this sync more efficient in a production environment:\n\nTo keep the metadata focused, you can click on the stream name to select the individual fields you want to sync. For example if the \u201cassignee\u201d or the \u201cmilestone\u201d field is never relevant to you, you can uncheck it and it won\u2019t be synced to the destination.\n\nThe sync mode can be used to sync issues incrementally while deduplicating the records in the vector database so no stale data will show up in searches\n\n\n\nIf everything went well, there should be a connection now syncing data from Github to Pinecone via the vector store destination. Give the sync a few minutes to run. Once the first run has completed, you can check the Pinecone index management page to see a bunch of indexed vectors ready to be queried.\n\nEach vector is associated with a metadata object that\u2019s filled with the fields that were not mentioned as \u201ctext fields\u201d in the destination configuration. These fields will be retrieved along with the embedded text and can be leveraged by our chatbot in later sections. This is how a vector with metadata looks like when retrieved from Pinecone:\n\n{ \"id\": \"599d75c8-517c-4f37-88df-ff16576bd607\", \"values\": [0.0076571689, ..., 0.0138477711], \"metadata\": { \"_airbyte_stream\": \"issues\", \"_record_id\": 1556650122, \"author_association\": \"CONTRIBUTOR\", \"comments\": 3, \"created_at\": \"2023-01-25T13:21:50Z\", // ... \"text\": \"...The acceptance-test-config.yml file is in a legacy format. Please migrate to the latest format...\", \"updated_at\": \"2023-07-17T09:20:56Z\", } }\n\nOn subsequent runs, Airbyte will only re-embed and update the vectors for the issues that changed since the last sync - this will speed up subsequent runs while making sure your data is always up-to-date and available.\n\n\n\nStep 4 - Chat interface\n\nThe data is ready, now let\u2019s wire it up with our LLM to answer questions in natural language. As we already used OpenAI for the embedding, the easiest approach is to use it as well for the question answering.\n\nWe will use Langchain as an orchestration framework to tie all the bits together.\n\nFirst, install a few pip packages locally:\n\npip install pinecone-client langchain openai\n\nThe basic functionality here works the following way:\n\nUser asks a question\n\nThe question is embedded using the same model used for generating the vectors in the vector database (OpenAI in this case)\n\nThe question vector is sent to the vector database and documents with similar vectors are returned - as the vectors represent the meaning of the text, the question and the answer to the question will have very similar vectors and relevant documents will be returned\n\nThe text of all documents with the relevant metadata are put together into a single string and sent to the LLM together with the question the user asked and the instruction to answer the user\u2019s question based on the", "start_char_idx": 4997, "end_char_idx": 9477, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eef8d306-c1d5-49e2-9b6a-301798ea4063": {"__data__": {"id_": "eef8d306-c1d5-49e2-9b6a-301798ea4063", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_name": "blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_type": "text/plain", "file_size": 24234, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7385d65d-a81a-4ee3-ab0d-801ee406e901", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_name": "blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_type": "text/plain", "file_size": 24234, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "11469c9b63f6b1fbe6f23ab9ed7f63920e6511ea08ff3cc4015bea32e04d016c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25595cc1-a897-4c2d-aaee-10dd5090f624", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_name": "blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_type": "text/plain", "file_size": 24234, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "d06bc28854a70231ebfed6668e39250725f32918194427b2c48140977ae4a18c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1687731c-baa4-4e78-a2ea-03fe2b4a854d", "node_type": "1", "metadata": {}, "hash": "c67bf9b2d505bf046afd6ba263a9655a1601ce08c7b5f7c3f1ed01992a8a9d7c", "class_name": "RelatedNodeInfo"}}, "hash": "c10e6e6ca28fb78cbc187d083a3acd556efdc994494ac3c801ef7e630711fcfd", "text": "together with the question the user asked and the instruction to answer the user\u2019s question based on the provided context\n\nThe LLM answers the question based on the provided context\n\nThe answer is presented to the user\n\nThis flow is often referred to as retrieval augmented generation. The RetrievalQA class from the Langchain framework already implements the basic interaction. The simplest version of our question answering bot only has to provide the vector store and the used LLM:\n\n# chatbot.py import os import pinecone from langchain.chains import RetrievalQA from langchain.embeddings import OpenAIEmbeddings from langchain.llms import OpenAI from langchain.vectorstores import Pinecone embeddings = OpenAIEmbeddings() pinecone.init(api_key=os.environ[\"PINECONE_KEY\"], environment=os.environ[\"PINECONE_ENV\"]) index = pinecone.Index(os.environ[\"PINECONE_INDEX\"]) vector_store = Pinecone(index, embeddings.embed_query, \"text\") qa = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0), chain_type=\"stuff\", retriever=vector_store.as_retriever()) print(\"Connector development help bot. What do you want to know?\") while True: query = input(\"\") answer = qa.run(query) print(answer) print(\"\n\nWhat else can I help you with:\")\n\nTo run this script, you need to set OpenAI and Pinecone credentials as environment variables:\n\nexport OPENAI_API_KEY=... export PINECONE_KEY=... export PINECONE_ENV=... export PINECONE_INDEX=... python chatbot.py\n\nThis works in general, but it has some limitations. By default, only the text fields are passed into the prompt of the LLM, so it doesn\u2019t know what the context of a text is and it also can\u2019t give a reference back to where it found its information:\n\nConnector development help bot. What do you want to know? > Can you give me information about how to authenticate via a login endpoint that returns a session token? Yes, the GenericSessionTokenAuthenticator should be supported in the UI[...]\n\nFrom here, there\u2019s lots of fine tuning to do to optimize our chat bot. For example we can improve the prompt to contain more information based on the metadata fields and be more specific for our use case:\n\nprompt_template = \"\"\"You are a question-answering bot operating on Github issues and documentation pages for a product called connector builder. The documentation pages document what can be done, the issues document future plans and bugs. Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Always state were you got this information from (and the github issue number if applicable). If the answer is based on a Github issue that's not closed yet, add 'This issue is not closed yet - the feature might not be shipped yet' to the answer. {context} Question: {question} Helpful Answer:\"\"\" prompt = PromptTemplate( template=prompt_template, input_variables=[\"context\", \"question\"] ) class ConnectorDevelopmentPrompt(PromptTemplate): def format_document(doc: Document, prompt: PromptTemplate) -> str: if doc.metadata[\"_airbyte_stream\"] == \"issues\": return f\"Excerpt from Github issue: {doc.page_content}, issue number: {doc.metadata['number']}, issue state: {doc.metadata['state']}\" else: return super().format_document(doc, prompt) document_prompt = ConnectorDevelopmentPrompt(input_variables=[\"page_content\"], template=\"{page_content}\") qa = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0), chain_type=\"stuff\", retriever=vector_store.as_retriever(), chain_type_kwargs={\"prompt\": prompt, \"document_prompt\": document_prompt})\n\nThe full script also be found on Github\n\nThis revised version of the RetrievalQA chain customizes the prompts that are sent to the LLM after the context has been retrieved:\n\nThe basic prompt template sets the broader context what this question is about (previously the LLM had to guess from the documents)\n\nIt also changes the way documents are added to the prompt - by default, only the text is added, but the ConnectorDevelopmentPrompt implementation sets the context where the data is coming from and also adds relevant metadata to the prompt so the LLM can base its answer on more than just the text\n\nConnector development help bot. What do you want to know? > Can you give me information about how to authenticate via a login endpoint that returns a session token? You can use the GenericSessionTokenAuthenticator to authenticate via a login endpoint that returns a session token. This is documented in the Connector Builder documentation with an example of how the request flow functions (e.g. metabase). This issue is not closed yet - the feature might not be shipped yet", "start_char_idx": 9373, "end_char_idx": 14031, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1687731c-baa4-4e78-a2ea-03fe2b4a854d": {"__data__": {"id_": "1687731c-baa4-4e78-a2ea-03fe2b4a854d", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_name": "blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_type": "text/plain", "file_size": 24234, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7385d65d-a81a-4ee3-ab0d-801ee406e901", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_name": "blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_type": "text/plain", "file_size": 24234, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "11469c9b63f6b1fbe6f23ab9ed7f63920e6511ea08ff3cc4015bea32e04d016c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eef8d306-c1d5-49e2-9b6a-301798ea4063", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_name": "blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_type": "text/plain", "file_size": 24234, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "c10e6e6ca28fb78cbc187d083a3acd556efdc994494ac3c801ef7e630711fcfd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a60f2398-83d0-4654-92fe-d209101d02f3", "node_type": "1", "metadata": {}, "hash": "310b2af8ab3a9691f39278aceb1e2ed8270e9437d4bef5c6e0993703ac7dd4b3", "class_name": "RelatedNodeInfo"}}, "hash": "c67bf9b2d505bf046afd6ba263a9655a1601ce08c7b5f7c3f1ed01992a8a9d7c", "text": "metabase). This issue is not closed yet - the feature might not be shipped yet (Github issue #26341).\n\n\n\nStep 5 - Put it on Slack\n\nSo far this helper can only be used locally. However, using the python slack sdk it\u2019s easy to turn this into a Slack bot itself.\n\nTo do so, we need to set up a Slack \u201cApp\u201d first. Go to https://api.slack.com/apps and create a new app based on the manifest here (this saves you some work configuring permissions by hand). After you set up your app, install it to the workspace you want to integrate with. This will generate a \u201cBot User OAuth Access Token\u201d you need to note down. Afterwards, go to the \u201cBasic information\u201d page of your app, scroll down to \u201cApp-Level Tokens\u201d and create a new token. Note down this \u201capp level token\u201d as well.\n\nWithin the regular Slack client, your app can be added to a slack channel by clicking the channel name and going to the \u201cIntegrations\u201d tab:\n\nAfter this, your Slack app is ready to receive pings from users to answer questions - the next step is to call Slack from within python code, so we need to install the python client library:\n\npip install slack_sdk\n\nAfterwards, we can extend our existing chatbot script with a Slack integration:\n\nfrom slack_sdk import WebClient from slack_sdk.socket_mode import SocketModeClient from slack_sdk.socket_mode.request import SocketModeRequest from slack_sdk.socket_mode.response import SocketModeResponse slack_web_client = WebClient(token=os.environ[\"SLACK_BOT_TOKEN\"]) handled_messages = {} def process(client: SocketModeClient, socket_mode_request: SocketModeRequest): if socket_mode_request.type == \"events_api\": event = socket_mode_request.payload.get(\"event\", {}) client_msg_id = event.get(\"client_msg_id\") if event.get(\"type\") == \"app_mention\" and not handled_messages.get(client_msg_id): handled_messages[client_msg_id] = True channel_id = event.get(\"channel\") text = event.get(\"text\") result = qa.answer(text) slack_web_client.chat_postMessage(channel=channel_id, text=result) return SocketModeResponse(envelope_id=socket_mode_request.envelope_id) socket_mode_client = SocketModeClient( app_token=os.environ[\"SLACK_APP_TOKEN\"], web_client=slack_web_client ) socket_mode_client.socket_mode_request_listeners.append(process) socket_mode_client.connect() print(\"listening\") from threading import Event Event().wait()\n\nThe full script also be found on Github\n\nTo run the script, the environment variables for the slack bot token and app token need to be added as environment variables as well:\n\nexport SLACK_BOT_TOKEN=... export SLACK_APP_TOKEN=... python chatbot.py\n\nRunning this, you should be able to ping the development bot application in the channel you added it to like a user and it will respond to questions by running the RetrievalQA chain that loads relevant context from the vector database and uses an LLM to formulate a nice answer:\n\nAll the code can also be found on Github\n\n\n\nStep 6 - Additional data source: Scrape documentation website\n\nGithub issues are helpful, but there is more information we want our development bot to know.\n\nThe documentation page for connector development is a very important source of information to answer questions, so it definitely needs to be included. The easiest way to make sure the bot has the same information as what\u2019s published, is to scrape the website. For this case, we are going to use the Apify service to take care of the scraping and turning the website into a nicely structured dataset. This dataset can be extracted using the Airbyte Apify Dataset source connector.\n\nFirst, log into Apify and navigate to the store. Choose the \u201cWeb Scraper\u201d actor as a basis - it already implements most of the functionality we need\n\nNext, create a new task and configure it to scrape all pages of the documentation, extracting the page title and all of the content:\n\nSet Start URLs to https://docs.airbyte.com/connector-development/connector-builder-ui/overview/ , the intro page of the documentation linking to other pages\n\nSet Link selector to a[href] to follow all links from every page\n\nSet Glob Patterns to https://docs.airbyte.com/connector-development/connector-builder-ui/* to limit the scraper to stick to the documentation and not crawl the whole internet\n\nConfigure the Page function to extract the page title and the content - in this case the content element can be found using the CSS class name\n\n\n\n\n\nasync function pageFunction(context) { const $ = context.jQuery; const pageTitle = $('title').first().text(); const content = $('.markdown').first().text(); return { url: context.request.url, pageTitle, content }; }\n\nRunning this actor will complete quickly and give us a nicely consumable dataset with a column for the page title and the content:\n\nNow", "start_char_idx": 13953, "end_char_idx": 18679, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a60f2398-83d0-4654-92fe-d209101d02f3": {"__data__": {"id_": "a60f2398-83d0-4654-92fe-d209101d02f3", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_name": "blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_type": "text/plain", "file_size": 24234, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7385d65d-a81a-4ee3-ab0d-801ee406e901", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_name": "blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_type": "text/plain", "file_size": 24234, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "11469c9b63f6b1fbe6f23ab9ed7f63920e6511ea08ff3cc4015bea32e04d016c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1687731c-baa4-4e78-a2ea-03fe2b4a854d", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_name": "blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_type": "text/plain", "file_size": 24234, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "c67bf9b2d505bf046afd6ba263a9655a1601ce08c7b5f7c3f1ed01992a8a9d7c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e80e809a-89a9-49c5-8db6-ecaf3c77894e", "node_type": "1", "metadata": {}, "hash": "46b7d470e3f7562c05a624b79168d0b87ff9dff1839748e632892dc98cbddc1f", "class_name": "RelatedNodeInfo"}}, "hash": "310b2af8ab3a9691f39278aceb1e2ed8270e9437d4bef5c6e0993703ac7dd4b3", "text": "and give us a nicely consumable dataset with a column for the page title and the content:\n\nNow it\u2019s time to connect Airbyte to the Apify data set - go to the Airbyte web UI and add your second Source - pick \u201cApify Dataset\u201d\n\nTo set up the Source, you only need to copy the dataset ID that\u2019s shown in the \u201cStorage\u201d tap of the \u201cRun\u201d in the Apify UI\n\nOnce the source is set up, follow the same steps as for the Github source to set up a connection moving data from the Apify dataset to the vector store. As the relevant text content is sitting in different fields, you also need to update the vector store destination - add data.pageTitle and data.content to the \u201ctext fields\u201d of the destination and save.\n\nStep 7 - Additional data source: Fetch Slack messages\n\nAnother valuable source of information relevant to connector development are Slack messages from the public help channel. These can be loaded in a very similar fashion. Create a new source using the Slack connector. When using cloud, you can authenticate using the \u201cAuthenticate your Slack account\u201d button for simple setup, otherwise follow the instructions in the documentation on the right hand side how to create a Slack \u201cApp\u201d with the required permissions and add it to your workspace. To avoid fetching messages from all channels, set the channel name filter to the correct channel.\n\nAs for Apify and Github, a new connection needs to be created to move data from Slack to Pinecone. Also add text to the \u201ctext fields\u201d of the destination to make sure the relevant data gets embedded properly so similarity searches will yield the right results.\n\nIf everything went well, there should be three connections now, all syncing data from their respective sources to the centralized vector store destination using a Pinecone index.\n\nBy adjusting the frequency of the connections, you can control how often Airbyte will rerun the connection to make sure the knowledge base of our chat bot stays up to date. As Github and Slack are frequently updated and support efficient incremental updates, it makes sense to set them to a daily frequency or higher. The documentation pages don\u2019t change as often, so they can be kept at a lower frequency or even just be triggered on demand when there are changes.\n\nAs we have more sources now, let\u2019s improve our prompt to make sure the LLM has all necessary information to formulate a good answer:\n\nclass ConnectorDevelopmentPrompt(PromptTemplate): def format_document(doc: Document, prompt: PromptTemplate) -> str: if doc.metadata[\"_airbyte_stream\"] == \"DatasetItems\": return f\"Excerpt from documentation page: {doc.page_content}\" elif doc.metadata[\"_airbyte_stream\"] == \"issues\": return f\"Excerpt from Github issue: {doc.page_content}, issue number: {doc.metadata['number']}, issue state: {doc.metadata['state']}\" elif doc.metadata[\"_airbyte_stream\"] == \"threads\" or doc.metadata[\"_airbyte_stream\"] == \"channel_messages\": return f\"Excerpt from Slack thread: {doc.page_content}\" else: return super().format_document(doc, prompt)\n\nBy default the RetrievalQA chain retrieves the top 5 matching documents, so if it\u2019s applicable the answer will be based on multiple sources at the same time:\n\nConnector development help bot. What do you want to know? > What different authentication methods are supported by the builder? Can I authenticate a login endpoint that returns a session token? The authentication methods supported by the builder are Basic HTTP, Bearer Token, API Key, and OAuth. The builder does not currently support authenticating a login endpoint that returns a session token, but this feature is planned and can be tracked in the Github issue #26341. This issue is not closed yet - the feature might not be shipped yet.\n\nThe first sentence about Basic HTTP, Bearer Token, API Key and OAuth is retrieved from the documentation page about authentication, while the second sentence is referring to the same Github issue as before.\n\nWrapping up\n\nWe covered a lot of ground here - stepping back a bit, we accomplished the following parts:\n\nSet up a pipeline that loads unstructured data from multiple sources into a vector database\n\nImplement an application that can answer plain text questions about the unstructured data in a general way\n\nExpose this application as a Slack bot\n\nWith data flowing through this system, Airbyte will make sure the data in your vector database will always be up-to-date while only syncing records that changed in the connected source, minimizing the load on embedding and vector database services while also providing an overview over the current state of running pipelines.\n\nThis setup isn\u2019t using a single black box service that encapsulates all the details and leaves us with limited options for tweaking behavior and controlling data processing - instead it\u2019s composed out of multiple components that be easily extended in various places:\n\nThe large catalog of Airbyte sources and the connector builder for integrating specialized sources allow to easily load just about", "start_char_idx": 18585, "end_char_idx": 23587, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e80e809a-89a9-49c5-8db6-ecaf3c77894e": {"__data__": {"id_": "e80e809a-89a9-49c5-8db6-ecaf3c77894e", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_name": "blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_type": "text/plain", "file_size": 24234, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7385d65d-a81a-4ee3-ab0d-801ee406e901", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_name": "blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_type": "text/plain", "file_size": 24234, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "11469c9b63f6b1fbe6f23ab9ed7f63920e6511ea08ff3cc4015bea32e04d016c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a60f2398-83d0-4654-92fe-d209101d02f3", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_name": "blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_type": "text/plain", "file_size": 24234, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "310b2af8ab3a9691f39278aceb1e2ed8270e9437d4bef5c6e0993703ac7dd4b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46a6286b-8c49-4d46-856c-ecb89c16bb59", "node_type": "1", "metadata": {}, "hash": "ba48b43c880171b9b736f7a2cdac310a36a4061cfa87677c09af9b5e9cbbd6a7", "class_name": "RelatedNodeInfo"}}, "hash": "46b7d470e3f7562c05a624b79168d0b87ff9dff1839748e632892dc98cbddc1f", "text": "large catalog of Airbyte sources and the connector builder for integrating specialized sources allow to easily load just about any data into a vector db using a single tool\n\nLangchain is very extensible and allows you to leverage LLMs in different ways beyond this simple application, including enriching data from other sources, keeping a chat history to be able to have full conversations and more\n\nIf you are interested in leveraging Airbyte to ship data to your LLM-based applications, take a moment to fill out our survey so we can make sure to prioritize the most important features.", "start_char_idx": 23461, "end_char_idx": 24050, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "46a6286b-8c49-4d46-856c-ecb89c16bb59": {"__data__": {"id_": "46a6286b-8c49-4d46-856c-ecb89c16bb59", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_code-interpreter-api_.txt", "file_name": "blog.langchain.dev_code-interpreter-api_.txt", "file_type": "text/plain", "file_size": 6414, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d3c395ca-e222-4561-ab7a-274b7cb5d76d", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_code-interpreter-api_.txt", "file_name": "blog.langchain.dev_code-interpreter-api_.txt", "file_type": "text/plain", "file_size": 6414, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "d3ce912d9d58cac8e946c939598ffb61a3c73f00c1d91f0737909d7a76b9cee7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e80e809a-89a9-49c5-8db6-ecaf3c77894e", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_name": "blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_type": "text/plain", "file_size": 24234, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "46b7d470e3f7562c05a624b79168d0b87ff9dff1839748e632892dc98cbddc1f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "892ca795-9525-4782-b037-3c622852c571", "node_type": "1", "metadata": {}, "hash": "c3bd1a515859755343494bb3c282ee02728e8825b58dd09cdb5d9bb3f0bb5c47", "class_name": "RelatedNodeInfo"}}, "hash": "ba48b43c880171b9b736f7a2cdac310a36a4061cfa87677c09af9b5e9cbbd6a7", "text": "URL: https://blog.langchain.dev/code-interpreter-api/\nTitle: Code Interpreter API\n\nEditor's Note: This is another installation of our guest blog posts highlighting interesting and novel use cases. This blog is written by Shroominic who built an open source implementation of the ChatGPT Code Interpreter.\n\nImportant Links:\n\nIn the world of open-source software, there are always exciting developments. Today, I am thrilled to announce a new project that I have been working on - Code Interpreter API. It's an implementation of the ChatGPT Code Interpreter using LangChain Agents. Keep in mind this is an unofficial implementation and I am not affiliated with OpenAI.\n\nMotivation\n\nAs an indie developer, I am constantly searching for new features to add to my projects. While working on a Discord bot, I attempted to incorporate the code interpreter as a feature. I never tried it at that point but got hyped by seeing people using it on YouTube. It started as an experiment, but with some tinkering, it evolved into a functional and progressively improving feature. I noticed a significant interest in this area and identified a gap in the market - there was no API for this and especially not open source.\n\nUnique Features and Benefits\n\nThe biggest advantage over the already existing Code Interpreter from OpenAI is that this open-source version has internet access. For example, this is useful when asking \u201cPlot the Bitcoin chart of 2023\u201d the agent uses yahoo finance to download live data about the current situation and then shows it as an image plot to the user. Another advantage is that you can use your hardware. For example, if you want to do something that includes running stable diffusion you need to have a dedicated GPU that can run the model. This is not the case in the cloud environment you have from OpenAI. Other features include chat memory so you can ask follow-up questions and automatic Python package installation (when missing) so the agent tries again.\n\nTechnical Details\n\nThe core of this project is a LangChain Agent, specifically the new OpenAIFunctionsAgent which gives the advantage of calling functions/tools to better handle the user needs. For the code interpreter, we need to give it access to a Python interpreter so it can execute Python code. This interpreter is represented as a tool so the Agent can call the tool and use Python to fulfill what the user wants.\n\nFor example, if the user asks to plot a sin wave and show it as an image, the agent writes Python code and inputs it to the Python interpreter which runs the code, outputs the image and then this image can be displayed to the user. Additionally, the Agent can comment on the result and explain what happened or give more details about the output.\n\nIt is not straightforward to give LangChain Agents access to a Code Execution Environment. There is already Python REPL but this does not allow for FileIO and it also forgets variables after every run. So I had to write my solution for this: CodeBoxAPI.\n\nWhen starting a Code Interpreter Session it creates a small Jupyter kernel on your device similar to how Jupyter notebooks work. A CodeBox is the underlying Python execution env where the agent can run code and receive its outputs. A small personal computer the agent has access to and which it can use to serve the user its request.\n\nExamples\n\nAI: Here is the normalized plot showing the relative performance of the tech giants (Apple, Google, Microsoft, Amazon) in 2023. The plot is normalized so that the starting point of each stock is 100, which allows for easy comparison of the percentage changes in each stock's price over the period.\n\nAnother Example\n\nPrompt: \u201dSelect representative ETFs for different sectors (Technology, Healthcare, Consumer Discretionary, etc.). Plot their performance during the last two economic cycles. Analyze how different sectors behave in different stages of an economic cycle.\u201d\n\nResponse\n\nAI: The plot above shows the performance of the selected ETFs representing the Technology, Healthcare, and Consumer Discretionary sectors during the last two economic cycles (the dot-com bubble from 2000-2002 and the financial crisis from 2007-2009).\n\nAs we can see, all sectors were negatively affected during these periods, with significant drops in value. However, the extent and timing of these drops vary between sectors.\n\nFor a more detailed analysis, we could look at the performance of these sectors during different stages of the economic cycles (e.g., pre-recession, during the recession, and post-recession). However, this would require a more complex analysis and might be beyond the scope of this discussion.\n\nPlease note that this analysis is quite simplified and doesn't take into account many factors that could influence the performance of these sectors during different economic cycles. For a more accurate and detailed analysis, you might want to consider consulting with a financial advisor or conducting a more in-depth research.\n\nRoadmap\n\nThe next step is to make CodeBox production-ready, enabling developers to create public apps as it allows for easy", "start_char_idx": 0, "end_char_idx": 5107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "892ca795-9525-4782-b037-3c622852c571": {"__data__": {"id_": "892ca795-9525-4782-b037-3c622852c571", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_code-interpreter-api_.txt", "file_name": "blog.langchain.dev_code-interpreter-api_.txt", "file_type": "text/plain", "file_size": 6414, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d3c395ca-e222-4561-ab7a-274b7cb5d76d", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_code-interpreter-api_.txt", "file_name": "blog.langchain.dev_code-interpreter-api_.txt", "file_type": "text/plain", "file_size": 6414, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "d3ce912d9d58cac8e946c939598ffb61a3c73f00c1d91f0737909d7a76b9cee7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46a6286b-8c49-4d46-856c-ecb89c16bb59", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_code-interpreter-api_.txt", "file_name": "blog.langchain.dev_code-interpreter-api_.txt", "file_type": "text/plain", "file_size": 6414, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "ba48b43c880171b9b736f7a2cdac310a36a4061cfa87677c09af9b5e9cbbd6a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db911fa6-8dbe-44cf-804f-f102990d879a", "node_type": "1", "metadata": {}, "hash": "dd5bd2f37675d630681523e4345edbb4e873862b0d4cc2c0068bda3085194016", "class_name": "RelatedNodeInfo"}}, "hash": "c3bd1a515859755343494bb3c282ee02728e8825b58dd09cdb5d9bb3f0bb5c47", "text": "step is to make CodeBox production-ready, enabling developers to create public apps as it allows for easy scaling. Currently, each user requires a small sandboxed execution environment, making it challenging to deploy to production with this API. Plans also include making it available using other LLMs) like ClaudeV2 or Open Orca. In time, users may be able to run this API 100% locally and offline on their home PC.\n\nHow to Use It\n\nThe installation is straightforward: get your OpenAI API Key here and install the package using pip. You can use the API in your Python code: start a session, generate a response based on user input - stop your session. You can find detailed instructions on the project's GitHub README.\n\nContribute\n\nIf you're interested in contributing to the Code Interpreter API, there are several opportunities. Have a look into the Issues I put some tagged as ToDo. But you can also work on your ideas and just push a PR. Thanks!\n\nDo Experiments\n\nIf you have not checked out the repository I highly recommend to do so and just try out your prompts! Try out a lot of different stuff to get an idea of what is possible and what is not. If you encounter any bugs please publish them as GitHub Issues and I will try to fix them.\n\nThis is my first blog post ever so thanks for reading this and I hope you had fun! If you want to get updated feel free to check out my Twitter @shroominic.", "start_char_idx": 5002, "end_char_idx": 6406, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db911fa6-8dbe-44cf-804f-f102990d879a": {"__data__": {"id_": "db911fa6-8dbe-44cf-804f-f102990d879a", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_conversational-retrieval-agents_.txt", "file_name": "blog.langchain.dev_conversational-retrieval-agents_.txt", "file_type": "text/plain", "file_size": 5988, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "35d71bcd-d63b-4bd6-9b22-72c0dd3093ae", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_conversational-retrieval-agents_.txt", "file_name": "blog.langchain.dev_conversational-retrieval-agents_.txt", "file_type": "text/plain", "file_size": 5988, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "d1836592af2222d2f3493290942b64fa792bbefda43b0444b49929fc785db324", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "892ca795-9525-4782-b037-3c622852c571", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_code-interpreter-api_.txt", "file_name": "blog.langchain.dev_code-interpreter-api_.txt", "file_type": "text/plain", "file_size": 6414, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "c3bd1a515859755343494bb3c282ee02728e8825b58dd09cdb5d9bb3f0bb5c47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b1779f68-717f-429c-8002-e0586889db7b", "node_type": "1", "metadata": {}, "hash": "d1344f8e901f4c386b1b49e9a3c61f9ac07314481e81c4f63a923f1295ac235c", "class_name": "RelatedNodeInfo"}}, "hash": "dd5bd2f37675d630681523e4345edbb4e873862b0d4cc2c0068bda3085194016", "text": "URL: https://blog.langchain.dev/conversational-retrieval-agents/\nTitle: Conversational Retrieval Agents\n\nTL;DR: There have been several emerging trends in LLM applications over the past few months: RAG, chat interfaces, agents. Our newest functionality - conversational retrieval agents - combines them all. This isn't just a case of combining a lot of buzzwords - it provides real benefits and superior user experience.\n\nKey Links:\n\nAs LLM applications are starting to make their way into more and more production use cases, a few common trends are starting emerge:\n\nRetrieval Augmented Generation\n\nLLMs only know what they are trained on. To combat this, a style of generation known as \"retrieval augmented generation\" has emerged. In this technique, documents are retrieved and then inserted into the prompt, and the language model is instructed to only respond based on those documents. This helps both in giving the language model additional context as well as in keeping it grounded.\n\nChat Interfaces\n\nWith the explosion of ChatGPT, chat interfaces have emerged as the dominant way with which to interact with language models. The ability to ask follow up questions about a previous response - especially as context windows grow longer and longer - proves invaluable.\n\nAgents\n\nThe term agents may be overloaded by now. By \"agents\" we mean a system where the sequence of steps is NOT known ahead of time, but is rather determined by a language model. This can allow the system greater flexibility in dealing with edge cases. However, if unbounded it can become quite unreliable.\n\nAt LangChain, we have had components for these trends from the very beginning. One of our first applications built was a RetrievalQA system over a Notion database. We've experimented and pushed the boundary with many different forms of memory, enabling chatbots of all kinds. And - of course - we've got many types of agents, from the \"old\" ones that use ReAct style prompting, to newer ones powered by OpenAI Functions.\n\nWe've also combined these ideas before. ConversationalRetrievalQA - a chatbot that does a retrieval step to start - is one of our most popular chains. From almost the beginning we've added support for memory in agents.\n\nYet we've never really put all three of these concepts together. Until now. With our conversational retrieval agents we capture all three aspects. Let's dive into what exactly this consists of, and why this is the superior retrieval system.\n\nThe basic outline of this system involves:\n\nAn OpenAI Functions agent\n\nTools that are themselves retrievers - they take in a string, and return a list of documents\n\n- they take in a string, and return a list of documents A new type of memory that not only remembers human <-> ai interactions, but also ai <-> tool interactions\n\nThe agent can then decide when to call the retrieval system if at all. If it does, the retrieved documents are returned and it can use them to reason about what to do next, whether it be respond directly or do a different retrieval step. Note that this relies upon a few things:\n\nLonger context windows: if context windows are short, then you can't just return all the documents into the agent's working memory\n\nBetter language models: if language models aren't good enough to reason about when they should retrieve documents, then this won't work\n\nLuckily, language models are getting better and getting longer context windows!\n\nHere's a LangSmith trace showing how it looks in action:\n\nhttps://smith.langchain.com/public/1e2b1887-ca44-4210-913b-a69c1b8a8e7e/r\n\nLet's compare this to the ConversationalRetrievalQA chain that most people use. The benefits that a conversational retrieval agent has are:\n\nDoesn't always look up documents in the retrieval system. Sometimes, this isn't needed! If the user is just saying \"hi\", you shouldn't have to look things up Can do multiple retrieval steps. In ConversationalRetrievalQA , one retrieval step is done ahead of time. If that retrieval step returns bad results, then you're out of luck! But with an agent you can try a different search query to see if that yields better results With this new type of memory, you can maybe avoid retrieval steps! This is because it remembers ai <-> tool interactions, and therefore remembers previous retrieval results. If a follow-up question the user asks can be answered by those, there's no need to do another retrieval step! Better support for meta-questions about the conversation - \"how many questions have I asked?\", etc. Because the old chain dereferences questions to be \"standalone\" and independent of the conversation history in order to query the vector store effectively, it struggles with this type of question.\n\nNote, that there are some downsides/dangers:\n\nWith agents, they can occasionally spiral out of control. That's why we've added", "start_char_idx": 0, "end_char_idx": 4834, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1779f68-717f-429c-8002-e0586889db7b": {"__data__": {"id_": "b1779f68-717f-429c-8002-e0586889db7b", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_conversational-retrieval-agents_.txt", "file_name": "blog.langchain.dev_conversational-retrieval-agents_.txt", "file_type": "text/plain", "file_size": 5988, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "35d71bcd-d63b-4bd6-9b22-72c0dd3093ae", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_conversational-retrieval-agents_.txt", "file_name": "blog.langchain.dev_conversational-retrieval-agents_.txt", "file_type": "text/plain", "file_size": 5988, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "d1836592af2222d2f3493290942b64fa792bbefda43b0444b49929fc785db324", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db911fa6-8dbe-44cf-804f-f102990d879a", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_conversational-retrieval-agents_.txt", "file_name": "blog.langchain.dev_conversational-retrieval-agents_.txt", "file_type": "text/plain", "file_size": 5988, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "dd5bd2f37675d630681523e4345edbb4e873862b0d4cc2c0068bda3085194016", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a79e45b-7652-45a5-b905-fcc898844cbf", "node_type": "1", "metadata": {}, "hash": "e884ccc308a0422ede8dbadf642e844a89c771c55629d685ef3895946336db96", "class_name": "RelatedNodeInfo"}}, "hash": "d1344f8e901f4c386b1b49e9a3c61f9ac07314481e81c4f63a923f1295ac235c", "text": "agents, they can occasionally spiral out of control. That's why we've added controls to our AgentExecutor to cap them at a certain max amount of steps. It's also worth noting that this is a VERY focused agent, in that it's only given one tool (and a pretty simple tool at that). In general, the fewer (and simpler) tools an agent is given, the more likely it is to be reliable. By remembering ai <-> tool interactions, that can hog the context window occasionally. That's why we've included a flag to disable that type of memory, and more generally have made memory pretty plug-and-play.\n\nThis new agent is in both Python and JS - you can use these guides to get started:\n\nLLM applications are rapidly evolving. Our NotionQA demo was one of the first we did - and although it was only ~9 months ago the best practices have shifted dramatically since then. This currently represents our best guess at what a GenAI question-answering system should look like, combining the grounded-ness of RAG with the UX of chat and the flexibility of agents.\n\nWe've got a few more ideas on how this can be further improved - we'll be rolling those out over the next few weeks. As always, we'd love to hear from you with any suggestions or ideas.", "start_char_idx": 4759, "end_char_idx": 5988, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a79e45b-7652-45a5-b905-fcc898844cbf": {"__data__": {"id_": "3a79e45b-7652-45a5-b905-fcc898844cbf", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_eden-ai-x-langchain_.txt", "file_name": "blog.langchain.dev_eden-ai-x-langchain_.txt", "file_type": "text/plain", "file_size": 14536, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f66c3f8b-0a4d-4cfd-a4b8-f323ab565cec", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_eden-ai-x-langchain_.txt", "file_name": "blog.langchain.dev_eden-ai-x-langchain_.txt", "file_type": "text/plain", "file_size": 14536, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "51720e99b3d2a89a60599ec086aaf403e002facfc46929b16b85fb9c08ac6eee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b1779f68-717f-429c-8002-e0586889db7b", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_conversational-retrieval-agents_.txt", "file_name": "blog.langchain.dev_conversational-retrieval-agents_.txt", "file_type": "text/plain", "file_size": 5988, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "d1344f8e901f4c386b1b49e9a3c61f9ac07314481e81c4f63a923f1295ac235c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00acb849-90b0-4d58-bc95-4d544b6fc949", "node_type": "1", "metadata": {}, "hash": "9b78afd5ba7e2d9b613ebd7e8996a5f022128ea16071fe7f010768dbb0b95ff5", "class_name": "RelatedNodeInfo"}}, "hash": "e884ccc308a0422ede8dbadf642e844a89c771c55629d685ef3895946336db96", "text": "URL: https://blog.langchain.dev/eden-ai-x-langchain/\nTitle: Eden AI x LangChain: Harnessing LLMs, Embeddings, and AI\n\nEditor's Note: This post was written in collaboration with the Eden AI team. We're really excited about Eden's approach to simplifying AI implementation so that we can get more applications into production! It grants access to a diverse range of AI capabilities, spanning text and image generation, OCR, speech-to-text, and image analysis, all with the convenience of a single API key and minimal code. And their integration with LangChain provides effortless access to lots of LLMs and Embeddings.\n\nIntroducing Eden AI: Pioneering AI Accessibility\n\nEden AI stands as a new revolutionary platform meant to deal with the growing complexity and diversity of AI solutions, which allows users to access a large variety of AI tools using a single API key and just a few lines of code.\n\nWhether you need Text or Image generation, OCR (Optical Character Recognition), Speech-to-Text conversion, Image Analysis, or more, Eden AI has got you covered. Gone are the days of navigating a complex maze of APIs and authentication processes; Eden AI consolidates it all into one convenient platform.\n\n\n\n\n\nGet your API key for FREE\n\nDesigned to be user-friendly and accessible to individuals of all proficiency levels, whether they are AI novices or experts, Eden AI seamlessly addresses a diverse spectrum of business requirements, including but not limited to: Data analysis, NLP capabilities, Computer Vision, Automation Optimization, and Custom model training.\n\n\n\nEden AI and LangChain: a powerful AI integration partnership\n\nLangChain is an open-source library that provides multiple tools to build applications powered by Large Language Models (LLMs), making it a perfect combination with Eden AI.\n\nWithin the LangChain ecosystem, Eden AI empowers users to fully leverage LLM providers without encountering any limitations. Here is how:\n\n\n\n1. A unified platform to access multiple LLMs and Embeddings\n\nEach LLM possesses unique strengths that make it suitable for specific use cases. However, finding the liberty to move between the best LLMs in the market can be challenging.\n\n\n\nBy integrating with LangChain, Eden AI opens the door to an extensive array of LLM and Embedding models. This integration empowers users to harness the capabilities of various providers, even models that are not directly integrated into LangChain's framework.\n\nThe core strength of this combination lies in its simplicity. With just one API key and a single line of code, LangChain users can tap into a diverse range of LLMs through Eden AI. This not only enhances LangChain's models but also provides great flexibility and adaptability to cater to different AI requirements.\n\n\n\n2. A robust dashboard to optimize your AI investments\n\nEden AI doesn't stop at simplifying access to AI models; it also offers robust monitoring and cost management features.\n\n\n\nWith our intuitive dashboard, you have the power to monitor your AI usage among multiple AI APIs, gain insights into resource allocation, and optimize costs effectively. Additionally, you\u2019ll have access to features such as logging for enhanced debugging and API caching to reduce usage and avoid redundant charges.\n\nThis streamlined approach to cost management ensures that you get the most out of your AI investment without any surprises in your budget.\n\n\n\n3. Advanced AI capabilities to enhance your applications\n\nThe integration of Eden AI into LangChain represents a significant breakthrough for developers working with LangChain's Agent Tools, empowering them to leverage more advanced capabilities to enhance their applications.\n\n\n\nLangChain Agents act as intermediaries between LLMs and various tools, facilitating a wide range of tasks in AI-powered applications, such as web searches, calculations, and code execution. They are especially crucial for creating versatile and responsive applications, allowing developers to execute functions dynamically and interact with external APIs based on specific user queries.\n\n\n\nThe key benefit of this integration is that LangChain users can now incorporate these advanced tools into their applications with ease, including features like Explicit Content Detection for both text and images, Invoice and ID parsing, Object Detection, Text-to-Speech, and Speech-to-Text.\n\n\n\nConsequently, this partnership enables developers to enhance their applications with the best AI models and providers, all accessible via a standard API key, thereby delivering an unprecedented level of versatility and responsiveness in executing various functions and interacting with external APIs.\n\nHow to use Eden AI LLMs and Embedding models into LangChain?\n\nHere are not one, but two tutorials that will empower you to redefine the way you approach AI-powered applications. If you\u2019re looking for a basic starter with Eden AI's LLMs and Embeddings, we advise you to follow the first tutorial. On the other hand, if you\u2019re interested in advanced integration, you can proceed directly to the second tutorial!\n\n\n\nTutorial 1: Get started with Eden AI to access multiple LLMs and Embeddings\n\nIn our first tutorial, you will learn how to harness the combined power of LangChain and Eden AI to access", "start_char_idx": 0, "end_char_idx": 5265, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00acb849-90b0-4d58-bc95-4d544b6fc949": {"__data__": {"id_": "00acb849-90b0-4d58-bc95-4d544b6fc949", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_eden-ai-x-langchain_.txt", "file_name": "blog.langchain.dev_eden-ai-x-langchain_.txt", "file_type": "text/plain", "file_size": 14536, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f66c3f8b-0a4d-4cfd-a4b8-f323ab565cec", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_eden-ai-x-langchain_.txt", "file_name": "blog.langchain.dev_eden-ai-x-langchain_.txt", "file_type": "text/plain", "file_size": 14536, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "51720e99b3d2a89a60599ec086aaf403e002facfc46929b16b85fb9c08ac6eee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a79e45b-7652-45a5-b905-fcc898844cbf", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_eden-ai-x-langchain_.txt", "file_name": "blog.langchain.dev_eden-ai-x-langchain_.txt", "file_type": "text/plain", "file_size": 14536, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "e884ccc308a0422ede8dbadf642e844a89c771c55629d685ef3895946336db96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a936117a-b9af-45d5-b7a4-17ac50e9d0fb", "node_type": "1", "metadata": {}, "hash": "266160ce16530c1c4d028b5f0ba4f4e8c50844090a8a9482644d31a1fb1ea7cc", "class_name": "RelatedNodeInfo"}}, "hash": "9b78afd5ba7e2d9b613ebd7e8996a5f022128ea16071fe7f010768dbb0b95ff5", "text": "first tutorial, you will learn how to harness the combined power of LangChain and Eden AI to access multiple Large Language Models (LLMs) and Embeddings.\n\n\n\nBy mastering the intricacies of embeddings and LLMs, you will unlock the capability to craft a diverse array of functionalities. From building a basic AI assistant to creating custom chatbots, the possibilities are limited only by your imagination.\n\n\n\nStep 1: Installation\n\nFirst, ensure you have Python installed. Then, install LangChain by running the following command:\n\nPip install langchain\n\n\n\nStep 2: Setting Up Your Eden AI Account\n\nTo start using Eden AI, you'll need to create an account on the Eden AI platform. Once you have an account, set your API KEY as an environment variable by running:\n\n\n\nexport Eden AI_API_KEY=\"your_api_key_here\"\n\n\n\n\n\nStep 3: Importing Eden AI LLMs and Embeddings\n\nThe Eden AI API brings together various providers, each offering multiple models. Let's import the necessary modules for Eden AI LLMs and Embeddings:\n\n\n\nfrom langchain.llms import EdenAI\n\nfrom langchain.embeddings.edenai import EdenAiEmbeddings\n\n\n\n\n\nStep 4: Using Eden AI LLMs\n\nNow, let\u2019s instantiate an Eden AI LLM, in this case, OpenAI\u2019s. Eden AI LLMs can be configured with multiple providers.\n\n\n\n```\n\nllm=EdenAI(provider=\"openai\", params={\"temperature\" : 0.2,\"max_tokens\" : 250})\n\nprompt = \"\"\"\n\nhow can i create ai powered chatbots with LLMS\n\n\"\"\n\nllm(prompt)\n\n\n\n\n\n\n\nWe've asked a question, and the LLM provides a detailed response:\n\n\n\n\"\n\n\n\nCreating an AI-powered chatbot with LLMS is relatively straightforward. First, you need to create a chatbot using the LLMS platform. This involves selecting a template, customizing the chatbot's conversation flow, and setting up the chatbot's natural language processing (NLP) capabilities. Once the chatbot is set up, you can then integrate it with your existing systems, such as customer service software, to enable it to interact with customers. Finally, you can use the LLMS platform to monitor and analyze the chatbot's performance, allowing you to make adjustments as needed.\"\n\n\n\n\n\nYou can see other examples of LLMs and how to set up chains with Eden AI here.\n\n\n\nStep 5: Exploring Eden AI Embeddings\n\nNext, we'll explore Eden AI's embeddings:\n\n\n\nembeddings = EdenAiEmbeddings(provider=\"openai\")\n\n\n\ndocs = [\"Eden AI is integrated in LangChain\", \"AskYoda is Available\"]\n\ndocument_result = embeddings.embed_documents(docs)\n\n\n\n\n\nHere is the response, with float numbers being the representation of the texts we had in input:\n\n\n\n[[0.013804426, -0.0032499523, -0.020794097, -0.01929681, -0.024726225, 0.015966397, -0.04086054, 0.0057792477, 0.0024628271, -0.01493089, 0.0055343644, 0.01719781, 0.008808806, -0.010725892, 0.007696335, 0.034283675, -0.0023963589, -0.006744788, -0.0066433363, 0.015700523, -0.024796192, 0.024334412, -0.018233318, -0.009914279, -0.001967813,\n\n...\n\n0.016727816, 0.0047793766, -0.015208363, -0.019269451, ...]]\n\n\ud83d\ude0e You\u2019re all set! With the knowledge of how to use embeddings and LLMs, you now possess the capability to create an array of impressive functionalities, ranging from basic AI assistants to the development of custom chatbots.\n\nTutorial 2: Supercharge your app with advanced AI capabilities\n\nIn our second tutorial, you will learn how to easily integrate Eden AI features (specifically Document Parsing) into your app.\n\n\n\nThis integration will catapult your applications to a new echelon of versatility and responsiveness, ensuring you remain at the forefront of innovation in the ever-evolving AI landscape.\n\n\n\n\n\nStep 1: Preparing Your Environment\n\nFirst, ensure Python is installed on your system. Then, install LangChain by running the following command:\n\n\n\npip install langchain\n\n\n\n\n\nStep 2: Obtaining an Eden AI API Key\n\nBefore you begin, you'll need an API key from the Eden AI platform.\n\n\n\nStep 3: Importing Necessary Modules\n\nLet's import the modules required for our advanced AI capabilities (here, Parsing ID and Invoice Tools)\n\nfrom langchain.llms import EdenAI\n\nfrom langchain.agents import", "start_char_idx": 5166, "end_char_idx": 9211, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a936117a-b9af-45d5-b7a4-17ac50e9d0fb": {"__data__": {"id_": "a936117a-b9af-45d5-b7a4-17ac50e9d0fb", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_eden-ai-x-langchain_.txt", "file_name": "blog.langchain.dev_eden-ai-x-langchain_.txt", "file_type": "text/plain", "file_size": 14536, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f66c3f8b-0a4d-4cfd-a4b8-f323ab565cec", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_eden-ai-x-langchain_.txt", "file_name": "blog.langchain.dev_eden-ai-x-langchain_.txt", "file_type": "text/plain", "file_size": 14536, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "51720e99b3d2a89a60599ec086aaf403e002facfc46929b16b85fb9c08ac6eee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00acb849-90b0-4d58-bc95-4d544b6fc949", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_eden-ai-x-langchain_.txt", "file_name": "blog.langchain.dev_eden-ai-x-langchain_.txt", "file_type": "text/plain", "file_size": 14536, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "9b78afd5ba7e2d9b613ebd7e8996a5f022128ea16071fe7f010768dbb0b95ff5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f3c4c837-50cc-4896-9123-106298446286", "node_type": "1", "metadata": {}, "hash": "77422305f3ef7b17687df17585c42495ab8c19c95edef88c84e520d694806bdb", "class_name": "RelatedNodeInfo"}}, "hash": "266160ce16530c1c4d028b5f0ba4f4e8c50844090a8a9482644d31a1fb1ea7cc", "text": "ID and Invoice Tools)\n\nfrom langchain.llms import EdenAI\n\nfrom langchain.agents import initialize_agent, AgentType\n\nfrom langchain.tools.edenai import (\n\nEdenAiParsingIDTool,\n\nEdenAiParsingInvoiceTool\n\n)\n\n\n\n\n\nimport os\n\n\n\n\n\n\n\nStep 4: Setting Up you Eden AI API key\n\nSet your Eden AI API key as an environment variable in your system. Replace it with your own API Key.\n\nos.environ['Eden AI_API_KEY'] = \"*******************\" # replace with your own API Key\n\n\n\n\n\nStep 5. Initializing the LLM\n\nEden AI provides a range of providers, which you can explore here. For this tutorial, we'll choose Eden AI LLM to setup the LLM provider (here, OpenAI, text-davinci-003):\n\n\n\nllm=EdenAI(provider=\"openai\", model=\"text-davinci-003\", params={\"temperature\" : 0.2,\"max_tokens\" : 250})\n\n\n\n\n\nStep 6. Setting Up Tools and the Agent\n\nNow, it's time to configure the tools and the agent:\n\n\n\ntools = [\n\nEdenAiParsingIDTool(providers=[\"amazon\",\"klippa\"],language=\"en\"),\n\nEdenAiParsingInvoiceTool(providers=[\"amazon\",\"google\"],language=\"en\"),\n\n]\n\n\n\nagent_chain = initialize_agent(\n\ntools,\n\nllm,\n\nagent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n\nverbose=True,\n\nreturn_intermediate_steps=True,\n\n)\n\n\n\n\n\nStep 7: Executing the Agent\n\nLet's put our agent to work with the Doc Parsing Bot to analyze identification or invoice documents.\n\n\n\nOur data consists of 2 image URLs:\n\nThe ID image:\n\nhttps://www.citizencard.com/images/sample-cards/uk-id-card-for-over-18s-2023.png\n\nThe invoice image: \u201chttps://app.edenai.run/assets/img/data_1.72e3bdcc.png\u201d\n\nNow, let\u2019s extract the information from the ID and create a welcoming text:\n\nid_result=agent_chain(\"\"\" i have this url of an id: \"https://www.citizencard.com/images/sample-cards/uk-id-card-for-over-18s-2023.png\"\n\nextract the information in it.\n\ncreate a text welcoming the person.\n\n\"\"\")\n\n\n\n> Entering new AgentExecutor chain...\n\n\n\n\n\nThe result:\n\nAction: Eden AI_identity_parsing\n\nAction Input: \"https://www.citizencard.com/images/sample-cards/uk-id-card-for-over-18s-2023.png\" Observation:\n\nlast_name : value : ANGELA\n\ngiven_names : value : GREENE\n\nbirth_place :\n\nbirth_date : value : 2000-11-09\n\nissuance_date :\n\nexpire_date : value : 2025-07-31\n\ndocument_id : value : 5843\n\nissuing_state :\n\naddress :\n\nage :\n\ncountry :\n\ndocument_type : value : DRIVER LICENSE FRONT\n\ngender :\n\n\n\nThought: I now have the information from the ID and can create a welcoming text. Final Answer: Welcome Angela Greene!\n\n\n\n\n\nThen, let\u2019s extract the information from the invoice and summarize it:\n\n\n\ninvoice_result=agent_chain(\"\"\" i have this url of an invoice document: \"https://app.Eden AI.run/assets/img/data_2.d6af6d85.png\"\n\nextract the information in it.\n\nSummarize them.\n\n\"\"\")\n\n\n\n> Entering new AgentExecutor chain...\n\nThe result:\n\nAction: Eden AI_invoice_parsing\n\nAction Input: \"https://app.Eden AI.run/assets/img/data_2.d6af6d85.png\"\n\nObservation:\n\ncustomer_information :\n\ncustomer_name : Wiseman Water\n\ncustomer_address : Wiseman Water,151 Narrows Parkway,Birmingham West Midlands B11,United Kingdom\n\nmerchant_information :\n\nmerchant_name : Gravity PDF\n\nmerchant_address : ABN: 74 212 487 581,48 Federation Way,Telegraph Point NSW 2441, Australia\n\nmerchant_website : gravitypdf.com\n\nmerchant_tax_id : 74 212 487 581\n\ninvoice_number : PDF47-WEB\n\ntaxes :\n\ndate : 2017-01-31\n\nlocale :\n\ncurrency : GBP\n\nbank_informations :\n\nitem_lines :\n\nproduct_code : Laptop\n\ndescription : Laptop Upgrades: 2GB Extra Ram,\n\nLaptop Upgrades: Second 512GB Hard Drive\n\ndescription : Accessories,Accessories: Laser Mouse / Keyboard Combo\n\n\n\nThought: I now have all the information from the invoice\n\n\n\nFinal Answer: The invoice from Wiseman Water to Gravity PDF (invoice number PDF47-WEB) contains the following items: Laptop Upgrades: 2GB Extra Ram, Laptop Upgrades: Second 512GB Hard Drive, Accessories: Laser Mouse / Keyboard Combo. The merchant is Gravity PDF, located at ABN: 74 212 487 581, 48 Federation Way, Telegraph Point NSW 2441, Australia, with website", "start_char_idx": 9125, "end_char_idx": 13066, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f3c4c837-50cc-4896-9123-106298446286": {"__data__": {"id_": "f3c4c837-50cc-4896-9123-106298446286", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_eden-ai-x-langchain_.txt", "file_name": "blog.langchain.dev_eden-ai-x-langchain_.txt", "file_type": "text/plain", "file_size": 14536, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f66c3f8b-0a4d-4cfd-a4b8-f323ab565cec", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_eden-ai-x-langchain_.txt", "file_name": "blog.langchain.dev_eden-ai-x-langchain_.txt", "file_type": "text/plain", "file_size": 14536, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "51720e99b3d2a89a60599ec086aaf403e002facfc46929b16b85fb9c08ac6eee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a936117a-b9af-45d5-b7a4-17ac50e9d0fb", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_eden-ai-x-langchain_.txt", "file_name": "blog.langchain.dev_eden-ai-x-langchain_.txt", "file_type": "text/plain", "file_size": 14536, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "266160ce16530c1c4d028b5f0ba4f4e8c50844090a8a9482644d31a1fb1ea7cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ddc4174b-3544-4928-a3b1-8882fa9e9cfd", "node_type": "1", "metadata": {}, "hash": "051e89319ed16f654e73bdf0fb2e4b7e6a3739142bd83eebfc14c57b96d579b0", "class_name": "RelatedNodeInfo"}}, "hash": "77422305f3ef7b17687df17585c42495ab8c19c95edef88c84e520d694806bdb", "text": "581, 48 Federation Way, Telegraph Point NSW 2441, Australia, with website gravitypdf.com and tax ID 74 212 487 581. The customer is Wiseman Water, located at 151 Narrows Parkway, Birmingham West Midlands B11, United Kingdom. The invoice was issued on 2017-01-31 and the currency is GBP.\n\n\n\n\n\n\ud83d\udc4f Congrats you\u2019re all done! The integration of Eden AI features into LangChain Tools opens up a world of possibilities for developers. With features such as invoice parsing, ID parsing, as well as object detection, or even explicit content detection, developers can enhance their applications with advanced AI capabilities.\n\n\n\n\n\nConclusion\n\nOverall, Eden AI streamlines the integration of AI, offering a user-friendly platform that simplifies the labyrinth of APIs and authentication. It grants access to a diverse range of AI capabilities, spanning text and image generation, OCR, speech-to-text, and image analysis, all with the convenience of a single API key and minimal code.\n\nThe integration with LangChain further enhances this by providing effortless access to various LLMs and Embeddings. Additionally, Eden AI provides robust cost management features, ensuring you optimize your AI investments effectively.\n\nWhether you're starting with the basics or delving into advanced integration, you now have the tools and knowledge to harness Eden AI and LangChain's capabilities to simplify AI integration and supercharge your applications. Get your API key for FREE and start revolutionizing your AI integration today!", "start_char_idx": 12993, "end_char_idx": 14506, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ddc4174b-3544-4928-a3b1-8882fa9e9cfd": {"__data__": {"id_": "ddc4174b-3544-4928-a3b1-8882fa9e9cfd", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_espilla-x-langchain-retrieval-augmented-generation-rag-in-llm-powered-question-answering-pipelines_.txt", "file_name": "blog.langchain.dev_espilla-x-langchain-retrieval-augmented-generation-rag-in-llm-powered-question-answering-pipelines_.txt", "file_type": "text/plain", "file_size": 5719, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c9c9fbe-b13d-40e4-b964-83cc4cdffdbe", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_espilla-x-langchain-retrieval-augmented-generation-rag-in-llm-powered-question-answering-pipelines_.txt", "file_name": "blog.langchain.dev_espilla-x-langchain-retrieval-augmented-generation-rag-in-llm-powered-question-answering-pipelines_.txt", "file_type": "text/plain", "file_size": 5719, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "7e1a92993e001c0c90a2be04ca4e50ec0ea1f7a25334dc535f9b7698f061eb56", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3c4c837-50cc-4896-9123-106298446286", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_eden-ai-x-langchain_.txt", "file_name": "blog.langchain.dev_eden-ai-x-langchain_.txt", "file_type": "text/plain", "file_size": 14536, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "77422305f3ef7b17687df17585c42495ab8c19c95edef88c84e520d694806bdb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eed170e3-014d-46e1-9cb2-1150e23634b3", "node_type": "1", "metadata": {}, "hash": "ba027ee577dcb41ecf1197c98e8494156fa012995629a71b315709b09947acf1", "class_name": "RelatedNodeInfo"}}, "hash": "051e89319ed16f654e73bdf0fb2e4b7e6a3739142bd83eebfc14c57b96d579b0", "text": "URL: https://blog.langchain.dev/espilla-x-langchain-retrieval-augmented-generation-rag-in-llm-powered-question-answering-pipelines/\nTitle: Epsilla x LangChain: Retrieval Augmented Generation (RAG) in LLM-Powered Question-Answering Pipelines\n\nEditor's Note: This post was written in collaboration with the Epsilla team. As more apps rely on Retrieval Augmented Generation (RAG) for building personalized applications on top of proprietary data, vector databases are becoming even more important. We're really excited about what Epsilla is doing here to help builders quickly and accurately fetch the most relevant documents and data points.\n\nBy leveraging the strengths of both LLMs and vector databases, this integration promises richer, more accurate, and context-aware answers.\n\nThe landscape of artificial intelligence is ever-evolving. As developers and businesses seek more effective ways to utilize Large Language Models (LLMs), integration tools like LangChain are paving the way. In this post, we'll explore Epsilla's recent integration with LangChain and how it revolutionizes the question-answering domain.\n\nRetrieval Augmented Generation (RAG) in LLM-Powered Question-Answering Pipelines\n\nSince October 2022, there has been a huge surge in the adoption and utilization of ChatGPT and other Large Language Models (LLMs). These advanced models have emerged as frontrunners in the realm of artificial intelligence, offering unprecedented capabilities in generating human-like text and understanding nuanced queries. However, despite their prowess, ChatGPT and similar models possess inherent limitations. One of the most significant challenges is their inability to incorporate updated knowledge post their last training cut-off, rendering them unaware of events or developments that have transpired since then. Moreover, while they possess vast general knowledge, they can't access proprietary or private company data, which is often crucial for businesses looking for tailored insights or decision-making. This is where Retrieval Augmented Generation (RAG) steps in as a game-changer. RAG bridges the knowledge gap by dynamically retrieving relevant information from external sources, ensuring that the generated responses are not only factual but also up-to-date. Vector databases play an integral role in the RAG mechanism by enabling efficient and semantic retrieval of information. These databases store information as vectors, allowing RAG to quickly and accurately fetch the most relevant documents or data points based on the semantic similarity of the input query, enhancing the precision and relevance of the LLM's generated responses.\n\n\n\n\n\n\n\nImplementing Question Answering Pipeline with LangChain and Epsilla\n\nLangChain offers a unified interface and abstraction layer on top of LLM ecosystem components, simplifying the process of building generative AI applications. With LangChain, developers can avoid boilerplate code and focus on delivering value.\n\nWith the Epsilla integration with LangChain, now the AI application developers can easily leverage the superior performance provided by Epsilla (benchmark) while building the knowledge retrieval component in the AI applications.\n\nHere is a step by step guide on implementing a question answering pipeline with LangChain and Epsilla.\n\nStep 1. Install LangChain and Epsilla\n\n\n\npip install langchain pip install openai pip install tiktoken pip install pyepsilla\n\ndocker pull epsilla/vectordb docker run --pull=always -d -p 8888:8888 epsilla/vectordb\n\nStep 2. Provide your OpenAI key\n\n\n\nimport os os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\n\n\n\nStep 3. Prepare for knowledge and embedding model\n\n\n\nfrom langchain.embeddings import OpenAIEmbeddings from langchain.document_loaders import WebBaseLoader from langchain.text_splitter import CharacterTextSplitter loader = WebBaseLoader(\"https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt\") documents = loader.load() documents = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0).split_documents(documents) embeddings = OpenAIEmbeddings()\n\n\n\n\n\nStep 4. Vectorize the knowledge documents\n\n\n\n\n\nfrom langchain.vectorstores import Epsilla from pyepsilla import vectordb client = vectordb.Client() vector_store = Epsilla.from_documents( documents, embeddings, client, db_path=\"/tmp/mypath\", db_name=\"MyDB\", collection_name=\"MyCollection\" )\n\n\n\n\n\nStep 5. Create a RetrievalQA chain for question answering on the uploaded knowledge\n\n\n\nfrom langchain.chains import RetrievalQA from langchain.llms import OpenAI qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=vector_store.as_retriever()) query = \"What did the president say about Ketanji Brown Jackson\" qa.run(query)\n\n\n\n\n\nHere", "start_char_idx": 0, "end_char_idx": 4765, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eed170e3-014d-46e1-9cb2-1150e23634b3": {"__data__": {"id_": "eed170e3-014d-46e1-9cb2-1150e23634b3", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_espilla-x-langchain-retrieval-augmented-generation-rag-in-llm-powered-question-answering-pipelines_.txt", "file_name": "blog.langchain.dev_espilla-x-langchain-retrieval-augmented-generation-rag-in-llm-powered-question-answering-pipelines_.txt", "file_type": "text/plain", "file_size": 5719, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c9c9fbe-b13d-40e4-b964-83cc4cdffdbe", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_espilla-x-langchain-retrieval-augmented-generation-rag-in-llm-powered-question-answering-pipelines_.txt", "file_name": "blog.langchain.dev_espilla-x-langchain-retrieval-augmented-generation-rag-in-llm-powered-question-answering-pipelines_.txt", "file_type": "text/plain", "file_size": 5719, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "7e1a92993e001c0c90a2be04ca4e50ec0ea1f7a25334dc535f9b7698f061eb56", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ddc4174b-3544-4928-a3b1-8882fa9e9cfd", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_espilla-x-langchain-retrieval-augmented-generation-rag-in-llm-powered-question-answering-pipelines_.txt", "file_name": "blog.langchain.dev_espilla-x-langchain-retrieval-augmented-generation-rag-in-llm-powered-question-answering-pipelines_.txt", "file_type": "text/plain", "file_size": 5719, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "051e89319ed16f654e73bdf0fb2e4b7e6a3739142bd83eebfc14c57b96d579b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83a93cdf-15ed-4c10-b34d-7df7b67b373a", "node_type": "1", "metadata": {}, "hash": "0adbe6d7868a43663ad211f3d0cba8a6d215f44ed6aff0cb699368722112aa20", "class_name": "RelatedNodeInfo"}}, "hash": "ba027ee577dcb41ecf1197c98e8494156fa012995629a71b315709b09947acf1", "text": "query = \"What did the president say about Ketanji Brown Jackson\" qa.run(query)\n\n\n\n\n\nHere is the response:\n\nThe president said that Ketanji Brown Jackson is one of the nation's top legal minds, a former top litigator in private practice, a former federal public defender, from a family of public school educators and police officers, a consensus builder, and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\n\n\n\n\n\nConclusion\n\nEpsilla's integration with LangChain signifies a leap forward in the domain of question-answering systems. By leveraging the strengths of both LLMs and vector databases, this integration promises richer, more accurate, and context-aware answers. As AI continues to reshape our world, tools like LangChain, coupled with powerful vector databases like Epsilla, will be at the forefront of this transformation.\n\nFor those eager to dive deeper, LangChain's source code and implementation details with Epsilla are available on Google Colab.", "start_char_idx": 4677, "end_char_idx": 5717, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83a93cdf-15ed-4c10-b34d-7df7b67b373a": {"__data__": {"id_": "83a93cdf-15ed-4c10-b34d-7df7b67b373a", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt", "file_name": "blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt", "file_type": "text/plain", "file_size": 15613, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "658b3e2b-bd12-4119-9520-c8386c71a9c9", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt", "file_name": "blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt", "file_type": "text/plain", "file_size": 15613, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "fdede87fbeebb677d1dddedec8993a4746bc1e2c61312b11ae46d688eadd0cc6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eed170e3-014d-46e1-9cb2-1150e23634b3", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_espilla-x-langchain-retrieval-augmented-generation-rag-in-llm-powered-question-answering-pipelines_.txt", "file_name": "blog.langchain.dev_espilla-x-langchain-retrieval-augmented-generation-rag-in-llm-powered-question-answering-pipelines_.txt", "file_type": "text/plain", "file_size": 5719, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "ba027ee577dcb41ecf1197c98e8494156fa012995629a71b315709b09947acf1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e19efa8-f10c-4f24-9936-3988ecbd728f", "node_type": "1", "metadata": {}, "hash": "f49caade165d0e77bd4b2ecdec63dd01369dd9ad7be145930f9c1d52a96da415", "class_name": "RelatedNodeInfo"}}, "hash": "0adbe6d7868a43663ad211f3d0cba8a6d215f44ed6aff0cb699368722112aa20", "text": "URL: https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/\nTitle: Evaluating RAG pipelines with Ragas + LangSmith\n\nEditor's Note: This post was written in collaboration with the Ragas team. One of the things we think and talk about a lot at LangChain is how the industry will evolve to identify new monitoring and evaluation metrics that evolve beyond traditional ML ops metrics. Ragas is an exciting new framework that helps developers evaluate QA pipelines in new ways. This post shows how LangSmith and Ragas can be a powerful combination for teams that want to build reliable LLM apps.\n\nHow important evals are to the team is a major differentiator between folks rushing out hot garbage and those seriously building products in the space.\n\nThis HackerNews comment emphasizes the importance of having a robust eval strategy when taking your application from a cool demo to a production-ready product. This is especially true when building LLM applications because of the underlying stochastic nature of the models powering them. You can easily build really impressive LLM applications with a few short training and verifying it on a few examples but to make it more robust you have to test it on a test dataset that matches the production distribution. This is an evolutionary process and something you will be doing throughout the lifecycle of your application but let's see how Ragas and LangSmith can help you here.\n\nThis article is going to be specifically about QA systems (or RAG systems). Every QA pipeline has 2 components\n\nRetriever - which retrieves the most relevant information needed to answer the query Generator - which generates the answer with the retrieved information.\n\nWhen evaluating a QA pipeline you have to evaluate both of these separately and together to get an overall score as well as the individual scores to pinpoint the aspects you want to improve. For example, using a better chunking strategy or embedding model can improve the retriever while using a different model or prompt can bring about changes to the generation step. You might want to measure the tendency for hallucinations in the generation step or the recall for the retrieval step.\n\nBut what metrics and datasets can you use to measure and benchmark these components? There are many conventional metrics and benchmarks for evaluating QA systems like ROUGE and BLUE but they have poor correlation with human judgment. Furthermore, the correlation between the performance of your pipeline over a standard benchmark (like Beir) and production data can vary depending on the distribution shift between both of them. Moreover, building such a golden test set is an expensive and time-consuming task.\n\nLeveraging LLMs for evaluations\n\nLeveraging a strong LLM for reference-free evaluation is an upcoming solution that has shown a lot of promise. They correlate better with human judgment than traditional metrics and also require less human annotation. Papers like G-Eval have experimented with this and given promising results but there are certain shortcomings too.\n\nLLM prefers outputs their own outputs and when asked to compare between different outputs the relative position of those outputs matters more. LLMs can also have a bias toward a value when asked to score given a range and they also prefer longer responses. Refer to the Large Language Models are not Fair Evaluators paper for more. Ragas aims to work around these limitations of using LLMs to evaluate your QA pipelines while also providing actionable metrics using as little annotated data as possible, cheaper, and faster.\n\nIntroducing Ragas\n\nRagas is a framework that helps you evaluate your QA pipelines across these different aspects. It provides you with a few metrics to evaluate the different aspects of your QA systems namely\n\nmetrics to evaluate retrieval: offers context_relevancy and context_recall which give you the measure of the performance of your retrieval system. metrics to evaluate generation: offers faithfulness which measures hallucinations and answer_relevancy which measures how to the point the answers are to the question.\n\nThe harmonic mean of these 4 aspects gives you the ragas score which is a single measure of the performance of your QA system across all the important aspects.\n\nMost of the measurements do not require any labeled data, making it easier for users to run it without worrying about building a human-annotated test dataset first. In order to run ragas all you need is a few questions and if your using context_recall , a reference answer. (The option to cold start your test dataset is also in the roadmap)\n\nNow let's see Ragas in action and try this by evaluating a QA chain build with Langchain.\n\nEvaluating a QA chain\n\nWe\u2019re going to build a QA chain over the NYC Wikipedia page and run our evaluations on top of it. This is a pretty standard QA chain but feel free to check out the docs.\n\nfrom langchain.document_loaders import WebBaseLoader from langchain.indexes import VectorstoreIndexCreator from langchain.chains import RetrievalQA from langchain.chat_models import ChatOpenAI # load the Wikipedia page and create index loader =", "start_char_idx": 0, "end_char_idx": 5167, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e19efa8-f10c-4f24-9936-3988ecbd728f": {"__data__": {"id_": "1e19efa8-f10c-4f24-9936-3988ecbd728f", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt", "file_name": "blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt", "file_type": "text/plain", "file_size": 15613, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "658b3e2b-bd12-4119-9520-c8386c71a9c9", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt", "file_name": "blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt", "file_type": "text/plain", "file_size": 15613, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "fdede87fbeebb677d1dddedec8993a4746bc1e2c61312b11ae46d688eadd0cc6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83a93cdf-15ed-4c10-b34d-7df7b67b373a", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt", "file_name": "blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt", "file_type": "text/plain", "file_size": 15613, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "0adbe6d7868a43663ad211f3d0cba8a6d215f44ed6aff0cb699368722112aa20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67c27eb4-00b7-40ab-9b44-65b72798cca1", "node_type": "1", "metadata": {}, "hash": "95034995580cc4ca03eaa2e31c5b5ec82af5db4f7a4b32198f566d1842be9ba7", "class_name": "RelatedNodeInfo"}}, "hash": "f49caade165d0e77bd4b2ecdec63dd01369dd9ad7be145930f9c1d52a96da415", "text": "from langchain.chat_models import ChatOpenAI # load the Wikipedia page and create index loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/New_York_City\") index = VectorstoreIndexCreator().from_loaders([loader]) # create the QA chain llm = ChatOpenAI() qa_chain = RetrievalQA.from_chain_type( llm, retriever=index.vectorstore.as_retriever(), return_source_documents=True ) # testing it out question = \"How did New York City get its name?\" result = qa_chain({\"query\": question}) result[\"result\"] # output # 'New York City got its name in 1664 when it was renamed after the Duke of York, who later became King James II of England. The city was originally called New Amsterdam by Dutch colonists and was renamed New York when it came under British control.'\n\nin order to use Ragas with LangChain, first import all the metrics you want to use from ragas.metrics. Next import the RagasEvaluatorChain which is a langchain chain wrapper to convert a ragas metric into a langchain EvaluationChain.\n\nfrom ragas.metrics import faithfulness, answer_relevancy, context_relevancy, context_recall from ragas.langchain import RagasEvaluatorChain # make eval chains eval_chains = { m.name: RagasEvaluatorChain(metric=m) for m in [faithfulness, answer_relevancy, context_relevancy, context_recall] }\n\nOnce the evaluator chains are created you can call the chain via the __call__() method with the outputs from the QA chain to run the evaluations\n\n# evaluate for name, eval_chain in eval_chains.items(): score_name = f\"{name}_score\" print(f\"{score_name}: {eval_chain(result)[score_name]}\") # output # faithfulness_score: 1.0 # answer_relevancy_score: 0.9193459596511587 # context_relevancy_score: 0.07480974380786602 # context_recall_score: 0.9193459596511587\n\nRagas uses LLMs under the hood to do the evaluations but leverages them in different ways to get the measurements we care about while overcoming the biases they have. Let's learn more about how they work under the hood to see how.\n\nUnder the hood\n\nOptional, not required to follow the next steps but helps understand the inner workings of ragas.\n\nAll the metrics are documented here but in this section let's try and understand how exactly each Ragas metric works.\n\nFaithfulness: measures the factual accuracy of the generated answer with the context provided. This is done in 2 steps. First, given a question and generated answer, Ragas uses an LLM to figure out the statements that the generated answer makes. This gives a list of statements whose validity we have we have to check. In step 2, given the list of statements and the context returned, Ragas uses an LLM to check if the statements provided are supported by the context. The number of correct statements is summed up and divided by the total number of statements in the generated answer to obtain the score for a given example. Answer Relevancy: measures how relevant and to the point the answer is to the question. For a given generated answer Ragas uses an LLM to find out the probable questions that the generated answer would be an answer to and computes similarity to the actual question asked. Context Relevancy: measures the signal-to-noise ratio in the retrieved contexts. Given a question, Ragas calls LLM to figure out sentences from the retrieved context that are needed to answer the question. A ratio between the sentences required and the total sentences in the context gives you the score Context Recall: measures the ability of the retriever to retrieve all the necessary information needed to answer the question. Ragas calculates this by using the provided ground_truth answer and using an LLM to check if each statement from it can be found in the retrieved context. If it is not found that means the retriever was not able to retrieve the information needed to support that statement.\n\nUnderstanding how each Ragas metric works gives you clues as to how the evaluation was performed making these metrics reproducible and more understandable. One easy way to visualize the results from Ragas is to use the traces from LangSmith and LangSmith\u2019s evaluation features. Let's look more into that now\n\nVisualising the Evaluations with LangSmith\n\nWhile Ragas provides you with a few insightful metrics, it does not help you in the process of continuously evaluation of your QA pipeline in production. But this is where LangSmith comes in.\n\nLangSmith is a platform that helps to debug, test, evaluate, and monitor chains and agents built on any LLM framework. LangSmith offers the following benefits\n\na platform to create and store a test dataset and run evaluations. a platform to visualise and dig into the evaluation results. Makes Ragas", "start_char_idx": 5071, "end_char_idx": 9732, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67c27eb4-00b7-40ab-9b44-65b72798cca1": {"__data__": {"id_": "67c27eb4-00b7-40ab-9b44-65b72798cca1", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt", "file_name": "blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt", "file_type": "text/plain", "file_size": 15613, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "658b3e2b-bd12-4119-9520-c8386c71a9c9", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt", "file_name": "blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt", "file_type": "text/plain", "file_size": 15613, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "fdede87fbeebb677d1dddedec8993a4746bc1e2c61312b11ae46d688eadd0cc6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e19efa8-f10c-4f24-9936-3988ecbd728f", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt", "file_name": "blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt", "file_type": "text/plain", "file_size": 15613, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "f49caade165d0e77bd4b2ecdec63dd01369dd9ad7be145930f9c1d52a96da415", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77883c74-6a43-4770-99ad-15367ee48e74", "node_type": "1", "metadata": {}, "hash": "0ee5f92abd275721249953e04957712aab509b29bf24ae795a810c320327fbd7", "class_name": "RelatedNodeInfo"}}, "hash": "95034995580cc4ca03eaa2e31c5b5ec82af5db4f7a4b32198f566d1842be9ba7", "text": "dataset and run evaluations. a platform to visualise and dig into the evaluation results. Makes Ragas metrics explainable and reproducible. The ability to continuously add test examples from production logs if your app is also monitored with LangSmith.\n\nWith RagasEvaluatorChain you can use the Ragas metrics for running LangSmith evaluations as well. To know more about LangSmith evaluations check out the quickstart.\n\nLet's start with your need to create a dataset inside LangSmith to store the test examples. We\u2019re going to start off with a small dataset with 5 questions and answers to those questions (used only for the context_recall metric).\n\n# test dataset eval_questions = [ \"What is the population of New York City as of 2020?\", \"Which borough of New York City has the highest population?\", \"What is the economic significance of New York City?\", \"How did New York City get its name?\", \"What is the significance of the Statue of Liberty in New York City?\", ] eval_answers = [ \"8,804,000\", # incorrect answer \"Queens\", # incorrect answer \"New York City's economic significance is vast, as it serves as the global financial capital, housing Wall Street and major financial institutions. Its diverse economy spans technology, media, healthcare, education, and more, making it resilient to economic fluctuations. NYC is a hub for international business, attracting global companies, and boasts a large, skilled labor force. Its real estate market, tourism, cultural industries, and educational institutions further fuel its economic prowess. The city's transportation network and global influence amplify its impact on the world stage, solidifying its status as a vital economic player and cultural epicenter.\", \"New York City got its name when it came under British control in 1664. King Charles II of England granted the lands to his brother, the Duke of York, who named the city New York in his own honor.\", 'The Statue of Liberty in New York City holds great significance as a symbol of the United States and its ideals of liberty and peace. It greeted millions of immigrants who arrived in the U.S. by ship in the late 19th and early 20th centuries, representing hope and freedom for those seeking a better life. It has since become an iconic landmark and a global symbol of cultural diversity and freedom.', ] examples = [{\"query\": q, \"ground_truths\": [eval_answers[i]]} for i, q in enumerate(eval_questions)]\n\n# dataset creation from langsmith import Client from langsmith.utils import LangSmithError client = Client() dataset_name = \"NYC test\" try: # check if dataset exists dataset = client.read_dataset(dataset_name=dataset_name) print(\"using existing dataset: \", dataset.name) except LangSmithError: # if not create a new one with the generated query examples dataset = client.create_dataset( dataset_name=dataset_name, description=\"NYC test dataset\" ) for q in eval_questions: client.create_example( inputs={\"query\": q}, dataset_id=dataset.id, ) print(\"Created a new dataset: \", dataset.name)\n\nIf you go to the LangSmith dashboard and check the datasets section you should be able to see the dataset we just created.\n\nLangSmith dataset tab with the NYC dataset we created\n\nTo run the evaluations you have to call the run_on_dataset() function from the LangSmith SDK. but before that, you have to create a chain factory that will return a new QA chain every time this is called. This is so that any states in the QA chain are not reused when evaluating with individual examples. Make sure the QA chains return the context if using metrics that check on context.\n\n# factory function that return a new qa chain def create_qa_chain(return_context=True): qa_chain = RetrievalQA.from_chain_type( llm, retriever=index.vectorstore.as_retriever(), return_source_documents=return_context, ) return qa_chain\n\nNow let's configure and run the evaluation. You use RunEvalConfig to configure the evaluation with the metrics/evaluator chains that you want to run against and the prediction_key for the returned result. Now call run_on_dataset and LangSmith to use the dataset we uploaded and run it against the chain from the factory and evaluate with the custom_evaluators we provided and upload the results.\n\nfrom langchain.smith import RunEvalConfig, run_on_dataset evaluation_config = RunEvalConfig( custom_evaluators=[eval_chains.values()], prediction_key=\"result\", ) result = run_on_dataset( client, dataset_name, create_qa_chain, evaluation=evaluation_config, input_mapper=lambda x: x, ) # output # View the evaluation results for project '2023-08-24-03-36-45-RetrievalQA' at: # https://smith.langchain.com/projects/p/9fb78371-150e-49cc-a927-b1247fdb9e8d?eval=true\n\nOpen", "start_char_idx": 9631, "end_char_idx": 14309, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77883c74-6a43-4770-99ad-15367ee48e74": {"__data__": {"id_": "77883c74-6a43-4770-99ad-15367ee48e74", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt", "file_name": "blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt", "file_type": "text/plain", "file_size": 15613, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "658b3e2b-bd12-4119-9520-c8386c71a9c9", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt", "file_name": "blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt", "file_type": "text/plain", "file_size": 15613, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "fdede87fbeebb677d1dddedec8993a4746bc1e2c61312b11ae46d688eadd0cc6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67c27eb4-00b7-40ab-9b44-65b72798cca1", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt", "file_name": "blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt", "file_type": "text/plain", "file_size": 15613, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "95034995580cc4ca03eaa2e31c5b5ec82af5db4f7a4b32198f566d1842be9ba7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b31ec050-c902-491c-9258-6bb677d5271a", "node_type": "1", "metadata": {}, "hash": "9e04172f0ac70f2d8242f1e12e12b7535043b471ebaa5e95cb5aa83c2a1564ca", "class_name": "RelatedNodeInfo"}}, "hash": "0ee5f92abd275721249953e04957712aab509b29bf24ae795a810c320327fbd7", "text": "up the results and you will see something like this\n\nThis shows the output for each example in the training dataset and the Feedback columns show the evaluation results. Now if you want to dive more into the reasons for the scores and how to improve them, click on any single example and open the Feedback tab.\n\nThis will show you each score and if you open the pop-up icon it will point you to the evaluation run of the RagasEvaluatorChain.\n\nYou can analyze each result to see why it was so and this will give you ideas on how to improve it.\n\nNow if your QA pipeline also uses LangSmith for logging, tracing, and monitoring you can leverage the add-to-dataset feature to set up a continuous evaluation pipeline that keeps adding interesting data points (based on human feedback of other indirect methods) into the test to keep the test dataset up to date with a more comprehensive dataset with wider coverage.\n\nConclusion\n\nRagas enhances QA system evaluation by addressing limitations in traditional metrics and leveraging Large Language Models. LangSmith provides a platform for running evaluations and visualizing results.\n\nBy using Ragas and LangSmith, you can ensure your QA systems are robust and ready for real-world applications, making the development process more efficient and reliable.", "start_char_idx": 14310, "end_char_idx": 15607, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b31ec050-c902-491c-9258-6bb677d5271a": {"__data__": {"id_": "b31ec050-c902-491c-9258-6bb677d5271a", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_exploring-genworlds_.txt", "file_name": "blog.langchain.dev_exploring-genworlds_.txt", "file_type": "text/plain", "file_size": 18004, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9e70d970-f163-4450-a05b-569fd6cda981", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_exploring-genworlds_.txt", "file_name": "blog.langchain.dev_exploring-genworlds_.txt", "file_type": "text/plain", "file_size": 18004, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "d606ecbe8a24b27a797bbac84e1236667b3de5c33b2df5636ae268c48048114e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77883c74-6a43-4770-99ad-15367ee48e74", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt", "file_name": "blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt", "file_type": "text/plain", "file_size": 15613, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "0ee5f92abd275721249953e04957712aab509b29bf24ae795a810c320327fbd7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb621787-34b2-4d44-9b43-40ae4d51ef6f", "node_type": "1", "metadata": {}, "hash": "8c98ae31bcb7e825671193ae091130cdfa335daa724a03686e34306c2ff2049e", "class_name": "RelatedNodeInfo"}}, "hash": "9e04172f0ac70f2d8242f1e12e12b7535043b471ebaa5e95cb5aa83c2a1564ca", "text": "URL: https://blog.langchain.dev/exploring-genworlds/\nTitle: Yeager.ai x LangChain: Exploring GenWorlds a Framework for Coordinating AI Agents\n\nEditor's Note: This is another edition of our series of guest posts highlighting the powerful applications of LangChain. We have been working with the Yeager.ai team for several months now, and they have built some really impressive applications of LangChain agents. We're excited to highlight this guest blog on their GenWorlds framework for multi-agent systems. We are especially excited to see their plan for making it seamless for the LangChain/GenWorlds community to monetize their projects.\n\n\ud83e\uddec\ud83c\udf0dGenWorlds is an open-source development framework for multi-agent systems. Core to the framework are dedicated environments or \"Worlds\" with specialized AI Agents and shared Objects. In these Worlds, Agents use shared Objects to coordinate to execute complex tasks and achieve common goals.\n\nOur research has shown that Agents perform better with narrow focus, so Agent coordination is essential for GenAI systems to perform complex operations.\n\nDevelopers Can Easily Monetize\n\nYeager.ai is building a robust ecosystem for GenAI applications. The framework is designed with composability and interoperability in mind. In addition to the framework, GenWorlds will offer a platform for hosting GenAI applications with minimal maintenance and a marketplace where developers can easily monetize their builds.\n\n\n\nThe Power of Modularity\n\nModularity is a cornerstone of the GenWorlds framework. In software engineering, breaking things down into smaller self-contained components can improve reusability and reliability of systems. The same is true for systems using LLMs - the more specific and narrow each call to the language model, the higher the reliability of the output. The GenWorlds framework applies this principle at multiple levels:\n\nInstead of a single agent trying to do everything, GenWorlds allows you to create multiple agents, each with a much narrower mission, that can work together towards a common goal Furthermore, each Agent's thought process is broken down into multiple Brains appropriate for the task at hand. Each brain needs only a fraction of the total context of the Agent and can therefore perform better Developers can configure each Brain to use advanced techniques such as 'chain-of-thought', 'self-evaluation', and 'tree-of-thought'. The framework is built with the flexibility to accommodate any new technique that comes out.\n\nThe end result is focused and highly reliable Agents that fully harness the power of the underlying language models to achieve complex goals.\n\nWith this context set, now let\u2019s dive into the \ud83e\uddec\ud83c\udf0d GenWorlds framework.\n\n\n\nWorld\n\nThe 'Worlds' is the setting for all the action. It keeps track of all the Agents, Objects, and World properties such as Agent inventories. The World ensures every Agent is informed about the World state, entities nearby, and the events that are available to them to interact with the World.\n\n\n\nWorld Definition YAML\n\nFor convenience, the RoundTable example (a multi-agent podcast simulation, more on this below) comes with a loader which reads a world configuration from a YAML file. This allows everyone to quickly create and modify various worlds. Here is an example:\n\n\n\nworld_definition: base_args: websocket_url: ws://localhost:7456/ws world: id: world class: genworlds.worlds.world_2d.world_2d.World2D name: All-In Podcast description: The four \"besties\" discuss any topic you want. locations: - roundtable objects: - id: mic1 ... agents: - id: jason_calacanis ...\n\nSimulation Socket\n\nThe \u201cSimulation Socket\u201d is a websocket server which serves as the communication backbone. It enables parallel operation of the World, Agents, and Objects, and allows them to communicate by sending events. This architecture supports connecting frontends or other services to the World, running Agents on external servers, and much more.\n\nAgents\n\nAgents in GenWorlds are autonomous entities built on top of LangChain that perceive their surroundings and make decisions to achieve specific goals as set by the user. The Agents interact with their environment by sending events through the Simulation Socket server. They dynamically learn about the World and the Objects around them, figuring out how to utilize these Objects to achieve their goals.\n\nThere can be many different types of Agents, as long as each one of them understands the event protocol used to communicate with the World.\n\nThe Agent\u2019s Mental Model\n\nAgents follow a specific mental model at each step of their interaction with the World:\n\nReview World state: The Agent assesses the environment to understand the context before planning any actions. Review new events: The Agent evaluates any new occurrences in the World. These could be actions taken by other Agents or changes in the World state due to Object interactions. Remember: Using its stored memories and awareness of previous state changes, the Agent recalls past experiences and data that might affect its current decision-making process. Create a", "start_char_idx": 0, "end_char_idx": 5088, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb621787-34b2-4d44-9b43-40ae4d51ef6f": {"__data__": {"id_": "fb621787-34b2-4d44-9b43-40ae4d51ef6f", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_exploring-genworlds_.txt", "file_name": "blog.langchain.dev_exploring-genworlds_.txt", "file_type": "text/plain", "file_size": 18004, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9e70d970-f163-4450-a05b-569fd6cda981", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_exploring-genworlds_.txt", "file_name": "blog.langchain.dev_exploring-genworlds_.txt", "file_type": "text/plain", "file_size": 18004, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "d606ecbe8a24b27a797bbac84e1236667b3de5c33b2df5636ae268c48048114e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b31ec050-c902-491c-9258-6bb677d5271a", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_exploring-genworlds_.txt", "file_name": "blog.langchain.dev_exploring-genworlds_.txt", "file_type": "text/plain", "file_size": 18004, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "9e04172f0ac70f2d8242f1e12e12b7535043b471ebaa5e95cb5aa83c2a1564ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a37b826-09f0-42ce-885b-bd38d77ff3e3", "node_type": "1", "metadata": {}, "hash": "269c42e82b4a55483c695934616e0fd9c81abaf49801366d69a7ccd1d26e69a8", "class_name": "RelatedNodeInfo"}}, "hash": "8c98ae31bcb7e825671193ae091130cdfa335daa724a03686e34306c2ff2049e", "text": "changes, the Agent recalls past experiences and data that might affect its current decision-making process. Create a plan: The Agent creates and updates action plans based on World state changes. An Agent can execute multiple actions in one step, improving overall efficiency. Execution: Agent implements its plan, influencing the World state and potentially triggering responses from other Agents.\n\nThis interactive process fosters the emergence of complex, autonomous behavior, making each Agent an active participant in the World.\n\nThe Thinking Process\n\nThe think() method in the code is the central function where an Agent\u2019s thinking process is carried out. The function first acquires the initial state of the World and potential actions to be performed. It then enters a loop, where it processes events and evaluates entities in the Agent's proximity to inform its decision-making.\n\nDepending on the current state and goals of the Agent, the think() function may choose to wait, respond to user input, or interact with entities. If the Agent selects an action, it executes it and updates its memory accordingly. The think() function continually updates the Agent's state in the World and repeats the process until it decides to exit. See diagram below showing the Agent Loop in RoundTable, our podcast simulation application:\n\nComponents of an Agent\n\nBrains: A Brain is a system that controls one step of an Agent's thinking process. It manages the process of thought generation, evaluation, and selection. The Brain defines the functions necessary for these processes and uses a Large Language Model to generate and evaluate thoughts. All Brains take full advantage of OpenAI Functions to allow easy specification of the desired output format.\n\nMost Agents have the following Brain types:\n\nNavigation Brain: The Navigation Brain is designed for selecting the next action of the agent which helps it achieve its goals. It generates a plan for the Agent's next steps in the World. The inputs to this class include the Agent's information (name, role, background, personality), goals, constraints, evaluation principles, and the number of thoughts to generate. It generates a set of possible plans, each consisting of an action to take, whether the action is valid, any violated constraints, and an updated plan. The NavigationBrain then evaluates these plans and selects the one that best meets the evaluation principles and constraints.\n\nSometimes you want to constrain the sequence of actions and force the Agent to follow a certain action with another one - this can be done using the Action-Brain map.\n\nHere you can see the constructor of a NavigationBrain:\n\nNavigationBrain( openai_api_key=openai_api_key, name=name, role=role, background=background, personality=personality, topic_of_conversation=topic_of_conversation, constraints=constraints, evaluation_principles=evaluation_principles, n_of_thoughts=3, ),\n\n\n\nThe n_of_thoughts=3 parameter specifies that the Brain will generate 3 possible next actions and pick the best one according to its evaluation_principles.\n\nExecution Brains: Execution Brains enable the execution of diverse tasks by Agents. These Brains accept Agent details, task attributes, constraints, and evaluation parameters. These brains can be configured to generate their output in a single call, or generate multiple potential outputs and select the best one using self-evaluation techniques.\n\nThe power of Execution Brains lies in their customizability. Developers can create Brains adapted for various tasks such as participating in a podcast, writing an essay, analyzing data, or scraping social media feeds. This flexibility allows the creation of uniquely skilled Agents capable of performing a wide array of tasks in their simulated environments.\n\nThe following is a constructor of an example execution brain - the PodcastBrain:\n\nPodcastBrain( openai_api_key=openai_api_key, name=name, role=role, background=background, personality=personality, communication_style=communication_style, topic_of_conversation=topic_of_conversation, constraints=constraints, evaluation_principles=evaluation_principles, n_of_thoughts=1, ),\n\nHere, the n_of_thoughts is set to 1, meaning the Podcast Brain will generate only one output message and skip the evaluation step.\n\nEvent Filler Brain: The Event Filler Brain is used for generating the JSON parameters required for an action the Agent executes in a World. The inputs are similar to the Navigation Brain but also include the command the Agent has decided to execute, as well as the outputs of any previous brains in the execution flow.\n\nYou can see it takes fewer parameters than the other Brains, because it doesn't need as much context to complete its task.\n\n\n\nEventFillerBrain( openai_api_key=openai_api_key, name=name, role=role, background=background, topic_of_conversation=topic_of_conversation, constraints=constraints, evaluation_principles=evaluation_principles, n_of_thoughts=1, ),\n\nDifferent types of Brains can be created to handle different tasks, scenarios, or problems. An Agent can have multiple Brains each focused on", "start_char_idx": 4972, "end_char_idx": 10083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a37b826-09f0-42ce-885b-bd38d77ff3e3": {"__data__": {"id_": "2a37b826-09f0-42ce-885b-bd38d77ff3e3", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_exploring-genworlds_.txt", "file_name": "blog.langchain.dev_exploring-genworlds_.txt", "file_type": "text/plain", "file_size": 18004, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9e70d970-f163-4450-a05b-569fd6cda981", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_exploring-genworlds_.txt", "file_name": "blog.langchain.dev_exploring-genworlds_.txt", "file_type": "text/plain", "file_size": 18004, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "d606ecbe8a24b27a797bbac84e1236667b3de5c33b2df5636ae268c48048114e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb621787-34b2-4d44-9b43-40ae4d51ef6f", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_exploring-genworlds_.txt", "file_name": "blog.langchain.dev_exploring-genworlds_.txt", "file_type": "text/plain", "file_size": 18004, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "8c98ae31bcb7e825671193ae091130cdfa335daa724a03686e34306c2ff2049e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5daa82f-1059-48b0-b69e-5474ea40a678", "node_type": "1", "metadata": {}, "hash": "5ac7709e0750ca519a0b79eb5060ae864bdab8ecae7b4865117840d0b4ec4680", "class_name": "RelatedNodeInfo"}}, "hash": "269c42e82b4a55483c695934616e0fd9c81abaf49801366d69a7ccd1d26e69a8", "text": "to handle different tasks, scenarios, or problems. An Agent can have multiple Brains each focused on a specific goal with its context tailored to that goal. For instance, a podcaster Agent would have a \u201cContent Brain\u201d which would be its only Brain that possesses information about the Agent\u2019s communication-style.\n\nUsing multiple Brains helps narrow the Agent\u2019s focus and thus significantly enhances the quality and reliability of an Agent's output. Additionally, each Brain can use a different LLM, which allows you to further optimize the system. For instance certain actions or decisions require more powerful LLMs (e.g. GPT-4) while other steps can be done with more simple, faster, and cheaper LLMs (e.g. GPT-3.5).\n\nAction-Brain Map: The Action-Brain Map of an Agent defines a deterministic path through the various Brains. It's the system that decides which Brain to use based on the Agent's next action. The output of each brain is passed on to the consecutive brain in the execution brain path of that action. From our Podcaster Agent example above, it will engage the Content Brain when the Agent is about to speak, and pass the output to the Event Filler Brain to generate a valid World event with the generated response.\n\nYou can furthermore specify a deterministic follow-up for each action if you don't want the agent to choose freely - for example, after speaking into the microphone, the Agent must pass it to someone else. This allows you to constrain the Agent to create more predictable execution paths and increase reliability.\n\nHere's what that looks like in code:\n\naction_brain_map = { \"Microphone:agent_speaks_into_microphone\": {\"brains\":[ \"podcast_brain\", \"event_filler_brain\", ], \"next_actions\": [\"World:agent_gives_object_to_agent_event\"]}, \"World:agent_gives_object_to_agent_event\": {\"brains\":[\"event_filler_brain\"], \"next_actions\": []}, \"default\": {\"brains\":[\"event_filler_brain\"], \"next_actions\": []}, }\n\nAgent Inventory: The specific Objects that an Agent currently holds. This state is maintained by the world, and allows Agents to pass items such as Tokens from one to another, and keep Objects with them as they move to different locations in the World.\n\n\n\nMemories: Current and Pregenerated\n\nAgents in GenWorlds have two types of memories, current and pregenerated memories:\n\nCurrent Memories: Agents remember [number] of the most recent events, [number] of the most relevant events, and a running summary of the whole history. The number of recent and relevant memories is configurable by the user. Pregenerated Memories: Generated from external content (e.g. youtube videos, books, etc.) and stored in vector dbs. Pre-generated memories are injected in Agents\u2019 prompts based on their relevance to the Agent's current goals, allowing for more focused and reliable interactions. These memories allow Agents to learn without fine-tuning the underlying model.\n\nBelow is how you can easily configure custom pregenerated memories in a World definition YAML file, which are stored in a Qdrant vector database:\n\nTo use the memories, you need to set the following values in the world_definition.yaml file\n\nworld_definition: world: path_to_to_external_memory: ./databases/summaries_qdrant\n\nFor each Agent you need to specify the collection name\n\nagents: - id: maria personality_db_collection_name: maria\n\n\n\n\n\nObjects\n\nObjects play a crucial role in facilitating interaction between Agents. Every Object defines a unique set of events, enabling Agents to accomplish specific tasks and work together in a dynamic environment. Objects can be in an Agent's vicinity or can be part of their inventory, expanding the scope of possible interactions.\n\nAgents are designed to adapt dynamically, learning about nearby Objects, understanding the event definitions, and determining the best way to interact with them to achieve their goals. Objects are the main way to give Agents new capabilities and organize them in a structure to achieve a collective broader goal.\n\n\n\nAgent Coordination\n\nSince Agents are aware of the events in the World including those of the other Agents, Agents naturally react to each other. In order to facilitate structure and organized behavior between the Agents we use shared Objects. See three examples below:\n\n\u201cToken Bearer\u201d: Agents use a Token in their inventory to communicate and to signal to the other Agents whose turn it is to perform an action. For example, in RoundTable (our podcast simulation app), Agents use a microphone as a token to speak to each other. Agents can only speak if they have the mic in their inventory. This ensures the Agents listen to each other and prevents them from interrupting each other, thus creating a dynamic discussion.\n\n\n\nThis is how it looks in the code\n\nworld_definition: world: path_to_to_external_memory: ./databases/summaries_qdrant\n\nPipeline: Each Agent is assigned a role in the pipeline and completes their role", "start_char_idx": 9983, "end_char_idx": 14888, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5daa82f-1059-48b0-b69e-5474ea40a678": {"__data__": {"id_": "e5daa82f-1059-48b0-b69e-5474ea40a678", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_exploring-genworlds_.txt", "file_name": "blog.langchain.dev_exploring-genworlds_.txt", "file_type": "text/plain", "file_size": 18004, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9e70d970-f163-4450-a05b-569fd6cda981", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_exploring-genworlds_.txt", "file_name": "blog.langchain.dev_exploring-genworlds_.txt", "file_type": "text/plain", "file_size": 18004, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "d606ecbe8a24b27a797bbac84e1236667b3de5c33b2df5636ae268c48048114e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a37b826-09f0-42ce-885b-bd38d77ff3e3", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_exploring-genworlds_.txt", "file_name": "blog.langchain.dev_exploring-genworlds_.txt", "file_type": "text/plain", "file_size": 18004, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "269c42e82b4a55483c695934616e0fd9c81abaf49801366d69a7ccd1d26e69a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a977913b-9ed7-4a01-b05a-04cd810a45ab", "node_type": "1", "metadata": {}, "hash": "09e813fef30aee33ff722ce94689ec5e5759ed45865cc293f7c2ed8fdfda32cc", "class_name": "RelatedNodeInfo"}}, "hash": "5ac7709e0750ca519a0b79eb5060ae864bdab8ecae7b4865117840d0b4ec4680", "text": "Each Agent is assigned a role in the pipeline and completes their role when it\u2019s their turn. It\u2019s like a factory line. For example, most sales processes go something like this:\n\nResearch > obtain contact info > create/send a compelling outreach message > follow up > schedule/conduct a call > review call notes > send/negotiate legal docs > follow-ups/calls > and on and on until the deal is won/lost.\n\nTo facilitate this sort of an operation, you would create a set of Objects such as boxes, where each agent can put their work output, and the next Agent in the pipeline can look to see when a new item has been passed to them, pick it up, process it according to the step of the pipeline they're in, and put in in the next box.\n\nPipeline is best for sequential tasks like in the sales example.\n\nProject Management: A project manager (human or Agent) uses a Blackboard, an Object for assigning roles to each Agent and keeping track of progress. The Project Manager and the Agents interact via the Blackboard, sharing files, updating assignments, tracking progress, etc.\n\nTools: Agents can use Tools to execute specific functions such as calling external APIs, running complex calculations, or triggering unique events in the World.\n\nThe framework provides the flexibility for anyone to create their own Objects and coordination methods.\n\n\n\nUse Case Highlight - RoundTable\n\nIn order to showcase the coordination capabilities of Agents in GenWorlds, we built RoundTable, a podcast simulation. Users can summon the brightest minds for a group discussion on any topic. It's not another ChatGPT wrapper; it\u2019s a team of AI Agents acting independently with specific personalities, memories, and expertise.\n\nRoundTable uses Objects, Agent inventories, and pre-generated memories to create dynamic discussions between Agents who sound like the people they are emulating. See a demo here:\n\nYou can also try it for free on Replit\n\nGo to Replit Fork the Replit (it\u2019s completely free) Select which use case you want to use\n\nThe GenAI Ecosystem - Our Northstar\n\nWe designed the GenWorlds framework with modularity, flexibility, and composability. We envision GenAI developers using this modularity to plug and play or build each element of the framework (Worlds, Agents, Objects, Memories, Brains, etc.) to create their own useful applications.\n\nWe are not stopping there. We are giving our developer community a platform and the tools for easy access to monetizing their applications in a Gen-AI marketplace.\n\n\n\n\n\nLearn More About \ud83e\uddec\ud83c\udf0dGenWorlds\n\nDemo: https://youtu.be/INsNTN4S680\n\nGitHub: https://github.com/yeagerai/genworlds\n\nDocs: https://genworlds.com/docs/intro\n\nDiscord: https://discord.gg/wKds24jdAX\n\nBlog: https://medium.com/yeagerai\n\nReplit: https://replit.com/@yeagerai/GenWorlds\n\n\n\n\n\nAbout Yeager\n\nAt Yeager.ai, we are on a mission to enhance the quality of life through the power of Generative AI. Our goal is to eliminate the burdensome aspects of work by making GenAI reliable and easily accessible. By doing so, we foster a conducive environment for learning, innovation, and decision-making, propelling technological advancement.", "start_char_idx": 14818, "end_char_idx": 17950, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a977913b-9ed7-4a01-b05a-04cd810a45ab": {"__data__": {"id_": "a977913b-9ed7-4a01-b05a-04cd810a45ab", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_fine-tune-your-llms-with-langsmith-and-lilac_.txt", "file_name": "blog.langchain.dev_fine-tune-your-llms-with-langsmith-and-lilac_.txt", "file_type": "text/plain", "file_size": 7600, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0352b9e5-6c5f-4e76-8f61-8bed337644a8", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_fine-tune-your-llms-with-langsmith-and-lilac_.txt", "file_name": "blog.langchain.dev_fine-tune-your-llms-with-langsmith-and-lilac_.txt", "file_type": "text/plain", "file_size": 7600, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "53dbea82b98557eb08dc620b5ae4e3384a52d5bcde7643f69485509e99ce6826", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5daa82f-1059-48b0-b69e-5474ea40a678", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_exploring-genworlds_.txt", "file_name": "blog.langchain.dev_exploring-genworlds_.txt", "file_type": "text/plain", "file_size": 18004, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "5ac7709e0750ca519a0b79eb5060ae864bdab8ecae7b4865117840d0b4ec4680", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f4e4fd6-4fc1-43b9-9c97-2ab0690244c0", "node_type": "1", "metadata": {}, "hash": "ed365ac78c69dbfc2f10a3b448789fe1602339d78bcb3e211377fd3f2760bcd7", "class_name": "RelatedNodeInfo"}}, "hash": "09e813fef30aee33ff722ce94689ec5e5759ed45865cc293f7c2ed8fdfda32cc", "text": "URL: https://blog.langchain.dev/fine-tune-your-llms-with-langsmith-and-lilac/\nTitle: Fine-tune your LLMs with LangSmith and Lilac\n\nIn taking your LLM from prototype into production, many have turned to fine-tuning models to get more consistent and high-quality behavior in their applications. Services like OpenAI and HuggingFace make it easy to fine-tune a model on your application-specific data. All it takes is a JSON file!\n\nThe tricky part is deciding what to include in that data. Once your LLM is deployed, it could be prompted given any input - how do you make sure it will respond appropriately for the user or machine it is meant to interact with?\n\nFor this, there is no real substitute for high-quality data taken from your unique application context. This is where LangSmith and Lilac can help out.\n\nLangSmith + Lilac\n\nTo understand and improve any language model application, it\u2019s important to be able to quickly explore and organize the data the model is seeing. To achieve this, LangSmith and Lilac provide complementary capabilities:\n\nLangSmith: Efficiently collects, connects, and manages datasets generated by your LLM applications at scale. Use this to capture quality examples (and failure cases) and user feedback you can use for fine-tuning.\n\nEfficiently collects, connects, and manages datasets generated by your LLM applications at scale. Use this to capture quality examples (and failure cases) and user feedback you can use for fine-tuning. Lilac: Offers advanced analytics to structure, filter, and refine datasets, making it easy to continuously improve your data pipeline.\n\nWe wanted to share how to connect these two powerful tools to kickstart your fine-tuning workflows.\n\nFine-tuning a Q&A Chatbot\n\nIn the following sections, we will use LangSmith and Lilac to curate a dataset to fine-tune an LLM powering a chatbot that uses retrieval-augmented generation (RAG) to answer questions about your documentation. For our example, we will use a dataset sampled from a Q&A app for LangChain\u2019s docs. The overall process is outlined in the image below:\n\nDataset Curation Pipeline with LangSmith + Lilac\n\nThe main steps are:\n\nCapture traces from the prototype and convert to a candidate dataset Import into Lilac to label, filter, and enrich. Fine-tune a model on the enriched dataset. Use the fine-tuned model in an improved application.\n\nCapture traces\n\nLangChain make it easy to design a prototype using prompt chaining. At first, the application may not be fully optimized or may run into errors when the prompt engineering is incomplete, but we can quickly create an alpha version of a feature to kickstart the dataset curation process. When building with LangChain, we can easily trace all the execution steps to LangSmith by setting a couple of environment variables.\n\nThen in LangSmith, we can select runs to add to a candidate dataset in the UI or programmatically (see the notebook).\n\nImport to Lilac\n\n\ud83d\udca1 The sections below give a high level overview of the Lilac UI. For a deeper dive reproducing this workflow, see the python cookbook.\n\nLilac provides a native integration with LangSmith datasets. After installing Lilac locally, set the LANGCHAIN_API_KEY in the environment and you should see a list of LangSmith datasets auto-populated in the Lilac UI. Select the one you\u2019ve earmarked for fine-tuning, and Lilac will handle the rest.\n\nThe \u201cAdd dataset\u201d page in the Lilac UI with the LangSmith data loader.\n\nCurate your dataset\n\nNow that we have our dataset in Lilac, we can run Lilac\u2019s signals, concepts and labels to help organize and filter the dataset. Our goal is to select distinct examples demonstrating good language model generations for a variety of input types. Let\u2019s see how Lilac can help us structure our dataset.\n\nSignals\n\nRight off the bat, Lilac provides two useful signals you can apply to your dataset: Near-duplicates and PII detection. Filtering near-duplicates for inputs is important to make sure the model gets diverse information and reduce changes of memorization. To compute a signal from the UI, expand the schema in the top left corner, and select \u201cCompute Signal\u201d from the context menu of the field you want to enrich.\n\nComputing a signal via the context menu of the answer field in the Lilac schema viewer.\n\nConcepts\n\nIn addition to signals, Lilac offers concepts, a powerful way to organize the data along axes that you care about. A concept is simply a collection of positive (text that is related to the concept) and negative examples (either the opposite, or unrelated to the concept). Lilac comes with several built-in concepts, like toxicity, profanity, sentiment, etc, or you can create your own. Before we apply a concept to the", "start_char_idx": 0, "end_char_idx": 4703, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f4e4fd6-4fc1-43b9-9c97-2ab0690244c0": {"__data__": {"id_": "5f4e4fd6-4fc1-43b9-9c97-2ab0690244c0", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_fine-tune-your-llms-with-langsmith-and-lilac_.txt", "file_name": "blog.langchain.dev_fine-tune-your-llms-with-langsmith-and-lilac_.txt", "file_type": "text/plain", "file_size": 7600, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0352b9e5-6c5f-4e76-8f61-8bed337644a8", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_fine-tune-your-llms-with-langsmith-and-lilac_.txt", "file_name": "blog.langchain.dev_fine-tune-your-llms-with-langsmith-and-lilac_.txt", "file_type": "text/plain", "file_size": 7600, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "53dbea82b98557eb08dc620b5ae4e3384a52d5bcde7643f69485509e99ce6826", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a977913b-9ed7-4a01-b05a-04cd810a45ab", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_fine-tune-your-llms-with-langsmith-and-lilac_.txt", "file_name": "blog.langchain.dev_fine-tune-your-llms-with-langsmith-and-lilac_.txt", "file_type": "text/plain", "file_size": 7600, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "09e813fef30aee33ff722ce94689ec5e5759ed45865cc293f7c2ed8fdfda32cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12a34e79-6b58-4d1a-88ba-b9366185df8d", "node_type": "1", "metadata": {}, "hash": "898d3bcf5b9239b2045bf008782bdceb1280cc1ff246530caac4301d46bf9bb9", "class_name": "RelatedNodeInfo"}}, "hash": "ed365ac78c69dbfc2f10a3b448789fe1602339d78bcb3e211377fd3f2760bcd7", "text": "sentiment, etc, or you can create your own. Before we apply a concept to the dataset, we need to compute text embeddings on the field that we care about.\n\nComputing embeddings for the question field to enable concept and semantic search via Lilac\u2019s search box.\n\nOnce we\u2019ve computed embeddings, we can preview a concept by selecting it from the search box menu.\n\nSelecting profanity on the answer field for previewing.\n\nTo compute a concept for the entire dataset, choose \u201cCompute concept\u201d from the context menu in the schema viewer.\n\nComputing a concept via the context menu of the answer field in the schema viewer.\n\nIn addition to concepts, embeddings enable two other useful functionalities for exploring the data: semantic search and finding similar examples.\n\nSemantic search for \u201cforget all previous instructions\u201d via Lilac\u2019s search box.\n\nFinding questions similar to \u201cwhat is 1213 divided\u2026.\u201d via the Lilac UI.\n\nLabels\n\nIn addition to automated labeling with signals and concepts, Lilac allows you to tag individual rows with custom labels that can be later used to prune your dataset.\n\nAdding a calculation label to an example in Lilac.\n\nWhen you add a new label, just like signals and concepts, it creates a new top-level column in your dataset. These can then be used to power additional analytics.\n\nExport the dataset\n\nOnce we\u2019ve computed the information needed for filtering, you can export the enriched dataset via python, as shown in the notebook or via Lilac\u2019s UI, which will create a browser download of a json file. We recommend the python API for downloading large amounts of data, or if you need a better control over the selection of data.\n\nLilac\u2019s Download data modal dialog.\n\nOnce we exported the enriched dataset, we can easily filter out the examples in python using the enriched fields.\n\nFine-tune\n\nWith the dataset in hand, it\u2019s time to fine-tune! It\u2019s easy to convert from LangChain\u2019s message format to the formats expected by OpenAI, HuggingFace or other training frameworks. You can check out the linked notebook for more info!\n\nUse in your Chain\n\nOnce we have the fine-tuned LLM, we can switch to it with a update to the \u201cmodel\u201d argument in our LLM.\n\nfrom langchain.chat_models import ChatOpenAI llm = ChatOpenAI(model=\"ft:gpt-3.5-turbo-0613:{openaiOrg}::{modelId}\")\n\nAssuming we\u2019ve structured the data appropriately, this model will have more awareness for the structure and style you wish to use in generating responses.\n\nConclusion\n\nThis is a simple overview of the process for going from traces to fine-tuned model by integrating Lilac and LangSmith. With the data process in place, you can continuously improve each components in your contextual reasoning application LangSmith makes it easy to collect user and model-assisted feedback to save time when capturing data, and Lilac helps you analyze, label, and organize all the text data so you can refine your model appropriately.", "start_char_idx": 4627, "end_char_idx": 7541, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12a34e79-6b58-4d1a-88ba-b9366185df8d": {"__data__": {"id_": "12a34e79-6b58-4d1a-88ba-b9366185df8d", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_goodbye-cves-hello-langchain_experimental_.txt", "file_name": "blog.langchain.dev_goodbye-cves-hello-langchain_experimental_.txt", "file_type": "text/plain", "file_size": 3141, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f4024f7c-c873-4d85-9473-f4a3020695b4", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_goodbye-cves-hello-langchain_experimental_.txt", "file_name": "blog.langchain.dev_goodbye-cves-hello-langchain_experimental_.txt", "file_type": "text/plain", "file_size": 3141, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "6cba803e3b32b7a892e966ee891181e18e58d95e66ed78d24179ce7b7aae810c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f4e4fd6-4fc1-43b9-9c97-2ab0690244c0", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_fine-tune-your-llms-with-langsmith-and-lilac_.txt", "file_name": "blog.langchain.dev_fine-tune-your-llms-with-langsmith-and-lilac_.txt", "file_type": "text/plain", "file_size": 7600, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "ed365ac78c69dbfc2f10a3b448789fe1602339d78bcb3e211377fd3f2760bcd7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6f4ae6f-a0f7-4fed-b655-6fc05df310f0", "node_type": "1", "metadata": {}, "hash": "5db5229ea2c48b97a7262778bd4deb25bbb54400b255113d6520ffd64f04d30a", "class_name": "RelatedNodeInfo"}}, "hash": "898d3bcf5b9239b2045bf008782bdceb1280cc1ff246530caac4301d46bf9bb9", "text": "URL: https://blog.langchain.dev/goodbye-cves-hello-langchain_experimental/\nTitle: Goodbye CVEs, Hello `langchain_experimental`\n\nOne of the things that LangChain seeks to enable is connecting language models to external sources of data and computation. This allows language models to act as the reasoning engine and outsource knowledge and execution to other systems. Some examples of this include:\n\nRetrieval augmented generation: Hooking a language model up to a retrieval system and using that influence the generation. This allows language models to answer questions about information other than what they were trained on.\n\nInteracting with APIs: Having a language model generate the route and parameters to call for a specific API request. This allows humans to interact with an API through natural language.\n\nCode generation: Having a language model write and then execute code. This can enable people to generate code just by asking.\n\nWhile this is powerful, it can also be dangerous. A lot has been discussed around AI safety, and there are many different types and considerations for AI safety. The type that we are most concerned with is what happens when you hook up language models to other systems. While doing this can enable lots of amazing experience (like the ones listed above) it also opens up a whole new risk vector.\n\nThis risk vector emerges when the language model generates output that is unsafe to pass downstream, whether that be an API request that deletes some data or some malicious code that deletes all files. This can occur naturally or maliciously. It can occur naturally when the language model simply messes up - as its prone to do. It can also occur maliciously, through techniques like prompt injection.\n\nLangChain started off as highly experimental and included a lot of these use cases, as those uses were the ones pushing boundary of what was possible. Some of these use cases have security concerns, some don\u2019t. As LangChain matures, we want to better separate those uses to allow for that distinction.\n\nWe\u2019ve taken a first stab at that by releasing langchain_experimental , a separate Python package. We\u2019ve moved all components that raised CVEs into that package. We\u2019ve also moved everything previously in the langchain.experimental module there as well. You can find instructions on how to migrate here.\n\nGoing forward, we have the dual goals of making core langchain more robust and production ready, while also pushing forward rapidly with langchain_experimental . We\u2019ve been slow to accept some more experimental features, but this separation will now hopefully speed that up.\n\nWe will also likely move more things over to langchain_experimental over time. When we do this, we will always give at least a week\u2019s notice before making any breaking changes, and update the migration guide.\n\nWe\u2019d like to thank the entire community for understanding, as well as their patience as we iron out any kinks. In particular we\u2019d like to thank some community members for their help and encouragement on this: Rich Harang (and the entire Nvidia team), Justin Flick, Or Raz, and Boaz Wasserman.", "start_char_idx": 0, "end_char_idx": 3125, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6f4ae6f-a0f7-4fed-b655-6fc05df310f0": {"__data__": {"id_": "e6f4ae6f-a0f7-4fed-b655-6fc05df310f0", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_gpt-researcher-x-langchain_.txt", "file_name": "blog.langchain.dev_gpt-researcher-x-langchain_.txt", "file_type": "text/plain", "file_size": 3556, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "529319ff-9d54-42a7-8896-5b102213bf49", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_gpt-researcher-x-langchain_.txt", "file_name": "blog.langchain.dev_gpt-researcher-x-langchain_.txt", "file_type": "text/plain", "file_size": 3556, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "6922b816ca54549450e1f7a66257b5965c22b95edb0e184bf0c75747bc357fe9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12a34e79-6b58-4d1a-88ba-b9366185df8d", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_goodbye-cves-hello-langchain_experimental_.txt", "file_name": "blog.langchain.dev_goodbye-cves-hello-langchain_experimental_.txt", "file_type": "text/plain", "file_size": 3141, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "898d3bcf5b9239b2045bf008782bdceb1280cc1ff246530caac4301d46bf9bb9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e964de9c-1efd-430b-a3f1-6217c7ce2962", "node_type": "1", "metadata": {}, "hash": "d7a438dcd45f661a9057c412bb26b90789501bdb3fb33ad19c51221eecea6eea", "class_name": "RelatedNodeInfo"}}, "hash": "5db5229ea2c48b97a7262778bd4deb25bbb54400b255113d6520ffd64f04d30a", "text": "URL: https://blog.langchain.dev/gpt-researcher-x-langchain/\nTitle: GPT Researcher x LangChain\n\nHere at LangChain we think that web research is fantastic use case for LLMs. So much so that we wrote a blog on it about a month ago. In that blog we mentioned the leading open-source implementation of a research assistant - gpt-researcher. Today we're excited to announce that GPT Researcher is integrated with LangChain. Specifically, it is integrated with our OpenAI adapter, which allows (1) easy usage of other LLM models under the hood, (2) easy logging with LangSmith.\n\nWhat is GPT Researcher? From the GitHub repo:\n\nThe main idea is to run \"planner\" and \"execution\" agents, whereas the planner generates questions to research, and the execution agents seek the most related information based on each generated research question. Finally, the planner filters and aggregates all related information and creates a research report. The agents leverage both gpt3.5-turbo-16k and gpt-4 to complete a research task.\n\nMore specifcally:\n\n- Generate a set of research questions that together form an objective opinion on any given task.\n\n- For each research question, trigger a crawler agent that scrapes online resources for information relevant to the given task.\n\n- For each scraped resources, summarize based on relevant information and keep track of its sources.\n\n- Finally, filter and aggregate all summarized sources and generate a final research report.\n\nAn image of the architecture can be seen below.\n\nUnder the hood this uses OpenAI's ChatCompletion endpoint. As number of viable models has started to increase (Anthropic, Llama2, Vertex models) we've been chatting with the GPT Researcher team about integrating LangChain. This would allow them to take advantage of the ~10 different Chat Model integrations that we have. It would also allow users to take advantage of LangSmith - our recently announced debugging/logging/monitoring platform.\n\nIn order to make this transition as seamless as possible we added an OpenAI adapter that can serve as a drop-in replacement for OpenAI. For a full walkthrough of this adapter, see the documentation here. This adapter can be use by the following code swap:\n\n- import openai + from langchain.adapters import openai\n\nSee here for the full PR enabling it on the GPT Researcher repo.\n\nThe first benefit this provides is enabling easy usage of other models. By passing in provider=\"ChatAnthropic\", model=\"claude-2\", to create, you easily use Anthropic's Claude model.\n\nThe second benefit this provides is seamless integration with LangSmith. Under the hood, GPT Researcher makes many separate LLM calls. This complexity is a big part of why it's able to perform so well. As the same time, this complexity can also make it more difficult to debug and understand what is going on. By enabling LangSmith, you can easily track that.\n\nFor example, here is the LangSmith trace for the call to the language model when it's generating an agent description to use:\n\nAnd here is the LangSmith trace for the final call to the language model - when it asks it to write the final report:\n\nWe're incredibly excited to be supporting GPT Researcher. We think this is one of the biggest opportunities for LLMs. We also think GPT Researcher strikes an appropriate balance, where the architecture is certainly very complex but it's more focused than a completely autonomous agent. We think applications that manage to strike that balance are the future, and we're very excited to be able to partner with and support them in any way.", "start_char_idx": 0, "end_char_idx": 3556, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e964de9c-1efd-430b-a3f1-6217c7ce2962": {"__data__": {"id_": "e964de9c-1efd-430b-a3f1-6217c7ce2962", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_how-correct-are-llm-evaluators_.txt", "file_name": "blog.langchain.dev_how-correct-are-llm-evaluators_.txt", "file_type": "text/plain", "file_size": 8844, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da7cd2f-71b1-4b5c-be5c-7494f7c964d6", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_how-correct-are-llm-evaluators_.txt", "file_name": "blog.langchain.dev_how-correct-are-llm-evaluators_.txt", "file_type": "text/plain", "file_size": 8844, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "92250e2742349fe52f810211aae79a2c342dd0e4f9ad0656a6d828a93522cd55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6f4ae6f-a0f7-4fed-b655-6fc05df310f0", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_gpt-researcher-x-langchain_.txt", "file_name": "blog.langchain.dev_gpt-researcher-x-langchain_.txt", "file_type": "text/plain", "file_size": 3556, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "5db5229ea2c48b97a7262778bd4deb25bbb54400b255113d6520ffd64f04d30a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39523747-413c-449d-bd8d-ad6ccf5f30c5", "node_type": "1", "metadata": {}, "hash": "79e86a592842647b83ef6948d6ba909e1ec13157d9974d14df6ac5f6865475d8", "class_name": "RelatedNodeInfo"}}, "hash": "d7a438dcd45f661a9057c412bb26b90789501bdb3fb33ad19c51221eecea6eea", "text": "URL: https://blog.langchain.dev/how-correct-are-llm-evaluators/\nTitle: How \"Correct\" are LLM Evaluators?\n\nSummary:\n\nWe tested LangChain's LLM-assisted evaluators on common tasks to provide guidelines on how to best use them in your practice.\n\nGPT-4 excels in accuracy across various tasks, while GPT-3.5 and Claude-2 lag for tasks requiring complex \"reasoning\" (when used in a zero-shot setting).\n\nContext\n\nEvaluating language model applications is a challenge. Evaluating by hand can be costly and time-consuming, and classic automated metrics like ROUGE or BLEU can often miss the point of what makes a \"good\" response. LLM-based evaluation methods are promising, but they aren't without issues. For instance, they can prefer their own output to human-written text, as revealed in recent research.\n\nAnother challenge is reliability. If an evaluation model operates in the same context as the model being assessed, its feedback might lack the depth needed for meaningful insights. This isn't a solved problem, and it's why we're committed to developing robust, flexible evaluation tools at LangChain.\n\nIn tasks such as question-answering and information extraction, 'correctness' is often the key metric. We've run experiments to measure the quality of LLM-based evaluators in determining \"correctness\" of outputs relative to a label, so we can share better guidelines and best practices for achieving reliable results.\n\nWhat we tested\n\nWe investigated three of LangChain's evaluators designed to grade whether a predicted output is \"correct\" relative to a label.\n\nQAEvalChain (link + prompt): prompts a model to grade the prediction as a teacher grading a quiz, ignoring spacing and wording.\n\nCoT evaluator (link + prompt): similar to the QA example above, but it instructs step-by-step reasoning using the provided context.\n\nLangChain also provides a \u201cCriteria\u201d evaluator (link), for testing whether a prediction meets the custom criterion provided (in this case, \"correctness\" relative to the reference). The prompt is similar to OpenAI's model graded evaluator prompt.\n\nWe tested all three evaluators using a binary 'right or wrong' scale, without giving them any few-shot examples for each task. Tests using additional prompting techniques or a continuous grading scale are saved for a future post. You can find the code for these experiments here (link) and the full summary table of these experiments here (link).\n\nCreating the datasets\n\nTo grade the reliability of these evaluators, we created benchmark datasets for three common tasks. For each source dataset, we transformed the answers using techniques to generate data splits where the predictions are known to be \u201cCorrect\u201d or \u201cIncorrect\u201d, assuming the original labels are reliable. Below is an overview for each dataset.\n\nQ&A: sampled from the WebQuestions dataset.\n\nThe \"Correct\" split was made by altering the true answers without changing their meaning. We swapped in synonyms, padded answers like, \"The answer to 'What is X' is Y,\" where \"Y\" is the correct answer, and we added small typos,\n\nThe \u201cIncorrect\u201d split was generated by selecting outputs from other rows in the dataset.\n\nTranslation: sampled from the Opus-100 dataset .\n\nThe \u201cCorrect\u201d split was made by padding with chit chat and inserting additional spaces where it wouldn't impact the way the sentence was read.\n\nThe \u201cIncorrect\u201d split was generated by selecting negative examples from other rows in the dataset or adding content not in the source phrase.\n\nExtraction: sampled from the CarbIE benchmark\n\nThe \u201cCorrect\u201d split was generated by shuffling the order of rows in the extracted triplets, keeping the content the same.\n\nThe \u201cIncorrect\u201d split was generated by inserting a new triple into each example.\n\nResults\n\nFor a full table of results, see the data in the link. We will answer some key questions in the sections below:\n\nWhich models should I use in evaluators?\n\nWhen selecting LLM's to use as a judge in our evaluators, we have traditionally recommended starting with GPT-4 since \"less capable\" models can give spurious results. Our experiments sought to validate this recommendation and provide more context on when a smaller model can be substituted in.\n\nEvaluator accuracy based on the eval LLM, for each dataset\n\nTable of Results The following results contain the accuracy/null rate of the evaluation outputs each model, selecting the *best* performing evaluator for each model. Dataset Claude-2 GPT-3.5-turbo GPT-3.5-turbo-instruct GPT-4 Carb-IE Correct 1.00 / 0.00 1.00 / 0.00 1.00 / 0.00 1.00 / 0.00 Carb-IE Incorrect 0.65 / 0.00 0.21 /", "start_char_idx": 0, "end_char_idx": 4584, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "39523747-413c-449d-bd8d-ad6ccf5f30c5": {"__data__": {"id_": "39523747-413c-449d-bd8d-ad6ccf5f30c5", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_how-correct-are-llm-evaluators_.txt", "file_name": "blog.langchain.dev_how-correct-are-llm-evaluators_.txt", "file_type": "text/plain", "file_size": 8844, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da7cd2f-71b1-4b5c-be5c-7494f7c964d6", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_how-correct-are-llm-evaluators_.txt", "file_name": "blog.langchain.dev_how-correct-are-llm-evaluators_.txt", "file_type": "text/plain", "file_size": 8844, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "92250e2742349fe52f810211aae79a2c342dd0e4f9ad0656a6d828a93522cd55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e964de9c-1efd-430b-a3f1-6217c7ce2962", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_how-correct-are-llm-evaluators_.txt", "file_name": "blog.langchain.dev_how-correct-are-llm-evaluators_.txt", "file_type": "text/plain", "file_size": 8844, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "d7a438dcd45f661a9057c412bb26b90789501bdb3fb33ad19c51221eecea6eea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62fd2cc8-8280-4ce1-9c3b-35523d3250ac", "node_type": "1", "metadata": {}, "hash": "7f41b049aee4e78285bd3ef9406577a40cdeced1090f3984a8292e85d14f7439", "class_name": "RelatedNodeInfo"}}, "hash": "79e86a592842647b83ef6948d6ba909e1ec13157d9974d14df6ac5f6865475d8", "text": "Carb-IE Incorrect 0.65 / 0.00 0.21 / 0.35 0.42 / 0.27 0.99 / 0.00 Opus100 - Correct 1.00 / 0.00 1.00 / 0.00 1.00 / 0.00 0.98 / 0.00 Opus100 - Incorrect 0.98 / 0.00 0.59 / 0.05 0.57 / 0.00 1.00 / 0.00 WebQ&A - Correct 1.00 / 0.00 1.00 / 0.00 1.00 / 0.00 1.00 / 0.00 WebQ&A - Incorrect 1.00 / 0.00 1.00 / 0.00 1.00 / 0.00 1.00 / 0.00\n\nThe results indicate GPT-4 indeed outperforms the others in structured \"reasoning\" tasks, such as when evaluating on the Carb-IE extraction dataset. On the other hand, Claude-2 and GPT-3.5 show reliability in simpler tasks like translation and Web Q&A but falter when additional reasoning is needed. Notably, the results table above shows that GPT-3.5-turbo struggled with false positives and had high null rates, meaning it often provided unusable responses.\n\nThis error analysis suggests that while prompt-tuning might improve performance, GPT-4 remains the most dependable general-purpose model for tasks requiring structured data reasoning. The instruct variant of GPT-3.5-turbo offers no significant advantage in response quality over its predecessor.\n\nHow reliable is a single evaluator across tasks?\n\nWe next wanted to see how well a single evaluator (which encapsulates a configurable prompt) generalizes across the different tasks. We used GPT-4 for prompt comparisons to ensure the evaluation was based on prompt effectiveness rather than model capability.\n\nEvaluator accuracy over each dataset\n\nTable of Results The following results contain the accuracy/null rate of the evaluation outputs using each evaluator, when using GPT-4 as the judge. Dataset cot_qa labeled_criteria qa Carb-IE Correct 1.00 / 0.00 0.98 / 0.01 1.00 / 0.00 Carb-IE Incorrect 0.76 / 0.00 0.98 / 0.01 0.98 / 0.00 Opus100 - Correct 0.98 / 0.00 0.87 / 0.00 0.98 / 0.00 Opus100 - Incorrect 0.99 / 0.01 1.00 / 0.00 0.99 / 0.00 Web Q&A - Correct 1.00 / 0.00 0.95 / 0.00 1.00 / 0.00 Web Q&A - Incorrect 1.00 / 0.00 1.00 / 0.00 1.00 / 0.00\n\nThe default \"qa\" prompt most consistently produced the expected answers, especially when compared to the chain-of-thought QA evaluator and the general criteria evaluator. In the Carb-IE Incorrect split, which tests the correctness of extracted knowledge triplets, the chain-of-thought QA evaluator underperformed significantly. It failed to penalize for extra, irrelevant triplets, revealing the limitation of applying a general \"quiz-style\" prompt to specialized tasks if without providing additional information.\n\nBelow are some examples to illustrate the relative behavior of the three evaluators on the same extraction data point:\n\nChain-of-thought QA evaluator (left), QA Evaluator (middle), and Criteria evaluator (right) outputs for a single dataset example.\n\nThe links in the images show how the chain-of-thought QA evaluator (link to run) disregards the extra information in its final grade, whereas both the standard QA (link) and labeled criteria (link) evaluators appropriately mark the prediction as \"incorrect\" for including spurious information.\n\nAdditional Insights\n\nTwo important observations also emerged from our tests:\n\nAt the time of testing, Claude-2 was sometimes prone to inconsistencies:\n\nIn test above, Claude-2 wrongly included \"Texas\" in its reference answer. Similarly, when using a different prompt, the model gets the chain of thought \"reasoning\" correct while still printing out the wrong answer.\n\n2. Zero-shot language models, like GPT-4 and Claude-2, carry inherent biases. These models can over-rely on their pre-trained knowledge, even when it conflicts with the actual data. For instance, when evaluating the example input \"who is the CEO of Twitter\" in the linked run,\n\nThe GPT-4 based", "start_char_idx": 4548, "end_char_idx": 8221, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62fd2cc8-8280-4ce1-9c3b-35523d3250ac": {"__data__": {"id_": "62fd2cc8-8280-4ce1-9c3b-35523d3250ac", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_how-correct-are-llm-evaluators_.txt", "file_name": "blog.langchain.dev_how-correct-are-llm-evaluators_.txt", "file_type": "text/plain", "file_size": 8844, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da7cd2f-71b1-4b5c-be5c-7494f7c964d6", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_how-correct-are-llm-evaluators_.txt", "file_name": "blog.langchain.dev_how-correct-are-llm-evaluators_.txt", "file_type": "text/plain", "file_size": 8844, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "92250e2742349fe52f810211aae79a2c342dd0e4f9ad0656a6d828a93522cd55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "39523747-413c-449d-bd8d-ad6ccf5f30c5", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_how-correct-are-llm-evaluators_.txt", "file_name": "blog.langchain.dev_how-correct-are-llm-evaluators_.txt", "file_type": "text/plain", "file_size": 8844, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "79e86a592842647b83ef6948d6ba909e1ec13157d9974d14df6ac5f6865475d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19b3d750-66b7-41ef-aa1e-1fcc6d1384e7", "node_type": "1", "metadata": {}, "hash": "521f737638b25d5e29ae3d93c4c787557d8d46ed9050f014cc6295d61e87ba74", "class_name": "RelatedNodeInfo"}}, "hash": "7f41b049aee4e78285bd3ef9406577a40cdeced1090f3984a8292e85d14f7439", "text": "input \"who is the CEO of Twitter\" in the linked run,\n\nThe GPT-4 based model marked the prediction of \"Elon Musk\" as incorrect, despite the reference answer providing the same information.\n\n\n\nThis problem can often be mitigated by refining the prompt or providing more context to the model. It is important to spot check your evaluation results to make sure they correspond with your intuition, especially if your task involves names or concepts where the model may have a \"high confidence\" in its trained knowledge.\n\nWhat's Next?\n\nWhile tweaks in prompt and output parsing have improved reliability, there are further enhancements that could further implement:", "start_char_idx": 8152, "end_char_idx": 8812, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19b3d750-66b7-41ef-aa1e-1fcc6d1384e7": {"__data__": {"id_": "19b3d750-66b7-41ef-aa1e-1fcc6d1384e7", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel_.txt", "file_name": "blog.langchain.dev_how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel_.txt", "file_type": "text/plain", "file_size": 10264, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6cc5d296-aa67-4c8e-849d-31c835174760", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel_.txt", "file_name": "blog.langchain.dev_how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel_.txt", "file_type": "text/plain", "file_size": 10264, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "98dd7f93dfd3792da4d3b849f9799304eac2a57f082d84a3d6e3faae54b5242b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62fd2cc8-8280-4ce1-9c3b-35523d3250ac", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_how-correct-are-llm-evaluators_.txt", "file_name": "blog.langchain.dev_how-correct-are-llm-evaluators_.txt", "file_type": "text/plain", "file_size": 8844, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "7f41b049aee4e78285bd3ef9406577a40cdeced1090f3984a8292e85d14f7439", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf154297-c15d-4b70-ad17-950be1b71696", "node_type": "1", "metadata": {}, "hash": "129160022a24838bc1fe3b3bf2f790b74df5e26364907bade7ca05a20d08f641", "class_name": "RelatedNodeInfo"}}, "hash": "521f737638b25d5e29ae3d93c4c787557d8d46ed9050f014cc6295d61e87ba74", "text": "URL: https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/\nTitle: How to Safely Query Enterprise Data with LangChain Agents + SQL + OpenAI + Gretel\n\nEditor's Note: This post was written in collaboration with the Gretel team. We're really excited by their approach to combining agent-based methods, LLMs, and synthetic data to enable natural language queries for databases and data warehouses, sans SQL. The post has a really helpful walkthrough (with code!) to bring the ideas to life.\n\nAgent-based approaches coupled with large language models (LLMs) are quickly transforming how we interact with databases and data warehouses. Combined, these technologies enable natural language queries to data in your application or business, eliminating the need for SQL expertise to interact with data and even facilitating seamless queries across diverse systems.\n\nIn this post, we\u2019ll walk through an example of how LangChain, LLMs (whether open-source models like Llama-2, Falcon, or API-based models from OpenAI, Google, Anthropic), and synthetic data from Gretel combine to create a powerful, privacy-preserving solution for natural language data interaction with data in databases and warehouses. We'll introduce key concepts such as Agents, LLM Chains, and synthetic data, then delve into a practical code example to bring these ideas to life.\n\nKey technologies\n\nLLM Chains : Frameworks such as LangChain for developing applications powered by language models by chaining them together.\n\n: Frameworks such as LangChain for developing applications powered by language models by chaining them together. Agents: Agents use an LLM to decide what actions to take and the order to take them in, making future decisions by iteratively observing the outcome of prior actions.\n\nAgents use an LLM to decide what actions to take and the order to take them in, making future decisions by iteratively observing the outcome of prior actions. Function Aware LLMs: Certain newer LLMs (like OpenAI\u2019s GPT-3.5-turbo-0613 and Google\u2019s PaLM text-bison) have been fine-tuned to detect when a function should be called and respond with the inputs that should be passed to the function.\n\nCertain newer LLMs (like OpenAI\u2019s GPT-3.5-turbo-0613 and Google\u2019s PaLM text-bison) have been fine-tuned to detect when a function should be called and respond with the inputs that should be passed to the function. Synthetic data: An artificial version of the real-world created by data-aware generative models that can offer strong privacy guarantees to data. Gretel offers generative models for working with tabular data based on Transformer, GAN, and graph-based architectures.\n\nAn artificial version of the real-world created by data-aware generative models that can offer strong privacy guarantees to data. Gretel offers generative models for working with tabular data based on Transformer, GAN, and graph-based architectures. SQL Databases: The backbone holding the data you'll be querying. For today, we\u2019ll use a SQLite database.\n\nWhat is an Agent in LangChain?\n\nSome applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user's input, too. In these types of chains, there is an \u201cagent\u201d that has access to a suite of tools \u2014 for example math, or the ability to query a SQL database. Depending on the user input, the agent can then decide which, if any, of these tools to call.\n\nUnder the hood, the LangChain SQL Agent uses a MRKL (pronounced Miracle)-based approach, and queries the database schema and example rows and uses these to generate SQL queries, which it then executes to pull back the results you're asking for.\n\nGenerating synthetic tabular data\n\nBefore diving into the example, let's talk about synthetic data. With Gretel's models, you can make an artificial but statistically similar version of your sensitive data. This synthetic data is safe to use, thanks to math-backed privacy features like differential privacy. In our example, we'll use both real and synthetic data to show why this privacy is crucial when letting language models access sensitive info.\n\nTo generate your own synthetic data for this example, grab the IBM HR Employee Attrition dataset (or your own) and an API key from https://console.gretel.ai. You can run Gretel's quickstart notebook or console-based workflow to create a synthetic version of the data.\n\nFor this example, I used the Gretel Tabular DP model (notebook, docs) with an epsilon value of 5 for strong privacy guarantees that are great for regulated environments. For maximum accuracy while still maintaining privacy, you can also try the Gretel ACTGAN model (docs), which excels at", "start_char_idx": 0, "end_char_idx": 4736, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf154297-c15d-4b70-ad17-950be1b71696": {"__data__": {"id_": "bf154297-c15d-4b70-ad17-950be1b71696", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel_.txt", "file_name": "blog.langchain.dev_how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel_.txt", "file_type": "text/plain", "file_size": 10264, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6cc5d296-aa67-4c8e-849d-31c835174760", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel_.txt", "file_name": "blog.langchain.dev_how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel_.txt", "file_type": "text/plain", "file_size": 10264, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "98dd7f93dfd3792da4d3b849f9799304eac2a57f082d84a3d6e3faae54b5242b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19b3d750-66b7-41ef-aa1e-1fcc6d1384e7", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel_.txt", "file_name": "blog.langchain.dev_how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel_.txt", "file_type": "text/plain", "file_size": 10264, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "521f737638b25d5e29ae3d93c4c787557d8d46ed9050f014cc6295d61e87ba74", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3aae7b83-4bc0-426c-9a88-dad4e183565a", "node_type": "1", "metadata": {}, "hash": "992d6a79c140f20bb236ab34e3ed97821c2e0065afc3e964fad45e7353b86e55", "class_name": "RelatedNodeInfo"}}, "hash": "129160022a24838bc1fe3b3bf2f790b74df5e26364907bade7ca05a20d08f641", "text": "maintaining privacy, you can also try the Gretel ACTGAN model (docs), which excels at working with highly dimensional tabular data to enable machine learning and analytics use cases.\n\nGetting started: Installation\n\nFollow along with our complete notebook in Colab or GitHub.\n\nFirst, install dependencies.\n\n!pip install -Uqq langchain openai gretel-client !pip install -Uqq smart_open tabulate\n\nInitializing the LangChain Agent\n\nNote: Please use your OpenAI key for this, which should be kept private.\n\nHere's the code to initialize the LangChain Agent and connect it to your SQL database.\n\nfrom langchain.agents import AgentExecutor, create_sql_agent from langchain.agents.agent_toolkits import SQLDatabaseToolkit from langchain.agents.agent_types import AgentType from langchain.chat_models import ChatOpenAI from langchain.llms.openai import OpenAI from langchain.sql_database import SQLDatabase def create_agent( db_uri, agent_type=AgentType.OPENAI_FUNCTIONS, verbose=VERBOSE_LANGCHAIN, temperature=0, model=\"gpt-3.5-turbo-0613\", ): db = SQLDatabase.from_uri(db_uri) toolkit = SQLDatabaseToolkit(db=db, llm=OpenAI(temperature=temperature)) return create_sql_agent( llm=ChatOpenAI(temperature=temperature, model=model), toolkit=toolkit, verbose=verbose, agent_type=agent_type, )\n\nHere, we are also importing some sample datasets. We'll use both a real and a synthetic version of the IBM attrition HR dataset. The synthetic version is generated using Gretel's Tabular DP model with an (\u03b5) Epsilon of 5.\n\n# Create SQLite databases from CSV datasets create_sqlite_db_from_csv( SYNTHETIC_DATA, db_name=\"synthetic-sqlite.db\", table_name=\"synthetic_ibm_attrition\" ) create_sqlite_db_from_csv( REAL_DATA, db_name=\"real-sqlite.db\", table_name=\"real_ibm_attrition\" ) # Create SQL agent to interact with synthetic IBM attrition data agent_synthetic_db = create_agent(\"sqlite:////content/synthetic-sqlite.db\") # Create SQL agent to interact with real-world IBM attrition data agent_real_db = create_agent(\"sqlite:////content/real-sqlite.db\")\n\nQuerying the data\n\nFirst, we'll create a helper function to compare the outputs of real data and synthetic data.\n\ndef run_and_compare_queries(synthetic, real, query: str): \"\"\"Compare outputs of Langchain Agents running on real vs. synthetic data\"\"\" query_template = f\"{query} Execute all necessary queries, and always return results to the query, no explanations or apologies please. Word wrap output every 50 characters.\" result1 = synthetic.run(query_template) result2 = real.run(query_template) print(\"=== Comparing Results for Query ===\") print(f\"Query: {query}\") table_data = [ {\"From Agent on Synthetic DB\": result1, \"From Agent on Real DB\": result2} ] print(tabulate(table_data, headers=\"keys\", tablefmt=\"pretty\"))\n\nSample queries\n\nWhich three departments have the highest attrition rates?\n\nprompt = \"Which 3 departments have the highest attrition rates? Return a list please.\" run_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)\n\nFigure 1. Comparing real and synthetic results for query #1.\n\nThe results were quite similar between the synthetic and real datasets, giving us confidence in the synthetic data's reliability.\n\nWhat is the distribution of ages by 10-year increments across the entire dataset?\n\nprompt = \"Show me a distribution of ages by 10 year increments. Return in list format please.\" run_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)\n\nAgain, the distributions were notably similar between the synthetic and real data sets.\n\nWhich department travels the furthest from home?\n\nprompt = \"Which department travels the furthest from home?\" run_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)\n\nFigure 3. Comparing real and synthetic results for query #3.\n\nIn this case, we get a perfect match.\n\nImportance of privacy: Re-identification attack example\n\nHere, we illustrate a \"re-identification attack\" where vulnerabilities in even de-identified datasets can allow an attacker to re-identify individuals by combining known attributes. Such risks emphasize the danger of sharing data stripped of direct identifiers yet containing attributes that, when combined, can lead to identification \u2014 such as the combination of an attacker who knew someone\u2019s age, gender, and department in the example below.\n\nSynthetic data prevents direct linking of individual information as no record in the output is based on a single user\u2019s", "start_char_idx": 4651, "end_char_idx": 9130, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3aae7b83-4bc0-426c-9a88-dad4e183565a": {"__data__": {"id_": "3aae7b83-4bc0-426c-9a88-dad4e183565a", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel_.txt", "file_name": "blog.langchain.dev_how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel_.txt", "file_type": "text/plain", "file_size": 10264, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6cc5d296-aa67-4c8e-849d-31c835174760", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel_.txt", "file_name": "blog.langchain.dev_how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel_.txt", "file_type": "text/plain", "file_size": 10264, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "98dd7f93dfd3792da4d3b849f9799304eac2a57f082d84a3d6e3faae54b5242b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf154297-c15d-4b70-ad17-950be1b71696", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel_.txt", "file_name": "blog.langchain.dev_how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel_.txt", "file_type": "text/plain", "file_size": 10264, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "129160022a24838bc1fe3b3bf2f790b74df5e26364907bade7ca05a20d08f641", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6bbe09c1-6aff-4f7a-af38-59c0a67c3b69", "node_type": "1", "metadata": {}, "hash": "dc0b24457c802e78a707d166bce64e2d935e7951933c9804d7cf726b4d860198", "class_name": "RelatedNodeInfo"}}, "hash": "992d6a79c140f20bb236ab34e3ed97821c2e0065afc3e964fad45e7353b86e55", "text": "data prevents direct linking of individual information as no record in the output is based on a single user\u2019s data, effectively thwarting re-identification attacks and upholding privacy.\n\nprompt = \"Is there an employee who is Age 46, Female, and who works in Human Resources. If so, what is their monthly income, performance rating, and years since their last promotion?\" run_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)\n\nConclusion\n\nBy using synthetic data, you not only protect privacy but also gain actionable insights\u2014essential for any data-driven organization. When you blend this with agent-based approaches and large language models, you open the door for more and better stakeholder collaborations. No SQL expertise needed; simply use natural language to engage with your data across all levels of your organization.\n\nThis scalable solution democratizes data access and ushers in a new era of smart, privacy-conscious data interaction. For businesses eager to maintain a competitive edge in today's data-centric world, adopting these technologies isn't just an option; it's a must.\n\nIf you're ready to up your data game, sign up for Gretel today and start synthesizing.", "start_char_idx": 9021, "end_char_idx": 10237, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6bbe09c1-6aff-4f7a-af38-59c0a67c3b69": {"__data__": {"id_": "6bbe09c1-6aff-4f7a-af38-59c0a67c3b69", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_incorporating-domain-specific-knowledge-in-sql-llm-solutions_.txt", "file_name": "blog.langchain.dev_incorporating-domain-specific-knowledge-in-sql-llm-solutions_.txt", "file_type": "text/plain", "file_size": 8916, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "591d6c26-7b4a-46cf-a55e-e0a3853009b0", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_incorporating-domain-specific-knowledge-in-sql-llm-solutions_.txt", "file_name": "blog.langchain.dev_incorporating-domain-specific-knowledge-in-sql-llm-solutions_.txt", "file_type": "text/plain", "file_size": 8916, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "cb27cbbc743c985099738c6e060f3fa01a681cb7419f8b65c1c3d3ff26e806dc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3aae7b83-4bc0-426c-9a88-dad4e183565a", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel_.txt", "file_name": "blog.langchain.dev_how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel_.txt", "file_type": "text/plain", "file_size": 10264, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "992d6a79c140f20bb236ab34e3ed97821c2e0065afc3e964fad45e7353b86e55", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "620e52d2-d58f-4b9b-80dc-3cf4a505692f", "node_type": "1", "metadata": {}, "hash": "0c2a99fff6a72c5ee947f949b359aa7dc7d26d00c64d5538e4f1800e55f84659", "class_name": "RelatedNodeInfo"}}, "hash": "dc0b24457c802e78a707d166bce64e2d935e7951933c9804d7cf726b4d860198", "text": "URL: https://blog.langchain.dev/incorporating-domain-specific-knowledge-in-sql-llm-solutions/\nTitle: Incorporating domain specific knowledge in SQL-LLM solutions\n\nEditor's Note: This post was written in collaboration with Manuel and Francisco from the Pampa Labs team. We're always excited to see new best practices emerge for more customizing/personalizing apps more thoroughly, and this post about extending the capabilities of the standard SQL toolkit by applying innovative RAG techniques is an awesome example.\n\nThe LangChain library provides different tools to interact with SQL databases which can be used to build and run queries based on natural language inputs. For example, the standard SQL Toolkit draws from standard best practices that have been extensively covered in this blogpost. However, there is still room for improvement when it comes to building a custom solution and adjusting the generic tools to the specific use case. The advantage of having a plug and play toolkit contrasts with having a solution that is not flexible enough for the user to incorporate their domain-specific knowledge about the databases.\n\nWe can extend the out-of-the-box SQL Toolkit with extra custom tools which leverage domain specific knowledge. In this way, we get the best of both worlds: anyone can run the standard SQL Agent with minimal setup while at the same time being able to incorporate extra tools that add relevant information to the prompt at inference time. In this blogpost we will cover how to expand the standard SQL Toolkit with some very useful example extra tools.\n\nThe Problems\n\nUsing the standard SQL Toolkit, an agent is able to construct and run queries to provide answers to user questions. Although this toolkit is robust enough for building a first out-of-the-box prototype by just connecting to a database, someone trying to use it with a complex enough database faces at least one of the following problems:\n\nQueries not generated correctly, leading to various retries until getting the right query.\n\nExcessive use of the tools, making the whole thinking process very inefficient in terms of time and tokens.\n\nVery extensive prompts with information that is not always relevant to the specific user question.\n\nThe underlying cause behind these problems is that we are trying to build a custom solution just using generic tools, without leveraging the fact that we do know the nuances of the use case. Therefore, we need to find a way of enhancing the agent with domain specific knowledge, without having to hardcode anything in the prompt template.\n\nExtending the SQL Toolkit\n\nIt has been proven that feeding the prompt with database information is crucial for constructing the right SQL query. This is why the toolkit enables the agent to get information about the table names, the schema, sample rows, etc. However, all these tools can do is retrieve information about the database, akin to how a data scientist would approach a new dataset during their initial interaction.\n\nBut what if it\u2019s not the first interaction?\n\nAnyone crafting an LLM-SQL solution brings a wealth of domain-specific knowledge to the table. They know which questions are typically hard to translate into queries, as well as when and what supplementary information should be incorporated into the prompt. This becomes especially crucial in scenarios where simply using the standard toolkit falls short. Such insights can be dynamically included into the prompt using Retrieval Augmented Generation, which involves semantically searching in a vector database and retrieving relevant data.\n\nIncluding few shot examples\n\nFeeding the prompt with few-shot examples of question-query matches improves the query generation accuracy. This can be achieved by simply appending standard static examples in the prompt to guide the agent on how it should build queries based on questions. However, a more powerful approach is to have a robust dataset of good examples, and dynamically include those which are relevant to the user question.\n\nTo achieve this, we need a custom Retriever Tool that handles the vector database in order to retrieve the examples that are semantically similar to the user\u2019s question. The agent can even decide whether it needs to use other tools or not.\n\nLet\u2019s see an example!\n\nagent.run(\"How many employees do we have?\") > Entering new AgentExecutor chain... Invoking: `sql_get_similar_examples` with `How many employees do we have?` [Document(page_content='How many employees are there', metadata={'sql_query': 'SELECT COUNT(*) FROM \"employee\"'}), Document(page_content='Which employee has sold the most?', metadata={'sql_query': \"SELECT e.FirstName || ' ' || e.LastName AS EmployeeName, SUM(i.Total) AS TotalSales\n\nFROM Employee e\n\nJOIN Customer c ON e.EmployeeId = c.SupportRepId\n\nJOIN Invoice i ON c.CustomerId = i.CustomerId\n\nGROUP BY e.EmployeeId\n\nORDER BY TotalSales DESC\n\nLIMIT 1;\"})] Invoking: `sql_db_query` with `SELECT COUNT(*) FROM employee` responded: {content} [(8,)]We have 8 employees. > Finished chain.\n\nFinding misspellings in proper nouns\n\nAnother nice use case of applying", "start_char_idx": 0, "end_char_idx": 5114, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "620e52d2-d58f-4b9b-80dc-3cf4a505692f": {"__data__": {"id_": "620e52d2-d58f-4b9b-80dc-3cf4a505692f", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_incorporating-domain-specific-knowledge-in-sql-llm-solutions_.txt", "file_name": "blog.langchain.dev_incorporating-domain-specific-knowledge-in-sql-llm-solutions_.txt", "file_type": "text/plain", "file_size": 8916, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "591d6c26-7b4a-46cf-a55e-e0a3853009b0", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_incorporating-domain-specific-knowledge-in-sql-llm-solutions_.txt", "file_name": "blog.langchain.dev_incorporating-domain-specific-knowledge-in-sql-llm-solutions_.txt", "file_type": "text/plain", "file_size": 8916, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "cb27cbbc743c985099738c6e060f3fa01a681cb7419f8b65c1c3d3ff26e806dc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6bbe09c1-6aff-4f7a-af38-59c0a67c3b69", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_incorporating-domain-specific-knowledge-in-sql-llm-solutions_.txt", "file_name": "blog.langchain.dev_incorporating-domain-specific-knowledge-in-sql-llm-solutions_.txt", "file_type": "text/plain", "file_size": 8916, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "dc0b24457c802e78a707d166bce64e2d935e7951933c9804d7cf726b4d860198", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad43e8ca-78d4-4915-87e2-1684d61c49b9", "node_type": "1", "metadata": {}, "hash": "364cf2b4ea29f02876ba89ea6d5d586bf81840e5eb80362eb53d3a266c3795c8", "class_name": "RelatedNodeInfo"}}, "hash": "0c2a99fff6a72c5ee947f949b359aa7dc7d26d00c64d5538e4f1800e55f84659", "text": "employees. > Finished chain.\n\nFinding misspellings in proper nouns\n\nAnother nice use case of applying RAG in LLM-SQL solutions is for making a system robust to misspellings. When querying for proper nouns like names or countries, a user may inadvertently write a proper noun wrongly and the system will not be able to find it in the database (e.g. \u2018Franc Sinatra\u2019).\n\nHow can we solve this problem?\n\nOne way to approach this problem is to create a vector store using all the distinct proper nouns that exist in the database. We can then have the agent query that vector store each time the user includes a proper noun in their question, to find the correct spelling for that word. In this way, the agent can make sure it understands which entity the user is referring to before building the target query.\n\nLet\u2019s see an example!\n\n` sql_agent(\"What is 'Francis Trembling's email address?\") Invoking: `name_search` with `Francis Trembling` [Document(page_content='Fran\u00e7ois Tremblay', metadata={}), Document(page_content='Edward Francis', metadata={}), Document(page_content='Frank Ralston', metadata={}), Document(page_content='Frank Harris', metadata={}), Document(page_content='N. Frances Street', metadata={})] Invoking: `sql_db_query_checker` with `SELECT Email FROM Customer WHERE FirstName = 'Fran\u00e7ois' AND LastName = 'Tremblay' LIMIT 1` responded: {content} SELECT Email FROM Customer WHERE FirstName = 'Fran\u00e7ois' AND LastName = 'Tremblay' LIMIT 1 Invoking: `sql_db_query` with `SELECT Email FROM Customer WHERE FirstName = 'Fran\u00e7ois' AND LastName = 'Tremblay' LIMIT 1` [('ftremblay@gmail.com',)]The email address of 'Fran\u00e7ois Tremblay' is 'ftremblay@gmail.com'. > Finished chain. {'input': \"What is 'Francis Trembling' email address?\", 'output': \"The email address of 'Fran\u00e7ois Tremblay' is 'ftremblay@gmail.com'.\"}\n\nImplementation note: when instructing the LLM to use tools in one order or another, we found it was usually more effective to instruct this in the agent\u2019s prompt rather than in the tool\u2019s description - for more information please refer to the SQL use case in the docs.\n\n\n\nGoing further\n\nAs well as these best practices improve the standard SQL Toolkit by leveraging the developer\u2019s field-specific knowledge, there is still room for improvement in terms of accuracy and cost.\n\nSome examples on enhancing the few-shot approach include:\n\nApplying a similarity threshold to decide whether the retrieved examples are related enough to be included in the prompt (e.g. a new question which is very different to other questions, shouldn\u2019t retrieve any examples).\n\nto decide whether the retrieved examples are related enough to be included in the prompt (e.g. a new question which is very different to other questions, shouldn\u2019t retrieve any examples). Similarly, setting a threshold to decide if the examples are far too related , and no other tools should be used, thus saving a lot of time & tokens (e.g. just adjusting a column filter, just having a related example is enough and no other tools should be necessary).\n\n, and no other tools should be used, thus saving a lot of time & tokens (e.g. just adjusting a column filter, just having a related example is enough and no other tools should be necessary). Prioritizing diversity of the few-shot examples in order to cover a wider area of examples, as covered in the following paper by Hongjin Su et al.\n\nAlso, some examples which aren\u2019t strictly related to the few-shot examples but do involve using RAG include:\n\nRetrieving all values from a relevant categoric column if the user\u2019s question involves filtering a column (e.g. a product name).\n\nAdjusting sample rows to show only the columns that are relevant to the user question.\n\nIf you want to help implementing any of these or have other best practices that you found helpful, don\u2019t hesitate to join the discussion in the #sql channel in Discord!", "start_char_idx": 5013, "end_char_idx": 8882, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad43e8ca-78d4-4915-87e2-1684d61c49b9": {"__data__": {"id_": "ad43e8ca-78d4-4915-87e2-1684d61c49b9", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_integrating-chatgpt-with-google-drive-and-notion-data_.txt", "file_name": "blog.langchain.dev_integrating-chatgpt-with-google-drive-and-notion-data_.txt", "file_type": "text/plain", "file_size": 4325, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "90b3f1ff-bfc5-4c52-aecf-6719d5d37e5d", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_integrating-chatgpt-with-google-drive-and-notion-data_.txt", "file_name": "blog.langchain.dev_integrating-chatgpt-with-google-drive-and-notion-data_.txt", "file_type": "text/plain", "file_size": 4325, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "aa697e530e9d51a9956fbd1548c8dac253111878d3c8eb4741d65f3896529a37", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "620e52d2-d58f-4b9b-80dc-3cf4a505692f", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_incorporating-domain-specific-knowledge-in-sql-llm-solutions_.txt", "file_name": "blog.langchain.dev_incorporating-domain-specific-knowledge-in-sql-llm-solutions_.txt", "file_type": "text/plain", "file_size": 8916, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "0c2a99fff6a72c5ee947f949b359aa7dc7d26d00c64d5538e4f1800e55f84659", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2223a2d0-86e0-41ed-b897-2b41bb92c1b7", "node_type": "1", "metadata": {}, "hash": "322062e97b9f6a22a9f074bc47d6acb44db93a60dddac688c29444892782ad22", "class_name": "RelatedNodeInfo"}}, "hash": "364cf2b4ea29f02876ba89ea6d5d586bf81840e5eb80362eb53d3a266c3795c8", "text": "URL: https://blog.langchain.dev/integrating-chatgpt-with-google-drive-and-notion-data/\nTitle: Tavrn x LangChain: Integrating Noah: ChatGPT with Google Drive and Notion data\n\nEditor's Note: This post was written in collaboration with the Tavrn team. They were able to build a new personal assistant app, Noah, that's highly personalized and highly context-aware using LangChain (with some interesting retrieval tactics) and LangSmith (for fine-tuning chains and prompts).\n\nChatGPT is already an indispensable tool for many in the workplace. Its impressive general purpose performance makes it extremely versatile to assist in workflows ranging from creative brainstorming to coding. In order to get the best outputs from ChatGPT, users are familiar with the process of prompting - providing the chat with as much context and instructions as possible so the output is satisfactory.\n\n\n\nThe POV of this laborious user experience usually involves multiple rounds of copy/pasting of parts of multiple documents that contain relevant information to the prompt. Given that ChatGPT has no context whatsoever on the user or his work, the output quality highly depends on information the user provides. For instance, if a user wants ChatGPT's best help to prioritize which product features to build first, he will have to:\n\n\n\n1. Find and open all documents that could potentially have useful context to ChatGPT (e.g. product meeting notes, user feedback reports, information about the product itself)\n\n\n\n2. Read through each document, copy the relevant parts and paste on ChatGPT\n\n\n\n3. Hope it all fits the character limit of ChatGPT and that he did not forget to include any important context\n\n\n\nThis inefficient, manual process of always having to supply the best context to ChatGPT prevents users from utilizing it for more complex use cases like the one illustrated above. We built Noah to resolve the context fetching problem and allow users to experience an AI copilot that always efficiently retrieves the best possible context to answer user queries.\n\n\n\nSimplicity and user-friendliness are core to Noah. We take care of all the heavy-lifting in the background. In just a few clicks, users can sync hundreds of files from their own Google Drive and Notion and start getting help from Noah.\n\n\n\nPowered by LangChain, Noah unlocks a more powerful and relevant use of LLMs in the workplace: a personal AI assistant that provides help specifically to users and their work. In the product prioritization example above, Noah would take care of all three steps (finding relevant documents, selecting the most relevant parts in each document, adding each part to the LLM prompt) so the user's sole input to the chat can be \"which product features should I prioritize?\"\n\n\n\nTo get started on Noah, users select the tools from which they would like to sync files.\n\nAfter the tool is selected, users can either choose specific files or quickly select their 200 most recent files from Google Drive or Notion.\n\n\n\nOnce users select their files, Noah processes the documents using optimized, context-aware document loaders in addition to state-of-the-art embeddings models. We tried multiple forms of semantic chunking but LangChain's CharacterTextSplitter with around 2,400 characters per chunk outperformed all the others for all types of documents - spreadsheets, documents, PDFs, slides.\n\n\n\nThen, once a user asks a question, Noah fetches the most relevant content across multiple sources utilizing cosine similarity vector search and passes them to multi-chain LLM calls where the best possible answer is obtained. We also tried other forms of retrieval but cosine similarity substantially outperformed the others.\n\n\n\n\n\nLangsmith was extra useful to us when fine-tuning which chains and prompts to use for the final user answer. Among the learnings, the optimal memory \"k\" parameter for ConversationBufferWindowMemory is 1 otherwise the answers get unreliable with so much historical context. Additionally, after the chunks are retrieved, we pass them into an intermediary, GPT-4 powered chain to filter out any conflicting information, prioritizing more recent sources.\n\n\n\nFinally, Noah provides the answer, with the appropriate sources cited.\n\n\n\nTo get started with Noah and boost your productivity, access https://tavrn.art/noah.", "start_char_idx": 0, "end_char_idx": 4317, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2223a2d0-86e0-41ed-b897-2b41bb92c1b7": {"__data__": {"id_": "2223a2d0-86e0-41ed-b897-2b41bb92c1b7", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_introducing-airbyte-sources-within-langchain_.txt", "file_name": "blog.langchain.dev_introducing-airbyte-sources-within-langchain_.txt", "file_type": "text/plain", "file_size": 6451, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f270085a-4b2d-4cd3-a38d-94e567ba30ba", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_introducing-airbyte-sources-within-langchain_.txt", "file_name": "blog.langchain.dev_introducing-airbyte-sources-within-langchain_.txt", "file_type": "text/plain", "file_size": 6451, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "497a81cb40ae1efef258e92b1d55990357d4fed7e10d53e63e0f32731e3b9d4b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad43e8ca-78d4-4915-87e2-1684d61c49b9", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_integrating-chatgpt-with-google-drive-and-notion-data_.txt", "file_name": "blog.langchain.dev_integrating-chatgpt-with-google-drive-and-notion-data_.txt", "file_type": "text/plain", "file_size": 4325, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "364cf2b4ea29f02876ba89ea6d5d586bf81840e5eb80362eb53d3a266c3795c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68ac249e-80c8-451f-84b4-56b62e56769e", "node_type": "1", "metadata": {}, "hash": "49581352f750b263f2befbaad51529d308a391e0398173bc53cfe32829cc4715", "class_name": "RelatedNodeInfo"}}, "hash": "322062e97b9f6a22a9f074bc47d6acb44db93a60dddac688c29444892782ad22", "text": "URL: https://blog.langchain.dev/introducing-airbyte-sources-within-langchain/\nTitle: Introducing Airbyte sources within LangChain\n\nEditor's Note: This post was written in collaboration with the Airbyte team. They've made it really easy to connect even more data sources to LangChain as document loaders.\n\nIt\u2019s now possible to utilize the Airbyte sources for Gong, Hubspot, Salesforce, Shopify, Stripe, Typeform and Zendesk Support directly within your LangChain-based application, implemented as document loaders.\n\nFor example, to load the Stripe invoices for a user, you can use the AirbyteStripeLoader. Installing it is super simple, when you have LangChain installed locally you only need to install the source you are interested in, and you are ready to go:\n\npip install airbyte-source-stripe\n\nAfter that, simply import the loader and pass in configuration and the stream you want to load:\n\nfrom langchain.document_loaders.airbyte import AirbyteStripeLoader config = { \"client_secret\": \"<secret key>\", \"account_id\": \"<account id>\", \"start_date\": \"<date from which to start retrieving records from in ISO format, e.g. 2020-10-20T00:00:00Z>\" } loader = AirbyteStripeLoader(config=config, stream_name=\"invoices\") documents = loader.load() # use documents in vector store or otherwise\n\nWhy does this matter?\n\nThis is the beginning of making Airbyte\u2019s 300+ sources available as document loaders in LangChain.\n\nAirbyte can move data from just about any source to your warehouse or vector database to power your LLM use case (check out this tutorial for setting up such a data pipeline!). This is normally done by using Airbyte Cloud or a local Airbyte instance, setting up a connection, and running it on a schedule (or via API trigger) to make sure your data stays fresh.\n\nBut if you are just getting started and are running everything locally, using a full Airbyte instance (including the UI, scheduling service, scale-out capabilities, etc..) may be overkill.\n\nWith this release, it\u2019s easier than ever to run any Python-based source in LangChain directly within your Python runtime - no need to spin up an Airbyte instance or make API calls to Airbyte Cloud.\n\nMoving between hosted and embedded Airbyte\n\nAs it\u2019s the same code running under the hood, every Airbyte-built loader is compatible with the respective source in the Airbyte service. This means it\u2019s trivial to lift your embedded loading pipeline into your self-hosted Airbyte installation or your Airbyte Cloud instance. The shape of the configuration object and the records is 100% compatible.\n\nRunning syncs on hosted Airbyte means:\n\nUI to keep track of running pipelines\n\nAlerting on failing syncs\n\nEasily running pipelines on a schedule\n\nRunning syncs with LangChain loaders means:\n\nNo overhead for running yet another service\n\nFull control over timing and pipeline execution\n\nMapping Airbyte records to LangChain documents\n\nBy default, each record gets mapped to a Document as part of the loader, with all the various fields in the record becoming the metadata of the record. The text portion of the document is left as an empty string. You can pass in a record handler to customize this behavior to build the text part of a record depending on the data:\n\ndef handle_record(record, id): return Document(page_content=record.data[\"title\"], metadata=record.data) loader = AirbyteGongLoader(config=config, record_handler=handle_record, stream_name=\"calls\")\n\nIncremental loads\n\nSince your python application is basically acting as the Airbyte platform, you have full control over how the \u201csync\u201d is executed. For example you can still benefit from incremental syncs if your stream supports it by accessing the \u201clast_state\u201d property of the loader. This allows you to load only documents that changed since the last time you loaded, allowing you to update an existing vector database effectively:\n\nimport airbyte_cdk.models.airbyte_protocol import AirbyteMessage with open('stripe_sync_checkpoint.json', 'wb') as file: file.write(loader.last_state.json()) // ... later with open('stripe_sync_checkpoint.json', 'r') as file: current_state = AirbyteStateMessage.parse_raw(file.read()) incremental_loader = AirbyteStripeLoader(config=config, stream_name=\"invoices\", state=current_state) new_docs = incremental_loader.load()\n\nCustom sources\n\nFor now, the following Airbyte sources are available as pip packages (with more to come):\n\nGong pip install airbyte-source-gong\n\nHubspot pip install airbyte-source-hubspot\n\nSalesforce pip install airbyte-source-salesforce\n\nShopify pip install airbyte-source-shopify\n\nStripe pip install airbyte-source-stripe\n\nTypeform pip install airbyte-source-typeform\n\nZendesk Support pip install airbyte-source-zendesk-support\n\nHowever, if you have implemented your own custom Airbyte", "start_char_idx": 0, "end_char_idx": 4766, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68ac249e-80c8-451f-84b4-56b62e56769e": {"__data__": {"id_": "68ac249e-80c8-451f-84b4-56b62e56769e", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_introducing-airbyte-sources-within-langchain_.txt", "file_name": "blog.langchain.dev_introducing-airbyte-sources-within-langchain_.txt", "file_type": "text/plain", "file_size": 6451, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f270085a-4b2d-4cd3-a38d-94e567ba30ba", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_introducing-airbyte-sources-within-langchain_.txt", "file_name": "blog.langchain.dev_introducing-airbyte-sources-within-langchain_.txt", "file_type": "text/plain", "file_size": 6451, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "497a81cb40ae1efef258e92b1d55990357d4fed7e10d53e63e0f32731e3b9d4b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2223a2d0-86e0-41ed-b897-2b41bb92c1b7", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_introducing-airbyte-sources-within-langchain_.txt", "file_name": "blog.langchain.dev_introducing-airbyte-sources-within-langchain_.txt", "file_type": "text/plain", "file_size": 6451, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "322062e97b9f6a22a9f074bc47d6acb44db93a60dddac688c29444892782ad22", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6049579a-0074-42a5-908f-ed134ceb3e0d", "node_type": "1", "metadata": {}, "hash": "d6a1d27daa32a3b9ad78b15efca8c60981276cc87e8a81d2d7100b4288c84a9e", "class_name": "RelatedNodeInfo"}}, "hash": "49581352f750b263f2befbaad51529d308a391e0398173bc53cfe32829cc4715", "text": "install airbyte-source-zendesk-support\n\nHowever, if you have implemented your own custom Airbyte sources, it\u2019s also possible to integrate them by using the AirbyteCDKLoader base class that works with the Source interface of the Airbyte CDK:\n\nfrom langchain.document_loaders.airbyte import AirbyteCDKLoader from my_source.source import MyCustomSource # plug in your own source here config = { # your custom configuration } loader = AirbyteCDKLoader(source_class=MyCustomSource, config=config, stream_name=\"my-stream\")\n\nYou can also install sources from the main Airbyte repository by installing directly via git - for example, to fetch the Github source, simply run\n\npip install \"source_github@git+https://github.com/airbytehq/airbyte.git@master#subdirectory=airbyte-integrations/connectors/source-github\"\n\nAfter that, the source is available to be plucked into the AirbyteCDKLoader:\n\nfrom source_github.source import SourceGithub issues_loader = AirbyteCDKLoader(source_class=SourceGithub, config=config, stream_name=\"issues\")\n\nCheck out the connector development documentation for how to get started writing your own sources - it\u2019s easy to get started with them and will allow you to move from local embedded loaders to using a hosted Airbyte instance seamlessly depending on your needs.\n\nAny questions? We would love to hear from you\n\nIf you are interested in leveraging Airbyte to ship data to your LLM-based applications, please take a moment to fill out our survey so we can make sure to prioritize the most important features.\n\nIf you have questions or are interested in other existing sources being exposed as loaders this way, do not hesitate to reach out on our community slack channel or in the Airbyte channel on the LangChain discord server.", "start_char_idx": 4670, "end_char_idx": 6423, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6049579a-0074-42a5-908f-ed134ceb3e0d": {"__data__": {"id_": "6049579a-0074-42a5-908f-ed134ceb3e0d", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-and-scrimba-partner-to-help-web-devs-become-ai-engineers_.txt", "file_name": "blog.langchain.dev_langchain-and-scrimba-partner-to-help-web-devs-become-ai-engineers_.txt", "file_type": "text/plain", "file_size": 2955, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "33f29cd2-df22-4a07-964c-35e7dffb43bd", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-and-scrimba-partner-to-help-web-devs-become-ai-engineers_.txt", "file_name": "blog.langchain.dev_langchain-and-scrimba-partner-to-help-web-devs-become-ai-engineers_.txt", "file_type": "text/plain", "file_size": 2955, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "2d49e5c562c87c082e8dcd053d1537963754c73a068bddfa098e9860700bf6b1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68ac249e-80c8-451f-84b4-56b62e56769e", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_introducing-airbyte-sources-within-langchain_.txt", "file_name": "blog.langchain.dev_introducing-airbyte-sources-within-langchain_.txt", "file_type": "text/plain", "file_size": 6451, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "49581352f750b263f2befbaad51529d308a391e0398173bc53cfe32829cc4715", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ce87ded-fe42-44cf-a315-2559355c3dfb", "node_type": "1", "metadata": {}, "hash": "a5a67953a23b24f1687959603fd874f70aac8616c85efa4ef9de4d65e4c47400", "class_name": "RelatedNodeInfo"}}, "hash": "d6a1d27daa32a3b9ad78b15efca8c60981276cc87e8a81d2d7100b4288c84a9e", "text": "URL: https://blog.langchain.dev/langchain-and-scrimba-partner-to-help-web-devs-become-ai-engineers/\nTitle: LangChain and Scrimba Partner to help Web Devs become AI Engineers\n\nWe\u2019re excited to announce that we are partnering with Scrimba, a code-learning platform with over a million users. Through their interactive video format known as scrims, they enable developers to grow their skills in a more fun and immersive way than regular video courses.\n\nBy combing the LangChain JS library with interactive scrims, we will provide web developers with a fast-lane into the exciting world of AI engineering. The collaboration involves two parts:\n\nCreating a free LangChain JS course Improving LangChain\u2019s documentation with scrims\n\nLet\u2019s have a closer look at each of the initiatives.\n\nCreating a free LangChain JS course\n\nThis course aims to be the most interactive LangChain learning resource to date. As a student, you\u2019ll be constantly challenged to solve coding assignments, making sure that your build up the necessary muscle memory to become a proficient AI engineer. The course will be fully project-based, so you\u2019ll learn the concepts while creating a real-world application.\n\nLaunch is set to mid-October, but the a sneak peek of it has already been released, so be sure to check out the trailer here.\n\nSign up for the waitlist here if you\u2019d like to get sneak peeks as we produce the course, and exclusive early access as the launch approaches.\n\nEnriching LangChain\u2019s docs with scrims\n\nTo make it easier for developers to understand the LangChain docs, we are going to add scrims that explain the code examples in the docs in detail. This will also give developers an opportunity to get hands-on experience with LangChain from the get-go.\n\nThe first scrim have already been deployed in the Expression Language Cookbook. In it, Scrimba teacher Daniel Rose explains how to compose a simple chain using the Expression Language.\n\nA scrim embedded into the Expression Language Cookbook.\n\nThe docs also have a dedicated section for all the scrims that will be added. This will make it easier for you to find out if a certain topic has been explained as a scrim. We\u2019re going to gradually deploy more scrims to the docs over the next few weeks.\n\nThe creator of LangChain, Harrison Chase, explains why they wanted to partner with Scrimba:\n\nWe\u2019re excited to partner with Scrimba to help even more web developers to venture into AI Engineering. They make it really easy (and fun) to explore and improve as developers.\n\nAccording to Scrimba\u2019s CEO, Per Harald Borgen, AI Engineering represents a lucrative opportunity for web developers who are looking to get ahead in their careers:\n\n\u201cLearning to implement LLMs into products is probably the best thing a web developer can do to boost their career these days, as the industry has a huge need for competent AI engineers. And this will just continue to grow over the next few years.\u201d\n\nLinks:", "start_char_idx": 0, "end_char_idx": 2931, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ce87ded-fe42-44cf-a315-2559355c3dfb": {"__data__": {"id_": "5ce87ded-fe42-44cf-a315-2559355c3dfb", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-demogpt-new-era-for-gen-ai-applications_.txt", "file_name": "blog.langchain.dev_langchain-demogpt-new-era-for-gen-ai-applications_.txt", "file_type": "text/plain", "file_size": 10318, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "64bed31e-7c59-4b52-b7c9-3d8eb6171e69", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-demogpt-new-era-for-gen-ai-applications_.txt", "file_name": "blog.langchain.dev_langchain-demogpt-new-era-for-gen-ai-applications_.txt", "file_type": "text/plain", "file_size": 10318, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "02efc94b5df8c8f1fe9f63556885e2102c3a738fee0d6d96a19e7087231c4c48", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6049579a-0074-42a5-908f-ed134ceb3e0d", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-and-scrimba-partner-to-help-web-devs-become-ai-engineers_.txt", "file_name": "blog.langchain.dev_langchain-and-scrimba-partner-to-help-web-devs-become-ai-engineers_.txt", "file_type": "text/plain", "file_size": 2955, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "d6a1d27daa32a3b9ad78b15efca8c60981276cc87e8a81d2d7100b4288c84a9e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab4be978-4237-47a1-b9c9-a53015888493", "node_type": "1", "metadata": {}, "hash": "e3fccbf683c3efa426eac96c7e0d346a623edc6e65c31bc0324c13f53c49d3a5", "class_name": "RelatedNodeInfo"}}, "hash": "a5a67953a23b24f1687959603fd874f70aac8616c85efa4ef9de4d65e4c47400", "text": "URL: https://blog.langchain.dev/langchain-demogpt-new-era-for-gen-ai-applications/\nTitle: LangChain \ud83e\udd1d DemoGPT: New Era for Gen-AI Applications\n\nEditor's Note: This post was written in collaboration with the DemoGPT team. We're excited about what they're doing to make it easier to not only build LLM applications, but also get them in the hands of users and build community in the process. We also thought way they built the platform on top of LangChain and Streamlit is really neat\u2013their under-the-hood walkthrough offers some cool ideas for anyone using a language model to generate an app.\n\nToday we\u2019re happy to announce the collaboration of DemoGPT with LangChain to make generative ai application creation easier. In this blog post, we\u2019ll dig deeper into the details of this collaboration and how to use DemoGPT to build scalable LLM-powered applications with LangChain.\n\n\n\nDemoGPT: Emerging Marketplace for LangChain Applications\n\nDemoGPT is an open-source project that aspires to keep pushing the boundaries of Large Language Model (LLM) based application development. At its core, DemoGPT synergizes the capabilities of various Foundation Models, enabling the auto-generation of LangChain x Streamlit applications with just a prompt.\n\n\n\nHere\u2019s a look under the hood at how it works and where we see it going in the future.\n\nUnpacking DemoGPT: A Glimpse into its Technical Core\n\nDemoGPT Architecture\n\n\n\nNavigating through the architecture of DemoGPT reveals a structured approach to code generation operations. This detailed exploration will take you through its core stages: Planning, Task Creation, Code Snippet Generation, Combining the Code Snippets, and DB Saving. Each stage plays a pivotal role in ensuring optimal functionality and efficiency. Let's delve into each of these components to understand the intricacies of DemoGPT's workflow.\n\n\n\nPlanning: DemoGPT starts by generating a plan from the user's instruction.\n\nWhen a user submits an instruction, its first port of call is the planning module. This segment is the bedrock of the entire DemoGPT structure because the following steps lean heavily on the valid global planning inspired by HuggingGPT. However, unlike HuggingGPT, which goes straight from instruction to task list, DemoGPT first creates a plan in natural language and later creates a task list. This way of processing is more intuitive for LLMs.\n\nThe Planning module knows all of the available toolsets to minimize hallucinations. It also uses a self-refining strategy so that planning continues until it is validated by itself.\n\nTask Creation: It then creates specific tasks using the plan and instruction.\n\n\n\nIn our experiments, we have seen that using a natural language plan minimizes hallucinations vs. going straight from instruction to task list. Our novel approach reduces the number of refining steps needed in the task creation process. This step also has a self-refining subphase to get rid of hallucinated tasks. During this subphase, the module checks the (input, output) pairs of each task, then according to the result, it gives feedback to itself, then generates the tasks again according to the last iteration and continues until it passes the tests.\n\nCode Snippet Generation: Tasks are transformed into Python code snippets.\n\nEach task has its own prompt so that when the corresponding task is converted into a Python code, it uses its custom prompts for this transformation. The transformation process is mindful of previously generated code, so everything works well in tandem.\n\nCombining the Code Snippets: The code snippets are combined into a final code, resulting in an interactive application.\n\nAll code snippets are put into a prompt to combine them together. Here, the final code is made compatible with Streamlit (such as state management). The output of this module is further improved by a self-refining technique to make sure everything is compatible with Streamlit.\n\n\n\nDB Saving (coming in next release): The generated plan, tasks and code snippets stored in a vector database\n\nIn the whole architecture, each phase is applying self-refining to itself to get rid of hallucinated results. In addition, each module has its own examples for few-shot learning and for most applications. This works pretty well and allows applications to be created by lighter models like GPT-3.5 at 10% of the cost of GPT-4. However, to decrease the cost even more and make it more performant, the DB Saving module aims to save the approved results (plans, tasks, and code snippets) to a vector database so that next time, the relevant examples from the vector database will be fetched and used for the few-shot learning to decrease the number of refining steps. This will decrease the cost of application generation and at the same time make generation faster.\n\n\n\nHow to Install DemoGPT?\n\nInstalling DemoGPT is a straightforward process, designed to get you up and running with", "start_char_idx": 0, "end_char_idx": 4923, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab4be978-4237-47a1-b9c9-a53015888493": {"__data__": {"id_": "ab4be978-4237-47a1-b9c9-a53015888493", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-demogpt-new-era-for-gen-ai-applications_.txt", "file_name": "blog.langchain.dev_langchain-demogpt-new-era-for-gen-ai-applications_.txt", "file_type": "text/plain", "file_size": 10318, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "64bed31e-7c59-4b52-b7c9-3d8eb6171e69", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-demogpt-new-era-for-gen-ai-applications_.txt", "file_name": "blog.langchain.dev_langchain-demogpt-new-era-for-gen-ai-applications_.txt", "file_type": "text/plain", "file_size": 10318, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "02efc94b5df8c8f1fe9f63556885e2102c3a738fee0d6d96a19e7087231c4c48", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ce87ded-fe42-44cf-a315-2559355c3dfb", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-demogpt-new-era-for-gen-ai-applications_.txt", "file_name": "blog.langchain.dev_langchain-demogpt-new-era-for-gen-ai-applications_.txt", "file_type": "text/plain", "file_size": 10318, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "a5a67953a23b24f1687959603fd874f70aac8616c85efa4ef9de4d65e4c47400", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23dbfd41-80ec-47ea-9de7-a08155e311b8", "node_type": "1", "metadata": {}, "hash": "2f5128c9bd40445b1eff306d0b84af6066e8815b83bef1fa88802eac1d11443c", "class_name": "RelatedNodeInfo"}}, "hash": "e3fccbf683c3efa426eac96c7e0d346a623edc6e65c31bc0324c13f53c49d3a5", "text": "DemoGPT is a straightforward process, designed to get you up and running with minimal hassle.\n\npip install demogpt\n\nHow to Use DemoGPT?\n\nYou can use the DemoGPT library either via CLI or by using its Python interface.\n\n\n\nAs a Command Line Interface (CLI)\n\nYou can run the DemoGPT application as a Streamlit app by simply typing:\n\ndemogpt\n\nOnce running; enter your own API key and choose which base model you want to use.\n\nWhen everything is ready, you can start creating applications just from a prompt. Let your imagination guide you. You can create a chat with your PDF app in seconds, or create a sentiment analysis tool that takes in a website and returns the tone of text.\n\nApplications are limited only by prompts given, so with longer prompts you too can create sophisticated and unique AI applications.\n\n\n\nTweet Generator: An application that can generate tweets from given hashtags and tone of the tweet.\n\n\n\nWeb Blogger: An application that can generate Medium blog from given website url\n\n\n\n\n\nAs a Python Library\n\nYou can run the DemoGPT application as a Python library. To incorporate DemoGPT into your Python applications, follow the steps below.\n\nImport the necessary module:\n\nfrom demogpt import DemoGPT\n\nInstantiate the DemoGPT agent\n\nagent = DemoGPT(model_name=\"gpt-3.5-turbo-0613\", openai_api_key=\"YOUR_API_KEY\", max_steps=10)\n\n\n\n\n\nSet your instruction and title\n\ninstruction = \"Your instruction here\"\n\ntitle = \"Your title here\"\n\n\n\n\n\nIterate through the generation stages and extract the final code\n\ncode = \"\"\n\nfor phase in agent(instruction=instruction, title=title):\n\nprint(phase) # This will display the resulting JSON for each generation stage.\n\nif phase[\"done\"]:\n\ncode = phase[\"code\"] # Extract the final code.\n\nprint(code)\n\n\n\n\n\nFor further information, you can visit DemoGPT Docs\n\n\n\nFrom Idea to Marketplace: The Journey with LangChain x DemoGPT\n\nTo provide a clearer picture of this collaboration, let's walk through a potential user journey:\n\nImagine Sarah, an AI enthusiast with a brilliant idea for an application that leverages the power of language models. She visits the LangChain website, where she's introduced to the integrated DemoGPT application generation tool.\n\nAs a first step, app generation occurs on LangChain website\n\nWith a few prompts and inputs, Sarah crafts her application, watching it come to life in real-time. Once satisfied with her creation, Sarah is presented with the opportunity to showcase her application on the DemoGPT Marketplace. With a simple click, her application is listed, making it accessible to a global audience.\n\nOnce the app is generated, it will be listed on DemoGPT Marketplace\n\n\n\nOther developers, businesses, or AI enthusiasts can now discover Sarah's application, interact with it, provide feedback, or even propose collaborative enhancements.\n\n\n\nAll the generated apps will be listed and used on DemoGPT Marketplace\n\n\n\nFurthermore, the marketplace offers Sarah the chance to monetize her application, either through licensing or API sales. As her application gains traction, she receives feedback from the community, leading her back to the LangChain website to iterate and refine her application, ensuring it remains relevant and valuable to its users.\n\n\n\nThis cyclical process of creation, showcase, feedback, and refinement ensures that the LangChain x DemoGPT ecosystem remains vibrant, innovative, and user-centric.\n\n\n\n\n\nThe Power of Collaboration: LangChain x DemoGPT\n\nOne of the most exciting prospects of our collaboration is the emergence of the DemoGPT Marketplace. We envision the DemoGPT Marketplace as a platform where the LangChain community, alongside developers and AI enthusiasts globally, can create, showcase, exchange, and even monetize their auto-generated applications.\n\nThis marketplace will be more than just a platform; it will be a vibrant community and a space where LangChain users can collaborate, iterate, and refine applications, ensuring that our ecosystem remains dynamic, user-centric, and on the cutting edge of technological advancements. With the added interactivity and user experience enhancements brought by Streamlit, these applications will promise to be not just functional but truly transformative.\n\n\n\n\n\nWhat\u2019s Next?\n\nAs users craft their unique applications on LangChain, we hope and envision that these innovative creations will find a new home on the DemoGPT Marketplace. This platform will be set to become a bustling hub where these auto-generated applications are prominently listed and showcased. It will not just be about giving visibility to the applications but also creating a space where a broader audience can discover, interact with, and derive value from these tools.\n\n\n\nThe vision behind this collaboration is to establish a synergistic ecosystem. By enabling application generation on LangChain and providing a platform for discovery on the DemoGPT Marketplace, we aim to bridge the gap between creators and consumers.\n\n\n\nCollaboration between LangChain and DemoGPT can be a really huge step for the LLM world!\n\n\n\nWe encourage our community to", "start_char_idx": 4846, "end_char_idx": 9926, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23dbfd41-80ec-47ea-9de7-a08155e311b8": {"__data__": {"id_": "23dbfd41-80ec-47ea-9de7-a08155e311b8", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-demogpt-new-era-for-gen-ai-applications_.txt", "file_name": "blog.langchain.dev_langchain-demogpt-new-era-for-gen-ai-applications_.txt", "file_type": "text/plain", "file_size": 10318, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "64bed31e-7c59-4b52-b7c9-3d8eb6171e69", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-demogpt-new-era-for-gen-ai-applications_.txt", "file_name": "blog.langchain.dev_langchain-demogpt-new-era-for-gen-ai-applications_.txt", "file_type": "text/plain", "file_size": 10318, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "02efc94b5df8c8f1fe9f63556885e2102c3a738fee0d6d96a19e7087231c4c48", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab4be978-4237-47a1-b9c9-a53015888493", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-demogpt-new-era-for-gen-ai-applications_.txt", "file_name": "blog.langchain.dev_langchain-demogpt-new-era-for-gen-ai-applications_.txt", "file_type": "text/plain", "file_size": 10318, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "e3fccbf683c3efa426eac96c7e0d346a623edc6e65c31bc0324c13f53c49d3a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0e4086e-400f-4e65-8532-c7fd7353cd8f", "node_type": "1", "metadata": {}, "hash": "56610c36aa1c213217538e3716eef1c16fb28fde02fd946e71155f1df1e24d67", "class_name": "RelatedNodeInfo"}}, "hash": "2f5128c9bd40445b1eff306d0b84af6066e8815b83bef1fa88802eac1d11443c", "text": "DemoGPT can be a really huge step for the LLM world!\n\n\n\nWe encourage our community to share their feedback, insights, and experiences on LangChain Discord channel. Your input is invaluable to us, and it will play a pivotal role in shaping the future of this collaboration.\n\n\n\nFor more detailed information, advanced configurations, or troubleshooting, you can always refer to the DemoGPT GitHub repository or DemoGPT Marketplace and consider giving a star.", "start_char_idx": 9841, "end_char_idx": 10297, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0e4086e-400f-4e65-8532-c7fd7353cd8f": {"__data__": {"id_": "b0e4086e-400f-4e65-8532-c7fd7353cd8f", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-docugami-webinar-lessons-from-deploying-llms-with-langsmith_.txt", "file_name": "blog.langchain.dev_langchain-docugami-webinar-lessons-from-deploying-llms-with-langsmith_.txt", "file_type": "text/plain", "file_size": 4262, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b30c2db-168b-4d8b-b7ea-82a8f4f27eca", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-docugami-webinar-lessons-from-deploying-llms-with-langsmith_.txt", "file_name": "blog.langchain.dev_langchain-docugami-webinar-lessons-from-deploying-llms-with-langsmith_.txt", "file_type": "text/plain", "file_size": 4262, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "d30037dc8e1cbc0481435627ba49b39e0475e6ab6c22e5c0c3b961f73d68a66c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23dbfd41-80ec-47ea-9de7-a08155e311b8", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-demogpt-new-era-for-gen-ai-applications_.txt", "file_name": "blog.langchain.dev_langchain-demogpt-new-era-for-gen-ai-applications_.txt", "file_type": "text/plain", "file_size": 10318, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "2f5128c9bd40445b1eff306d0b84af6066e8815b83bef1fa88802eac1d11443c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c48ccc50-dfc0-4e9e-b942-debce010f5a3", "node_type": "1", "metadata": {}, "hash": "e88fa52a49b60892f0aebbfaa0608b4cecb14669bd73d34c1ead27b8651aa17f", "class_name": "RelatedNodeInfo"}}, "hash": "56610c36aa1c213217538e3716eef1c16fb28fde02fd946e71155f1df1e24d67", "text": "URL: https://blog.langchain.dev/langchain-docugami-webinar-lessons-from-deploying-llms-with-langsmith/\nTitle: LangChain + Docugami Webinar: Lessons from Deploying LLMs with LangSmith\n\nEditor's Note: This post was written in collaboration with the Docugami team. We recently did a webinar with them and Rechat to talk about what it actually requires to get an LLM application into production. You can find the recording of the webinar here\u2013and this post provides a helpful overview of what they discussed and dives even deeper on their learnings.\n\nAt Docugami we have been using, training or fine-tuning language models for multiple years in our mission to transform documents to data. We initially started using smaller models for text completion and OCR correction, as well as pretraining for sequence labeling tasks. As these models have exploded in size and complexity, we have continued to invest in this space with question answering and Retrieval Augmented Generation (RAG) using our unique approach with the Document XML Knowledge Graph.\n\nWe chose from the beginning to host the language models in our cloud to ensure customer data confidentiality.\n\nWe started using LangChain very early, impressed with the expressive API and vibrant community. The LangChain Docugami Loader was added in May, and we continue to be amazed by the responsiveness of the LangChain team as they incorporate community feedback and continue to grow the LangChain framework.\n\nYesterday, we were super happy to share our learnings with the community in an educational webinar hosted by LangChain. Our goal was to share some of the real-world challenges we have encountered with LLMs in production and how we are using LangChain, and especially the new LangSmith (beta) tool, in our LLMOps flow.\n\nIf you missed the webinar, no problem! Here is a summary of the key points we covered:\n\n1. Real documents are more than flat text: We described in detail how Docugami structurally chunks documents (Scanned PDF, Digital PDF, DOCX, DOC) and stitches together the complex reading orders including tables and multi-column flows. We discussed how humans create documents to be readable by other humans, including visual and structural cues that contain semantic meaning which is often missed by text-only systems.\n\n2. Documents are Knowledge Graphs: We briefly showed some examples of the hierarchical Document XML Knowledge Graph produced by Docugami. It contains deep hierarchies, custom semantic labels on nodes and all the complex relationships that can be expressed semantically using the XML Data Model. We showed through code how RAG using Docugami\u2019s XML Knowledge Graph leads to more accurate results that cannot be achieved with simple linear chunking.\n\n3. Building Complex Chains with the LangChain Expression Language: Real-world chains can get complicated with parallel branches, output parsers, few shot examples, conditional sub-chains, and more. We walked through a quick example with SQL generation, with agent-like fixup for invalid SQL. We discussed how you can step through these complex chains in the LangSmith tool, and shared some example traces.\n\n4. Debugging Complex Chain Failures in Production: Things go wrong for various reasons when LLMs are deployed in production. It could be something as simple as a context length overflow, or something more subtle like an output parser throwing exceptions in some edge cases. We shared some tips to make your run traces in LangSmith more debuggable.\n\n5. Docugami\u2019s end to end LLM Ops with LangChain + LangSmith: Finally, we summarized our overall flow to deploy models, monitor them under real customer use, identify problematic runs, and then fix up these runs (manually as well as with help from other LLMs offline). This is a nascent area, where we are excited to work with LangSmith to improve the tooling given our previous experience running similar model ops for other (non-LLM) machine learning models.\n\nThe slides (including links to code samples and LangSmith traces) are here.\n\n\n\nYou can also watch the webinar here.\n\nWe are excited to see what you build with LangChain and Docugami. Tag us @docugami on twitter to share your results and experience, or just reach out https://www.docugami.com/contact-us", "start_char_idx": 0, "end_char_idx": 4256, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c48ccc50-dfc0-4e9e-b942-debce010f5a3": {"__data__": {"id_": "c48ccc50-dfc0-4e9e-b942-debce010f5a3", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-expression-language_.txt", "file_name": "blog.langchain.dev_langchain-expression-language_.txt", "file_type": "text/plain", "file_size": 6988, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d013c5ef-a363-4978-8dbe-b07fe972bf3f", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-expression-language_.txt", "file_name": "blog.langchain.dev_langchain-expression-language_.txt", "file_type": "text/plain", "file_size": 6988, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "78247e9ca2174cd9893e00acfa08b2bbc266a12ed8e7233b37383c00844e9133", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0e4086e-400f-4e65-8532-c7fd7353cd8f", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-docugami-webinar-lessons-from-deploying-llms-with-langsmith_.txt", "file_name": "blog.langchain.dev_langchain-docugami-webinar-lessons-from-deploying-llms-with-langsmith_.txt", "file_type": "text/plain", "file_size": 4262, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "56610c36aa1c213217538e3716eef1c16fb28fde02fd946e71155f1df1e24d67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ccb20e4d-511e-4573-9e0c-f87f3c45c42d", "node_type": "1", "metadata": {}, "hash": "a80c6f38e4cba3e8ac6d48883d1aecfc515b0e8478c4f372725b932d73fb48db", "class_name": "RelatedNodeInfo"}}, "hash": "e88fa52a49b60892f0aebbfaa0608b4cecb14669bd73d34c1ead27b8651aa17f", "text": "URL: https://blog.langchain.dev/langchain-expression-language/\nTitle: LangChain Expression Language\n\nTL;DR:\n\nWe\u2019re excited to announce a new syntax to create chains with composition. This comes along with a new interface that supports batch, async, and streaming out of the box. We\u2019re calling this syntax LangChain Expression Language (LCEL)\n\nWe've created a \"LangChain Teacher\" to help teach you LCEL (assumes LangChain familiarity)\n\nWe'll be doing a webinar on 8/2 about this and how to use it\n\nThis is aimed at making it easier to construct complex chains, and pairs nicely with LangSmith - the platform we recently released aimed at making it easier to go from prototype to production.\n\nThe idea of chaining has proven popular when building applications with language models. Chaining can come in a few different forms, each with their own benefits. Some examples of these are:\n\nMaking Multiple LLM Calls\n\nChaining can mean making multiple LLM calls in a sequence. Language models are often non deterministic and can make errors, so making multiple calls to check previous outputs or to break down larger tasks into bite-sized steps can improve results.\n\nConstructing the Input to LLMs\n\nChaining can mean combining data transformation with a call to an LLM. For example, formatting a prompt template with user input or using retrieval to look up additional information to insert into the prompt template. This is necessary because you often need data from multiple sources to perform a task, which may be fetched at runtime conditional on the input.\n\nUsing the Output of LLMs\n\nAnother form of chaining refers to passing the output of an LLM call to a downstream application. For example, using the LLM to generate Python code and then running that code; using the LLM to generate SQL and then executing that against a SQL database.\n\nThere\u2019s also something about working with language models that makes the idea of chaining appealing. Sure, all the above operations could be done with code, but people have gravitated towards the idea of chaining - as evidenced by the multitude of low-code/no-code platforms for building language model applications (some like Flowwise and LangFlow built on top of LangChain). Why? It\u2019s become a bit of a meme, but if text is the universal interface, and all of these operations involve manipulation of text, then this sets itself up incredibly naturally for an expression language to support this.\n\nLangChain was born from the idea of making these types of operations easy. We saw people doing common patterns and factored them out into pre-built chains: LLMChain, ConversationalRetrievalChain, SQLQueryChain.\n\nBut these chains weren\u2019t really composable. Sure - we had SequentialChain, but that wasn\u2019t amazingly usable. And under the hood the other chains involved a lot of custom code, which made it tough to enforce a common interface for all chains, and ensure that all had equal levels of batch, streaming, and async support.\n\nToday we\u2019re excited to announce a new way of constructing chains. We\u2019re calling this the LangChain Expression Language (in the same spirit as SQLAlchemyExpressionLanguage). This is a declarative way to truly compose chains - and get streaming, batch, and async support out of the box. You can use all the same existing LangChain constructs to create them.\n\nWe\u2019ve included guides on how to work with the interface as well as some examples of using it. Let\u2019s take a look at one of the more common ways below:\n\nfrom langchain.chat_models import ChatOpenAI from langchain.prompts import ChatPromptTemplate model = ChatOpenAI() prompt = ChatPromptTemplate.from_template(\"tell me a joke about {foo}\") chain = prompt | model chain.invoke({\"foo\": \"bears\"}) >>> AIMessage(content=\"Why don't bears ever wear shoes?\n\n\n\nBecause they have bear feet!\", additional_kwargs={}, example=False)\n\nThis uses a standard ChatOpenAI model and prompt template. You chain them together with the | operator, and then call it with chain.invoke . We can also get async, batch, and streaming support out of the box.\n\nBatch\n\nbatch takes in a list of inputs. If optimizations can be done internally (like literally batching calls to LLM providers) those are done.\n\nchain.batch([{\"foo\": \"bears\"}, {\"foo\": \"cats\"}]) >>> [AIMessage(content=\"Why don't bears ever wear shoes?\n\n\n\nBecause they have bear feet!\", additional_kwargs={}, example=False), AIMessage(content=\"Why don't cats play poker in the wild?\n\n\n\nToo many cheetahs!\", additional_kwargs={}, example=False)]\n\nStream\n\nstream returns an iterable that you can consume.\n\nfor s in chain.stream({\"foo\": \"bears\"}): print(s.content, end=\"\")\n\nAsync\n\nAll of invoke , batch , and stream expose async methods. We only show ainvoke here for simplicity, although you can check out our notebook that deep dives into the interface to see more.\n\nawait", "start_char_idx": 0, "end_char_idx": 4826, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ccb20e4d-511e-4573-9e0c-f87f3c45c42d": {"__data__": {"id_": "ccb20e4d-511e-4573-9e0c-f87f3c45c42d", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-expression-language_.txt", "file_name": "blog.langchain.dev_langchain-expression-language_.txt", "file_type": "text/plain", "file_size": 6988, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d013c5ef-a363-4978-8dbe-b07fe972bf3f", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-expression-language_.txt", "file_name": "blog.langchain.dev_langchain-expression-language_.txt", "file_type": "text/plain", "file_size": 6988, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "78247e9ca2174cd9893e00acfa08b2bbc266a12ed8e7233b37383c00844e9133", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c48ccc50-dfc0-4e9e-b942-debce010f5a3", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-expression-language_.txt", "file_name": "blog.langchain.dev_langchain-expression-language_.txt", "file_type": "text/plain", "file_size": 6988, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "e88fa52a49b60892f0aebbfaa0608b4cecb14669bd73d34c1ead27b8651aa17f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "822d6083-6ad0-4204-a82d-f62393a59aa1", "node_type": "1", "metadata": {}, "hash": "7de4b34caedcbb0239b1af6dce2be34ba79aa84f6c453ee6e20799eae2f523c3", "class_name": "RelatedNodeInfo"}}, "hash": "a80c6f38e4cba3e8ac6d48883d1aecfc515b0e8478c4f372725b932d73fb48db", "text": "simplicity, although you can check out our notebook that deep dives into the interface to see more.\n\nawait chain.ainvoke({\"foo\": \"bears\"})\n\nIn our cookbook we\u2019ve included examples of doing this with:\n\nWe\u2019ll be constantly beefing up support for this and adding more examples of functionality, so let us know what you\u2019d like to see. We'll also be incorporating this more into LangChain - already the create_sql_query_chain uses this under the hood.\n\nBesides the benefit of adding standard interfaces, another benefit is that this will make it easier for users to customize parts of the chain. Since the chain is expressed in such a declarative and composable nature, it will be much more clear how to swap certain components out. It also now brings the prompts front and center - making it more clear how to modify those. The prompts in LangChain are just defaults, and are largely intended to be modified for your particular use case if you are seriously trying to take an application into production. Previously, the prompts were a bit hidden and hard to change. With LCEL, they are more prominent and easily swappable.\n\nLangChain Expression Language creates chains that integrate seamlessly with LangSmith. Here is a trace for the above:\n\nYou can inspect the trace here. Previously, when creating a custom chain there was actually a good bit of work to be done to make sure callbacks were passed through correctly so that it could be traced correctly. With LangChain Expression Language that happens automatically.\n\nWe've also tried to make this as easy as possible for people to learn by creating a \"LangChain Teacher\" application that will walk you through the basics of getting started with LangChain Expression Language. You can access it here. We'll be open sourcing this soon.\n\nWe'll also be doing a webinar on this tomorrow. We'll cover the standard interface it exposes, how to use it, and why to use it. Register for that here.\n\nWe're incredibly excited about this being an easy and lightweight way to truly compose chains together. If you're excited as well, we're hiring for roles that would work directly on this. The best way to get our attention is to open a PR or two adding more functionality. There's still a lot to build :)", "start_char_idx": 4720, "end_char_idx": 6962, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "822d6083-6ad0-4204-a82d-f62393a59aa1": {"__data__": {"id_": "822d6083-6ad0-4204-a82d-f62393a59aa1", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-prompt-hub_.txt", "file_name": "blog.langchain.dev_langchain-prompt-hub_.txt", "file_type": "text/plain", "file_size": 9266, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "768dec39-fe48-4c1d-a18a-e7f054e32739", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-prompt-hub_.txt", "file_name": "blog.langchain.dev_langchain-prompt-hub_.txt", "file_type": "text/plain", "file_size": 9266, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "5d546cbf3a1beba0824ee3915e787ca4a581a84b28dc7979f5eb6e070c7c1827", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ccb20e4d-511e-4573-9e0c-f87f3c45c42d", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-expression-language_.txt", "file_name": "blog.langchain.dev_langchain-expression-language_.txt", "file_type": "text/plain", "file_size": 6988, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "a80c6f38e4cba3e8ac6d48883d1aecfc515b0e8478c4f372725b932d73fb48db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc748c2e-0c29-4b64-b923-0f81adad1429", "node_type": "1", "metadata": {}, "hash": "0765e6d70f806a4ea652cee24ad666dc8545b4a7233e5290a37f895c5bb4e9b7", "class_name": "RelatedNodeInfo"}}, "hash": "7de4b34caedcbb0239b1af6dce2be34ba79aa84f6c453ee6e20799eae2f523c3", "text": "URL: https://blog.langchain.dev/langchain-prompt-hub/\nTitle: Announcing LangChain Hub\n\nToday, we're excited to launch LangChain Hub\u2013a home for uploading, browsing, pulling, and managing your prompts. (Soon, we'll be adding other artifacts like chains and agents).\n\nLangChain Hub is built into LangSmith (more on that below) so there are 2 ways to start exploring LangChain Hub.\n\nWith LangSmith access: Full read and write permissions. You can explore all existing prompts and upload your own by logging in and navigate to the Hub from your admin panel.\n\nFull read and write permissions. You can explore all existing prompts and upload your own by logging in and navigate to the Hub from your admin panel. Without LangSmith access: Read only permissions. You can view and download and run prompts. Head directly to https://smith.langchain.com/hub to start exploring.\n\nIf you would like to upload a prompt but don't have access to LangSmith fill out this form and we will expedite your access so you can start publishing your prompts.\n\nMotivation for LangChain Hub\n\nWe launched a very early version of LangChain Hub at the beginning of the year as a directory of code and README's with the same goal we have today\u2013make it easier to share and discover prompts for any use-case.\n\nAs LangChain and the broader ecosystem has evolved, the role of prompting has only become more important to the LLM development process. As Ethan Mollick recently wrote in a (FANTASTIC) article on the topic, \"now is the time for grimoires.\" By \"grimoires\" he means \"prompt libraries that encode the expertise of their best practices into forms that anyone can use.\"\n\nWe whole-heartedly agree\u2013the value of a Hub extends beyond individual applications. It's about advancing our collective wisdom and translating that into knowledge we can all put to use now. We want to help make this easier on an individual, team, and organization scale, across any use-case and every industry.\n\nOur goal for LangChain Hub is that it becomes the go-to place for developers to discover new use cases and polished prompts.\n\nToday, polished prompts and the wisdom that comes with it are distributed across the web and all-too-often buried in the crannies of blog posts, Twitter threads, and people's head's. By bringing all tis knowledge together in one easily-navigable place, we think we can accelerate the pace of development and learning together.\n\nTo use Mollick's terminology\u2013we're starting with public grimoires today, but we'll be enabling private, company-specific grimoires very soon.\n\nSo why now? A few new insights emerged over the past months that motivated us to rebuild the hub properly.\n\nModel Variety and Non-Transferable Prompts\n\nPeople aren't just using OpenAI anymore. Anthropic with claude-2 has become the go-to choice for people needing long context windows. Google is releasing (and will release) more powerful models. And, most excitingly, the open source model community is catching up and Llama2 proving to be a viable alternative.\n\nUnfortunately, prompts don't simply transfer from one model to another. Each model may have different tricks that work best for that model (e.g. claude-2 prefers XML encoding when prompting) or different syntax (e.g. SYS and INST for Llama2).\n\nAs developers explore the wide variety of models, we hope the LangChain Hub can assist in that exploration by providing starter prompts for those models. We've added tags to prompts to indicate which model(s) they work best with.\n\n2. Inspectability\n\nPrompts power the chains and agents in LangChain. Often times, the prompts are obfuscated away. We built LangChain Hub in a way that puts them front and center, so that anyone can see what's going on under the hood.\n\n3. Cross-Team Collaboration\n\nWhile most LLM applications require substantial engineering work to set up, we've noticed that non-technical team members are participating in the process of editing and refining prompts. We wanted to make it much easier for more team members to get involved in what we believe is going to become a core part of every company's app development process. Along these lines, we don't believe that prompts should be treated as traditional code\u2013it's simply not the best way to facilitate this kind of collaboration.\n\nWe're aiming to make LangChain Hub the best place for teams to write and manage prompts, together. The product isn't quite there today\u2013this first iteration only supports personal accounts\u2013but we're actively looking for organizations that are excited to explore an Alpha with us so if you want organizational support for the Hub, please reach out to us directly at support@langchain.dev with the subject [Hub: Orgs]\n\n4. Artifact Management and LangSmith\n\nFrom partnering with early LangSmith users, the tie-in between debugging, logging,", "start_char_idx": 0, "end_char_idx": 4804, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc748c2e-0c29-4b64-b923-0f81adad1429": {"__data__": {"id_": "cc748c2e-0c29-4b64-b923-0f81adad1429", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-prompt-hub_.txt", "file_name": "blog.langchain.dev_langchain-prompt-hub_.txt", "file_type": "text/plain", "file_size": 9266, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "768dec39-fe48-4c1d-a18a-e7f054e32739", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-prompt-hub_.txt", "file_name": "blog.langchain.dev_langchain-prompt-hub_.txt", "file_type": "text/plain", "file_size": 9266, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "5d546cbf3a1beba0824ee3915e787ca4a581a84b28dc7979f5eb6e070c7c1827", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "822d6083-6ad0-4204-a82d-f62393a59aa1", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-prompt-hub_.txt", "file_name": "blog.langchain.dev_langchain-prompt-hub_.txt", "file_type": "text/plain", "file_size": 9266, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "7de4b34caedcbb0239b1af6dce2be34ba79aa84f6c453ee6e20799eae2f523c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d279576f-f8a2-4c88-a2c4-5cc635028747", "node_type": "1", "metadata": {}, "hash": "22510de4ac256c9a096a2e4feef3b78fe28927f97f048fa24c555b9e9d228e36", "class_name": "RelatedNodeInfo"}}, "hash": "0765e6d70f806a4ea652cee24ad666dc8545b4a7233e5290a37f895c5bb4e9b7", "text": "and LangSmith\n\nFrom partnering with early LangSmith users, the tie-in between debugging, logging, testing, and evaluation and artifact management has become increasingly obvious. By making LangChain Hub a part of LangSmith, we knew we could help teams not only identify and collaborate on prompts, but also make informed decisions about how to implement them. Testing integrations with prompts aren't out yet but they are coming soon!\n\nFavorite Features\n\nHome Page\n\nWe want to make discoverability and navigability as easy as possible. You should be able to go from curiosity to coding in just a few clicks.\n\nYou can view sort prompts by:\n\nMost favorites\n\nMost viewed\n\nMost downloaded\n\nRecently uploaded\n\nYou can filter prompts by:\n\nUse cases (chatbots, extraction, summarization, etc)\n\nType (prompt template, etc)\n\nLanguage (English, Chinese, etc)\n\nModel (OpenAI, Anthropic, Llama2, VertexAI, etc)\n\nDownloading and Uploading Prompts\n\nWe have released an SDK to enable easy programatic downloading of prompts:\n\nfrom langchain import hub prompt = hub.pull(\"hwchase17/eli5-solar-system\")\n\nYou can also easily upload prompts via the SDK\n\nfrom langchain import hub from langchain.prompts.chat import ChatPromptTemplate prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") hub.push(\"<handle>/topic-joke-generator\", prompt)\n\nIf you want to upload an prompt to the Hub, but don't yet have access to LangSmith, fill out this form and we will expedite your access.\n\nPrompt Versioning\n\nEach time you commit a prompt, it is added as a new commit. This means that you can easily access previous versions of prompts should you want to go back to a previous version.\n\nPlayground\n\nAll prompts can be opened in the playground by clicking the \"Try it\" button. This allows you to interact with prompts right from LangChain Hub. It's useful for testing prompts...and it's fun!\n\nNote: You will be required to enter an OpenAI or Anthropic API key in order to run it in the playground. These keys are only stored in your browser are used solely to communicate directly to services.\n\nEditing and Saving\n\nFrom the playground you can edit a prompt, and then save it by clicking the \"Commit\" button in the top right corner. You can do this either for your own prompts, or for others (when saving, you will have to create your own repo to save it to). This is exciting because it helps everyone build on top of each other's work!\n\nComing Soon\n\nMore Artifact Types: Right now, only prompt templates are supported. We plan to expand support for other types of artifacts like chains and agents.\n\nRight now, only prompt templates are supported. We plan to expand support for other types of artifacts like chains and agents. Organization Support: Right now the Hub only works for your personal account. If your organization needs the ability to collaborate on prompts, for now please reach out to us directly at support@langchain.dev with the subject [Hub: Orgs] . We will be rolling this out more widely in a few weeks.\n\nRight now the Hub only works for your personal account. If your organization needs the ability to collaborate on prompts, for now please reach out to us directly at with the subject . We will be rolling this out more widely in a few weeks. Integration with testing: Just as you test code, you should test prompts. We are working on integrating the Hub with our dataset & testing functionality. If you need to test your prompts in the meantime, please check out our LangSmith cookbooks.\n\nJust as you test code, you should test prompts. We are working on integrating the Hub with our dataset & testing functionality. If you need to test your prompts in the meantime, please check out our LangSmith cookbooks. More social features: Just as you test code, you should test prompts. We are working on integrating the Hub with our dataset and testing functionality. If you need to test your prompts in the meantime, please check out our LangSmith cookbooks.\n\nJust as you test code, you should test prompts. We are working on integrating the Hub with our dataset and testing functionality. If you need to test your prompts in the meantime, please check out our LangSmith cookbooks. What else? If you have product feedback or ideas for us, we want to hear it! Join us in Discord to share more.\n\nShow us your prompts!\n\nWe\u2019ll be rounding up and sharing the most creative, useful, thought-provoking prompts with the community.\n\nSo share your prompts, \u2764\ufe0f your favorites, and tag us when you post your prompts or stumble across ones you like!", "start_char_idx": 4707, "end_char_idx": 9246, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d279576f-f8a2-4c88-a2c4-5cc635028747": {"__data__": {"id_": "d279576f-f8a2-4c88-a2c4-5cc635028747", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_lepton-x-langchain-earning-sage_.txt", "file_name": "blog.langchain.dev_lepton-x-langchain-earning-sage_.txt", "file_type": "text/plain", "file_size": 6197, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2473d46c-0ccc-4fc6-ae14-ea0a15e51b6b", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_lepton-x-langchain-earning-sage_.txt", "file_name": "blog.langchain.dev_lepton-x-langchain-earning-sage_.txt", "file_type": "text/plain", "file_size": 6197, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "afa488ad6e97574f87bb67b3802b01315e1ebd5e4cbf87cd328ef7c17abfd3ea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc748c2e-0c29-4b64-b923-0f81adad1429", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-prompt-hub_.txt", "file_name": "blog.langchain.dev_langchain-prompt-hub_.txt", "file_type": "text/plain", "file_size": 9266, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "0765e6d70f806a4ea652cee24ad666dc8545b4a7233e5290a37f895c5bb4e9b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fab507da-6367-4727-906c-c0f3c6588536", "node_type": "1", "metadata": {}, "hash": "01ebbe52058eb32a9a63155dc9776d2bed4fd399fcd27472f453182265e281b0", "class_name": "RelatedNodeInfo"}}, "hash": "22510de4ac256c9a096a2e4feef3b78fe28927f97f048fa24c555b9e9d228e36", "text": "URL: https://blog.langchain.dev/lepton-x-langchain-earning-sage/\nTitle: Lepton x LangChain: Earning Sage, How to Transform AI into a Savvy CFO\n\nEditor\u2019s Note: This blog post was written in collaboration LeptonAI Team, an early LangSmith BETA user. Lots of folks are talking about how best to finetune an open-source model for their specific use case, and LeptonAI has actually done that. We're excited to share their journey and hope it can inform others.\n\nIntroduction\n\nHave you ever thought about joining an earning call and asking questions to these CFOs? That used to be the privilege held by the investors from high-end investment banks such as JP Morgan, Goldman Sachs and Morgan Stanley.\n\nYet with the capability of LLM and proper techniques around it, not anymore. And if you don\u2019t feel like reading the whole post, feel free to try out a demo here. This demo is created based on the Apple Q2 2023 earning call.\n\nStep into the realm where cutting-edge technology meets financial acumen, let\u2019s dive deep into the transformative process of harnessing the capability of AI, and unveil the secrets to crafting an AI that speaks like a seasoned Chief Financial Officer (CFO), as demonstrated below.\n\nProblem Statement\n\nTo begin with, I would like to breakdown the challenge mentioned above into an abstraction, which hopefully can help you understand at an engineering level on what problem we are facing. In a nutshell, the problem looks like this:\n\nThe problem we are facing here is to organically combine the original earning call transcript, the text generation model ( mostly could be OpenAI ChatGPT 3.5) and the toolset(python, langchain, chroma, nothing fancy here) to mimic a CFO.\n\nThought Through Process for solutions\n\nStarting with openAI\n\nTo begin with, I started with using ChatGPT 3.5 from open AI with Langchain retrievalQA chain, which is a pretty standard approach for anyone building out an application like this. With that being said, the solution now looks like this:\n\nNot surprisingly, ChatGPT 3.5 works quite well for questions simply enough, eg. What's covered in this earning call? . The open source tools works like a charm in terms of prototyping. It doesn\u2019t take long to build up the first version of the product. Yet for questions bit more complicated, ChatGPT gives up very quick. You may check it out here .\n\nThe full questions list with response from ChatGPT 3.5 is here\n\nThen I tried vanilla vicuna\n\nOverheard from friends, and as an Open Source Developer ( worked on Jupyter Lab, yes, the notebook, but beyond notebook! \ud83d\ude05), I decided to try out Vicuna, an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. With that being said, the solution now looks like this:\n\nThe tricky part here is the first version of the product is built upon Langchain which is initially built upon OpenAI\u2019s API. So as a lot of other prompt engineering frameworks. In this case, switching to another model is a lot of work in terms of compatibility issues. Eg. the other model may doesn\u2019t have the same embedding api endpoint. or the tiktoken lib doesn\u2019t support certain models.\n\nDue to this problem, engineering team at Lepton.AI found a way to make the model compatible with the original OpenAI\u2019s API endpoint, makes switching models for a LLM application much easier. The model service enable users to switch the model by simply altering the environment from\n\nOPENAI_API_BASE=https://api.openai.com/v1 OPENAI_API_KEY=YOUR_OPEN_API_KEY\n\nto\n\nOPENAI_API_BASE=YOUR_DEPLOYMENT_URL OPENAI_API_KEY=YOUR_LEPTON_AI_API_KEY\n\nThe result turns to be pretty solid at first glance, yet evaluation on the outputs is quite challenging. This is where LangSmith comes in handy. It allows me to add four lines of code to alter the environment variables, and it could handle everything for me from there.\n\nTurns out the Fine Tuned model is even better\n\nEven though the vanilla model works by not giving up so fast, it still doesn\u2019t really talk quite like a CFO. That is saying the way it talks does not give me the feeling of actually attending an earning call surrounded by talents from top financial institutions.\n\nHence inspired by Vicuna, the fine-tuned model of llama, I decided to fine-tune a model that utilize data from the earning call question & answer section. By collecting data from the earning transcripts, I managed to sample out quite a few earning calls. Then using TUNA, a model augmentation service that augment both the data and model, to create a model that\u2019s more focused on earning call context. With that being said, the solution now looks", "start_char_idx": 0, "end_char_idx": 4610, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fab507da-6367-4727-906c-c0f3c6588536": {"__data__": {"id_": "fab507da-6367-4727-906c-c0f3c6588536", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_lepton-x-langchain-earning-sage_.txt", "file_name": "blog.langchain.dev_lepton-x-langchain-earning-sage_.txt", "file_type": "text/plain", "file_size": 6197, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2473d46c-0ccc-4fc6-ae14-ea0a15e51b6b", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_lepton-x-langchain-earning-sage_.txt", "file_name": "blog.langchain.dev_lepton-x-langchain-earning-sage_.txt", "file_type": "text/plain", "file_size": 6197, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "afa488ad6e97574f87bb67b3802b01315e1ebd5e4cbf87cd328ef7c17abfd3ea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d279576f-f8a2-4c88-a2c4-5cc635028747", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_lepton-x-langchain-earning-sage_.txt", "file_name": "blog.langchain.dev_lepton-x-langchain-earning-sage_.txt", "file_type": "text/plain", "file_size": 6197, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "22510de4ac256c9a096a2e4feef3b78fe28927f97f048fa24c555b9e9d228e36", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60466d64-81fb-46a4-a9f9-105ee1f4c9da", "node_type": "1", "metadata": {}, "hash": "fe91018d3e49697abbbac67b7c5e624b4919e58636fd059418514acb1f17f5e0", "class_name": "RelatedNodeInfo"}}, "hash": "01ebbe52058eb32a9a63155dc9776d2bed4fd399fcd27472f453182265e281b0", "text": "a model that\u2019s more focused on earning call context. With that being said, the solution now looks like this:\n\nHere are few query result from the question list\n\nAgain, the only thing changed in my code is the OPENAI_API_BASE and everything works from there. By leveraging LangSmith, I get to compare the result more efficiently and share them to people who are interested in looking at it as demonstrated in this post multiple times.\n\nConclusion\n\nIn conclusion, the integration of data and LLM techniques, such as data augmentation and fine-tuning, stands as a pivotal milestone in the development of AI applications. By combining vast and diverse datasets with the power of LLM, we unlock unprecedented potential, enabling AI systems to generate more accurate, context-aware, and coherent outputs. The synergy between data and LLM not only enhances the overall performance of AI applications but also opens up new avenues for innovation and discovery.\n\nAs we continue to refine and expand our understanding of this dynamic relationship, we embark upon a journey where the fusion of data-driven insights and advanced language models redefines what is possible, propelling us into an era of AI excellence and transforming the way we interact with technology. The future awaits, as we stride confidently toward a horizon where AI transcends expectations and becomes an indispensable asset in our quest for progress.\n\nAnd for the tools mentioned above, both LangSmith and LeptonAI are still under closed beta, but feel free to sign up on the waitlist and give it a try. Feel free to shoot me an email at uz@lepton.ai, I would love to hear from you on your thoughts!", "start_char_idx": 4513, "end_char_idx": 6174, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60466d64-81fb-46a4-a9f9-105ee1f4c9da": {"__data__": {"id_": "60466d64-81fb-46a4-a9f9-105ee1f4c9da", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_llms-and-sql_.txt", "file_name": "blog.langchain.dev_llms-and-sql_.txt", "file_type": "text/plain", "file_size": 13010, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65409667-83da-4b4c-8725-3c8393996070", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_llms-and-sql_.txt", "file_name": "blog.langchain.dev_llms-and-sql_.txt", "file_type": "text/plain", "file_size": 13010, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "db29abf1cd48762b90fbc0f137ff3cec85075c51c7c9b3bc45eeeedbbe61e0f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fab507da-6367-4727-906c-c0f3c6588536", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_lepton-x-langchain-earning-sage_.txt", "file_name": "blog.langchain.dev_lepton-x-langchain-earning-sage_.txt", "file_type": "text/plain", "file_size": 6197, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "01ebbe52058eb32a9a63155dc9776d2bed4fd399fcd27472f453182265e281b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "649a9591-69f7-49da-96cc-d5db61df64e4", "node_type": "1", "metadata": {}, "hash": "f4efcf2fff18c9ba659e8eded4bf8fb336ec1de60ef75ce2faa654042e282ecc", "class_name": "RelatedNodeInfo"}}, "hash": "fe91018d3e49697abbbac67b7c5e624b4919e58636fd059418514acb1f17f5e0", "text": "URL: https://blog.langchain.dev/llms-and-sql/\nTitle: LLMs and SQL\n\nFrancisco Ingham and Jon Luo are two of the community members leading the change on the SQL integrations. We\u2019re really excited to write this blog post with them going over all the tips and tricks they\u2019ve learned doing so. We\u2019re even more excited to announce that we\u2019ll be doing an hour long webinar with them to discuss these learnings and field other related questions. This webinar will be on March 22nd - sign up at the below link:\n\nThe LangChain library has multiple SQL chains and even an SQL agent aimed at making interacting with data stored in SQL as easy as possible. Here are some relevant links:\n\nIntroduction\n\nMost of an enterprise\u2019s data is traditionally stored in SQL databases. With the amount of valuable data stored there, business intelligence (BI) tools that make it easy to query and understand the data present there have risen in popularity. But what if you could just interact with a SQL database in natural language? With LLMs today, that is possible. LLMs have an understanding of SQL and are able to write it pretty well. However, there are several issues that make this a non-trivial task.\n\nThe Problems\n\nSo LLMs can write SQL - what more is needed?\n\nUnfortunately, a few things.\n\nThe main issue that exists is hallucination. LLMs can write SQL, but they are often prone to making up tables, making up fields, and generally just writing SQL that if executed against your database would not actually be valid. So one of the big challenges we face is how to ground the LLM in reality so that it produces valid SQL.\n\nThe main idea to fix this (we will go into more detail below) is to provide the LLM with knowledge about what actually exists in the database and tell it to write a SQL query consistent with that. However, this runs into a second issue - the context window length. LLMs have some context window which limits the amount of text they can operate over. This is relevant because SQL databases often contain a lot of information. So if we were to naively pass in all the data to ground the LLM in reality, we would likely run into this issue.\n\nA third issue is a more basic one: sometimes the LLM just messes up. The SQL it writes may be incorrect for whatever reason, or it could be correct but just return an unexpected result. What do we do then? Do we give up?\n\nThe (High Level) Solutions\n\n\n\nWhen thinking about how to tackle these issues, it\u2019s informative to think about how we as humans tackle these issues. If we can then replicate the steps that we would take to solve those problems, we can help the LLM do so as well. So let\u2019s think about what a data analyst would do if they were asked to answer a BI question.\n\nWhen data analysts query SQL databases, there\u2019s a few things they normally do that help them make the right queries. For example, they usually make a sample query beforehand to understand what the data looks like. They can look at the schema of the tables, or even certain rows. This can be thought of as the data analyst learning what the data looks like so that when they write a SQL query in the future it is grounded in what actually exists. Data analysts also don\u2019t usually just look at all the data (or thousands of rows) at the same time - they may limit any exploratory queries to the top K rows, or look at summary stats instead. This can yield some hints at how to get around the context window limitations. And finally, if a data analyst hits an error, they don\u2019t just give up - they learn from the error and write a new query.\n\nWe discuss each of these solutions in a separate section below.\n\nDescribing your database\n\nTo provide the LLM with enough information for it to generate reasonable queries for a given database, we need to effectively describe the database in the prompt. This can include describing table structure, examples of what the data looks like, and even examples of good queries for the database. The examples below come from the Chinook database.\n\nDescribing the schema\n\nIn older versions of LangChain, we simply provided the table names, columns, and their types:\n\nTable 'Track' has columns: TrackId (INTEGER), Name (NVARCHAR(200)), AlbumId (INTEGER), MediaTypeId (INTEGER), GenreId (INTEGER), Composer (NVARCHAR(220)), Milliseconds (INTEGER), Bytes (INTEGER), UnitPrice (NUMERIC(10, 2))\n\nRajkumar et al performed a study evaluating the Text-to-SQL performance of OpenAI Codex given a variety of different prompting structures. They achieved the best performance when prompting Codex with the CREATE TABLE commands, which include column names, their types, column references, and keys. For the Track table, this looks", "start_char_idx": 0, "end_char_idx": 4677, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "649a9591-69f7-49da-96cc-d5db61df64e4": {"__data__": {"id_": "649a9591-69f7-49da-96cc-d5db61df64e4", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_llms-and-sql_.txt", "file_name": "blog.langchain.dev_llms-and-sql_.txt", "file_type": "text/plain", "file_size": 13010, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65409667-83da-4b4c-8725-3c8393996070", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_llms-and-sql_.txt", "file_name": "blog.langchain.dev_llms-and-sql_.txt", "file_type": "text/plain", "file_size": 13010, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "db29abf1cd48762b90fbc0f137ff3cec85075c51c7c9b3bc45eeeedbbe61e0f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60466d64-81fb-46a4-a9f9-105ee1f4c9da", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_llms-and-sql_.txt", "file_name": "blog.langchain.dev_llms-and-sql_.txt", "file_type": "text/plain", "file_size": 13010, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "fe91018d3e49697abbbac67b7c5e624b4919e58636fd059418514acb1f17f5e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5364800-f3bd-4e89-8e38-0333a123361e", "node_type": "1", "metadata": {}, "hash": "07f408eb8d5a29c9163d240a49b151ba5d601cfd7fac342520670d5f69ae978c", "class_name": "RelatedNodeInfo"}}, "hash": "f4efcf2fff18c9ba659e8eded4bf8fb336ec1de60ef75ce2faa654042e282ecc", "text": "include column names, their types, column references, and keys. For the Track table, this looks like:\n\nCREATE TABLE \"Track\" ( \"TrackId\" INTEGER NOT NULL, \"Name\" NVARCHAR(200) NOT NULL, \"AlbumId\" INTEGER, \"MediaTypeId\" INTEGER NOT NULL, \"GenreId\" INTEGER, \"Composer\" NVARCHAR(220), \"Milliseconds\" INTEGER NOT NULL, \"Bytes\" INTEGER, \"UnitPrice\" NUMERIC(10, 2) NOT NULL, PRIMARY KEY (\"TrackId\"), FOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"), FOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"), FOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\") )\n\nDescribing the data\n\nWe can further improve the LLM\u2019s ability to create optimal queries by additionally providing examples of what the data looks like. For example, if we are searching for composers in the Track table, it will be quite useful to know if the Composer column consists of full names, abbreviated names, both, or perhaps even other representation. Rajkumar et al found that providing example rows in a SELECT statement following the CREATE TABLE description resulted in consistent performance improvements. Interestingly, they found that providing 3 rows was optimal, and that providing more database content can even decrease performance.\n\nWe\u2019ve adopted the best practice findings from their paper as the default settings. Together, our database description in the prompt looks like this:\n\ndb = SQLDatabase.from_uri( \"sqlite:///../../../../notebooks/Chinook.db\", include_tables=['Track'], # including only one table for illustration sample_rows_in_table_info=3 ) print(db.table_info)\n\nWhich outputs:\n\nCREATE TABLE \"Track\" ( \"TrackId\" INTEGER NOT NULL, \"Name\" NVARCHAR(200) NOT NULL, \"AlbumId\" INTEGER, \"MediaTypeId\" INTEGER NOT NULL, \"GenreId\" INTEGER, \"Composer\" NVARCHAR(220), \"Milliseconds\" INTEGER NOT NULL, \"Bytes\" INTEGER, \"UnitPrice\" NUMERIC(10, 2) NOT NULL, PRIMARY KEY (\"TrackId\"), FOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"), FOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"), FOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\") ) SELECT * FROM 'Track' LIMIT 3; TrackId Name AlbumId MediaTypeId GenreId Composer Milliseconds Bytes UnitPrice 1 For Those About To Rock (We Salute You) 1 1 1 Angus Young, Malcolm Young, Brian Johnson 343719 11170334 0.99 2 Balls to the Wall 2 2 1 None 342562 5510424 0.99 3 Fast As a Shark 3 2 1 F. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman 230619 3990994 0.99\n\nUsing custom table information\n\nAlthough LangChain conveniently assembles the schema and sample row descriptions automatically, there are a few cases in which it is preferable to override the automatic info with hand-crafted descriptions. For example, if you know that the first few rows of a table are uninformative, it is best to manually provide example rows that provide the LLM with more information. As an example, in the `Track` table, sometimes multiple composers are separated by slashes instead of commas. This first appears in row 111 of the table, well beyond our limit of 3 rows. We can provide this custom information such that the example rows contain this new information. Here\u2019s an example of doing this in practice.\n\nIt is also possible to use a custom description to limit the columns of a table that are visible to the LLM. An example of these two uses applied to the Track table might look like:\n\nCREATE TABLE \"Track\" ( \"TrackId\" INTEGER NOT NULL, \"Name\" NVARCHAR(200) NOT NULL, \"Composer\" NVARCHAR(220), PRIMARY KEY (\"TrackId\"), ) SELECT * FROM 'Track' LIMIT 4; TrackId Name Composer 1 For Those About To Rock (We Salute You) Angus Young, Malcolm Young, Brian Johnson 2 Balls to the Wall None 3 Fast As a Shark F. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman 4 Money Berry Gordy, Jr./Janie Bradford\n\nIf you have sensitive data that you do not wish to send to an API, you can use this feature to provide mock data instead of your actual database.\n\nConstraining the size of the output\n\nWhen we make queries with LLMs within a chain or agent, the result from our query will be used as the input for another LLM. If the query result is too big this will max out our model\u2019s input size. So it is usually a good practice to sensibly limit", "start_char_idx": 4582, "end_char_idx": 8764, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5364800-f3bd-4e89-8e38-0333a123361e": {"__data__": {"id_": "c5364800-f3bd-4e89-8e38-0333a123361e", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_llms-and-sql_.txt", "file_name": "blog.langchain.dev_llms-and-sql_.txt", "file_type": "text/plain", "file_size": 13010, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65409667-83da-4b4c-8725-3c8393996070", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_llms-and-sql_.txt", "file_name": "blog.langchain.dev_llms-and-sql_.txt", "file_type": "text/plain", "file_size": 13010, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "db29abf1cd48762b90fbc0f137ff3cec85075c51c7c9b3bc45eeeedbbe61e0f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "649a9591-69f7-49da-96cc-d5db61df64e4", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_llms-and-sql_.txt", "file_name": "blog.langchain.dev_llms-and-sql_.txt", "file_type": "text/plain", "file_size": 13010, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "f4efcf2fff18c9ba659e8eded4bf8fb336ec1de60ef75ce2faa654042e282ecc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35df53c4-4d0e-4233-9fe7-93a81839b6d6", "node_type": "1", "metadata": {}, "hash": "2312e270eb576ce6ddfcb28c0904c239ff10ad863ce64a194d7dd38535d17bfb", "class_name": "RelatedNodeInfo"}}, "hash": "07f408eb8d5a29c9163d240a49b151ba5d601cfd7fac342520670d5f69ae978c", "text": "will max out our model\u2019s input size. So it is usually a good practice to sensibly limit the size of the output of our query. We can do this by instructing our LLM to use as few columns as possible and limit the number of returned rows.\n\nAs we can in the following example, if we ask for the list of total sales per country without specifying a number of countries, the query will be capped to 10. You can manage this limit with the top_k parameter.\n\n\n\nagent_executor.run(\"List the total sales per country. Which country's customers spent the most?\")\n\n>>\n\n\u2026 Action Input: SELECT c.Country, SUM(i.Total) AS TotalSales FROM Invoice i INNER JOIN Customer c ON i.CustomerId = c.CustomerId GROUP BY c.Country ORDER BY TotalSales DESC LIMIT 10 Observation: [('USA', 523.0600000000003), ('Canada', 303.9599999999999), ('France', 195.09999999999994), ('Brazil', 190.09999999999997), ('Germany', 156.48), ('United Kingdom', 112.85999999999999), ('Czech Republic', 90.24000000000001), ('Portugal', 77.23999999999998), ('India', 75.25999999999999), ('Chile', 46.62)] \u2026\n\nChecking syntax\n\nIf our LLM generated query is syntactically broken we will find that we will get a traceback when running our chain or agent. This is highly problematic if we want to use this for productive purposes. How could we help the LLM correct the query? We can replicate exactly what we would have done if we had made the mistake ourselves. We send the original query with the traceback log to the LLM and ask it to make it right, by understanding exactly what went wrong. This concept is inspired by this blogpost where you can find a more detailed explanation.\n\nIn the following example from the docs, you can see that the model was trying to query for an unexisting column and when it finds the query is wrong it promptly corrects it with the query_checker_sql_db tool:\n\nObservation: Error: (sqlite3.OperationalError) no such column: Track.ArtistId\n\n[SQL: SELECT Artist.Name, SUM(InvoiceLine.Quantity) AS TotalQuantity FROM Artist INNER JOIN Track ON Artist.ArtistId = Track.ArtistId INNER JOIN InvoiceLine ON Track.TrackId = InvoiceLine.TrackId GROUP BY Artist.Name ORDER BY TotalQuantity DESC LIMIT 3]\n\n(Background on this error at: https://sqlalche.me/e/14/e3q8)\n\nThought: I should double check my query before executing it.\n\nAction: query_checker_sql_db\n\nAction Input: SELECT Artist.Name, SUM(InvoiceLine.Quantity) AS TotalQuantity FROM Artist INNER JOIN Track ON Artist.ArtistId = Track.ArtistId INNER JOIN InvoiceLine ON Track.TrackId = InvoiceLine.TrackId GROUP BY Artist.Name ORDER BY TotalQuantity DESC LIMIT 3\n\nObservation:\n\nSELECT Artist.Name, SUM(InvoiceLine.Quantity) AS TotalQuantity\n\nFROM Artist\n\nINNER JOIN Track ON Artist.ArtistId = Track.ArtistId\n\nINNER JOIN InvoiceLine ON Track.TrackId = InvoiceLine.TrackId\n\nGROUP BY Artist.Name\n\nORDER BY TotalQuantity DESC\n\nLIMIT 3;\n\nThought: I now know the final answer.\n\nAction: query_sql_db\n\nAction Input: SELECT Artist.Name, SUM(InvoiceLine.Quantity) AS TotalQuantity FROM Artist INNER JOIN Album ON Artist.ArtistId = Album.ArtistId INNER JOIN Track ON Album.AlbumId = Track.AlbumId INNER JOIN InvoiceLine ON Track.TrackId = InvoiceLine.TrackId GROUP BY Artist.Name ORDER BY TotalQuantity DESC LIMIT 3\n\nFuture work\n\nAs you know, the field is moving fast and we are collectively finding out the best ways for achieving optimal LLM-SQL interaction. Here is the backlog going forward:\n\nFew-shot examples\n\nRajkumar et al also found that Codex\u2019s SQL generation accuracy improved in benchmarks with few-shot learning, where question-query examples are appended to the prompt (see Figure 2).\n\n\n\nUse subqueries\n\nSome users have found that telling the agent to break down the problem into multiple subqueries, including comments on what each subquery does, helps the agent get to the correct answer. Thinking in subqueries forces the agent to think in logical steps and thus reduces the probability of making structural mistakes in the query. This is analogous to adding CoT type phrases to the prompt like \u2018think this problem step by step\u201d for non-sql problems.\n\nIf you want to help implementing any of these or have other best practices that you found helpful, please do share your thoughts in the discussion in the #sql channel in Discord or directly take a stab at a PR!", "start_char_idx": 8677, "end_char_idx": 12972, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35df53c4-4d0e-4233-9fe7-93a81839b6d6": {"__data__": {"id_": "35df53c4-4d0e-4233-9fe7-93a81839b6d6", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_llms-to-improve-documentation_.txt", "file_name": "blog.langchain.dev_llms-to-improve-documentation_.txt", "file_type": "text/plain", "file_size": 4689, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a43a6eb-1ccb-4acc-bc84-6e65487ec16a", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_llms-to-improve-documentation_.txt", "file_name": "blog.langchain.dev_llms-to-improve-documentation_.txt", "file_type": "text/plain", "file_size": 4689, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "19d0e6b6668f8e2df3435668e72d81af7766cd30bb43eba4026841c5c3a5a16c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5364800-f3bd-4e89-8e38-0333a123361e", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_llms-and-sql_.txt", "file_name": "blog.langchain.dev_llms-and-sql_.txt", "file_type": "text/plain", "file_size": 13010, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "07f408eb8d5a29c9163d240a49b151ba5d601cfd7fac342520670d5f69ae978c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c20ae551-1854-4eb4-821b-e2bd868bcb0d", "node_type": "1", "metadata": {}, "hash": "2160d5d94a9a4ff2330b654aa99d9a80022826d2e1193b2a6cba98c47d80e11f", "class_name": "RelatedNodeInfo"}}, "hash": "2312e270eb576ce6ddfcb28c0904c239ff10ad863ce64a194d7dd38535d17bfb", "text": "URL: https://blog.langchain.dev/llms-to-improve-documentation/\nTitle: Analyzing User Interactions with LLMs to Improve our Documentation\n\nIntroduction\n\nWe're strongly committed to consistently enhancing our documentation and its navigability. Using Mendable, a AI-enabled chat application, users can search our documentation using keywords or questions. Over time, Mendable has collected a large dataset of questions that highlights areas for documentation improvement.\n\nChallenge\n\nDistilling common themes from tens of thousands of questions per month is a significant challenge. Manual labeling can be effective, but is slow and laborious. Statistical methods can analyze word distributions to infer common topics, but may not capture the semantic richness and context of the questions.\n\nProposal\n\nLLMs can help us summarize and identify documentation gaps from the questions collected by Mendable. We experimented with two methods to pass large question datasets to an LLM: 1) Group similar questions via clustering before summarizing each group and 2) Apply a map-reduce approach that splits questions into small segments, summarizes each, and then combines them into a final synthesis.\n\nApproaches for summarizing large datasets of user questions\n\nThere are tradeoffs between the approaches, which we wanted to examine:\n\nTrade-offs between clustering and map-reduce\n\nResults\n\nWe tested an end-to-end LLM summarization pipeline that uses LangChain\u2019s map-reduce chain to split questions into groups based on the context window of either GPT-3.5-16k (16k tokens) or Claude-2 (100k tokens), summarize each (map), and then distill the group summaries into a final synthesis (reduce).\n\nWe also tested k-Means clustering of embedded questions followed by GPT-4 to summarize each cluster, an approach similar to what OpenAI reported in one of their cookbooks. For consistency, we use the same input dataset as map-reduce.\n\nWe open sourced the notebooks and the data (see repo here) so that this analysis can be reproduced. Here is a sheet with detailed results, which we summarize in the table below; we asked both methods to summarize the major question themes being asked by users with a proportion of questions that fall into each bucket:\n\nDistribution of question themes summarized in different experiments\n\nSpecific themes can be interrogated using alternative summarization prompts; for example, using map-reduce was can ask the reduce stage to return top questions on a specific theme (e.g., data processing). For example, using this reduce prompt:\n\nThe following is a list of summaries for questions entered into a Q+A system: {question_summaries} Take these and distill it into a final, consolidated list with: (1) the top 10 question related to loading, processing, and manipulating different types of data and documents. (2) estimate the proportion of each question\n\nWe get granular thematic breakdown of the Top 10 Questions Related to Loading, Processing, and Manipulating Different Types of Data and Documents:\n\n1. \"How can I load a PDF file and split it into chunks using langchain?\" - 15%` 2. \"How do I load and process a CSV file using Langchain?\" - 12% 3. \"How do I use the 'readfiletool' to load a text file?\" - 11% 4. \"How do I use Langchain to summarize a PDF document using the LLM model?\" - 10% 5. \"What are the different data loaders available in Langchain, and how do I choose the right one for my use case?\" - 9% 6. \"How do I load and process multiple PDFs?\" - 9% 7. \"How do I load all documents in a folder?\" - 8% 8. \"How do I split a string into a list of words in Python?\" - 8% 9. \"How do I load and process HTML content using BeautifulSoup?\" - 8% 10. \"How can I add metadata to the Pinecone upsert?\" - 10%\n\nTo get better diagnostic analysis of the cost, we use soon-to-launch LangChain tooling to compare diagnostics (token usage, etc) for the approaches. For example, we quantify token usage, which shows that map-reduce indeed has higher cost:\n\n~500k tokens\n\n~80k tokens (~8k / cluster with 10 clusters)\n\nSummary\n\nAs expected, there are trade-offs between the approaches. Map-Reduce provides high customizability because questions can be split into arbitrarily granular groups and summarized with tunable map-reduce prompts. However, the cost may be considerably higher as noted by token usage. Clustering risks information loss due to hand-tuning (e.g., of the cluster number) in the preprocessing stage, but it offers lower cost and may be a sensible way to quickly compressive very large datasets prior to more granular (and high cost) LLM summarization.", "start_char_idx": 0, "end_char_idx": 4586, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c20ae551-1854-4eb4-821b-e2bd868bcb0d": {"__data__": {"id_": "c20ae551-1854-4eb4-821b-e2bd868bcb0d", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_llms-to-improve-documentation_.txt", "file_name": "blog.langchain.dev_llms-to-improve-documentation_.txt", "file_type": "text/plain", "file_size": 4689, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a43a6eb-1ccb-4acc-bc84-6e65487ec16a", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_llms-to-improve-documentation_.txt", "file_name": "blog.langchain.dev_llms-to-improve-documentation_.txt", "file_type": "text/plain", "file_size": 4689, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "19d0e6b6668f8e2df3435668e72d81af7766cd30bb43eba4026841c5c3a5a16c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35df53c4-4d0e-4233-9fe7-93a81839b6d6", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_llms-to-improve-documentation_.txt", "file_name": "blog.langchain.dev_llms-to-improve-documentation_.txt", "file_type": "text/plain", "file_size": 4689, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "2312e270eb576ce6ddfcb28c0904c239ff10ad863ce64a194d7dd38535d17bfb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c570dc5-7ecf-40a6-99ec-219181965149", "node_type": "1", "metadata": {}, "hash": "dbb5bd85edf5552ba5d442cf444d55b5117309735e3c43c3ccaa11eb24c20797", "class_name": "RelatedNodeInfo"}}, "hash": "2160d5d94a9a4ff2330b654aa99d9a80022826d2e1193b2a6cba98c47d80e11f", "text": "compressive very large datasets prior to more granular (and high cost) LLM summarization. The thoughtful union of these two methods offers considerable promise for addressing this challenge.", "start_char_idx": 4497, "end_char_idx": 4687, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c570dc5-7ecf-40a6-99ec-219181965149": {"__data__": {"id_": "8c570dc5-7ecf-40a6-99ec-219181965149", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_making-data-ingestion-production-ready-a-langchain-powered-airbyte-destination_.txt", "file_name": "blog.langchain.dev_making-data-ingestion-production-ready-a-langchain-powered-airbyte-destination_.txt", "file_type": "text/plain", "file_size": 4144, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f9fa58d-29eb-4248-b6cb-52465ee67e91", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_making-data-ingestion-production-ready-a-langchain-powered-airbyte-destination_.txt", "file_name": "blog.langchain.dev_making-data-ingestion-production-ready-a-langchain-powered-airbyte-destination_.txt", "file_type": "text/plain", "file_size": 4144, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "2821bb0bcfe6f290c0e290cbf0dca14242cccade32643b28b2945da097d38b56", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c20ae551-1854-4eb4-821b-e2bd868bcb0d", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_llms-to-improve-documentation_.txt", "file_name": "blog.langchain.dev_llms-to-improve-documentation_.txt", "file_type": "text/plain", "file_size": 4689, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "2160d5d94a9a4ff2330b654aa99d9a80022826d2e1193b2a6cba98c47d80e11f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c48cc975-3ac0-4be5-83e4-c519dab13668", "node_type": "1", "metadata": {}, "hash": "ca88e1c77e86294b059a9cb6f750a120dede93b6cf13547ae65543bac4c32e67", "class_name": "RelatedNodeInfo"}}, "hash": "dbb5bd85edf5552ba5d442cf444d55b5117309735e3c43c3ccaa11eb24c20797", "text": "URL: https://blog.langchain.dev/making-data-ingestion-production-ready-a-langchain-powered-airbyte-destination/\nTitle: Making Data Ingestion Production Ready: a LangChain-Powered Airbyte Destination\n\nA big focus of ours over the past few months has been enabling teams to go from prototype to production. To take apps they developed in an hour and get them into a place where they can actually be reliably used. Arguably the biggest category of applications LangChain helps enable is retrieval based applications (where you connect LLMs to your own data). There are a few things that are needed to take retrieval based applications from prototype to production.\n\nOne component of that is everything related to the querying of the data. That\u2019s why we launched LangSmith - to help debug and monitor how LLMs interact with the user query as well as the retrieved documents. Another huge aspect is the querying algorithms and UX around that - which is why we\u2019re pushing on things like Conversational Retrieval Agents. (If you are interested in this part in particular, we\u2019re doing a webinar on \u201cAdvanced Retrieval\u201d on August 9th). A third - and arguably the most important part - is the ingestion logic itself. When taking an application into production, you want the data it\u2019s connecting to be refreshed on some schedule in a reliable and efficient way.\n\nOur first stab at tackling this is another, deeper integration with Airbyte. The previous Airbyte integration showed how to use one of their sources as a Document Loader within LangChain. This integration goes the other direction, and adds a LangChain destination within Airbyte.\n\nTo read more about this integration, you can check out Airbyte\u2019s release blog here. We will try not to repeat too much of that blog, but rather cover why we think this is an important step.\n\nLangChain provides \u201csources\u201d and \u201cdestinations\u201d of our own - we have hundreds of document loaders and 50+ vectorstore/retriever integrations. But far from being replacements for one another, this is rather a mutually beneficial integration that provides a lot of benefits for the community.\n\nFirst, Airbyte provides hundreds more sources, a robust orchestration logic, as well as tooling to create more sources. Let\u2019s focus on the orchestration logic. When you create a chatbot that has access to an index of your data, you don\u2019t just want to index your data there once and forget about it. You want to reindex it on some schedule, so that it stays up to date. This type of data pipelines is exactly what Airbyte excels at and has been building.\n\nSecond, the ingestion process isn\u2019t only about moving data from a source to a destination. There\u2019s also some important, non-trivial and nuanced transformations that are necessary to enable effective retrieval. Two of the most important - text splitting and embedding.\n\nSplitting text is important because you need to create chunks of data to put in the vectorstore. You want these chunks to be semantically meaningful by themselves - so that they make sense when retrieved. This is why it\u2019s often a bit trickier than just splitting a text every 1000 characters. LangChain provides implementations of 15+ different ways to split text, powered by different algorithms and optimized for different text types (markdown vs Python code, etc). To assist in the exploration of what these different text splitters offer, we've open-source and hosted a playground for easy exploration.\n\nEmbeddings are important to enable retrieval of those chunks, which is often done by comparing embeddings of a user query to embeddings of ingested documents. There are many different embedding providers and hosting platforms - and LangChain provides integrations with 50+ of them.\n\nOverall, we\u2019re really excited about this LangChain - Airbyte integration. It provides robust orchestration and scheduling for ingestion jobs while leveraging LangChain\u2019s transformation logic and integrations. We also think there\u2019s more features (and integrations) to add to make data ingestion production ready - keep on the lookout for more of those over the next few weeks.", "start_char_idx": 0, "end_char_idx": 4106, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c48cc975-3ac0-4be5-83e4-c519dab13668": {"__data__": {"id_": "c48cc975-3ac0-4be5-83e4-c519dab13668", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_multion-x-langchain-powering-next-gen-web-automation-navigation-with-ai_.txt", "file_name": "blog.langchain.dev_multion-x-langchain-powering-next-gen-web-automation-navigation-with-ai_.txt", "file_type": "text/plain", "file_size": 3739, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "93cdd84e-6b13-4405-adcc-134add3cbe11", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_multion-x-langchain-powering-next-gen-web-automation-navigation-with-ai_.txt", "file_name": "blog.langchain.dev_multion-x-langchain-powering-next-gen-web-automation-navigation-with-ai_.txt", "file_type": "text/plain", "file_size": 3739, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "cf26736eb97f66588b4b968e3b6ba5a8d75489f962449f2982060f6abb4d911e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c570dc5-7ecf-40a6-99ec-219181965149", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_making-data-ingestion-production-ready-a-langchain-powered-airbyte-destination_.txt", "file_name": "blog.langchain.dev_making-data-ingestion-production-ready-a-langchain-powered-airbyte-destination_.txt", "file_type": "text/plain", "file_size": 4144, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "dbb5bd85edf5552ba5d442cf444d55b5117309735e3c43c3ccaa11eb24c20797", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70ce7632-c91d-4431-a3f5-68397739189e", "node_type": "1", "metadata": {}, "hash": "eb956a0c87b7cd37fbe87e7a6b93b7da0c4725d7f34085d572824d4fdc5b19cc", "class_name": "RelatedNodeInfo"}}, "hash": "ca88e1c77e86294b059a9cb6f750a120dede93b6cf13547ae65543bac4c32e67", "text": "URL: https://blog.langchain.dev/multion-x-langchain-powering-next-gen-web-automation-navigation-with-ai/\nTitle: MultiOn x LangChain: Powering Next-Gen Web Automation & Navigation with AI\n\nEditor's Note: This post was written in collaboration with MultiOn. We're really excited about the way they're using Agents to automate and streamline online interactions. They are one of the first real world, production agent applications that we know of. Their integration with LangChain as a Toolkit makes it quick and easy to personalize and automate everyday web tasks.\n\nMultiOn: Your Personal AI Agent Now on LangChain\n\nWhether it's searching for information, filling out forms, or navigating complex websites, daily web tasks can often be tedious and time-consuming. That's why we're thrilled to introduce MultiOn, a next-generation personal AI assistant designed to interact with the web, to handle these tasks on your behalf.\n\nOperating much like the sci-fi concept of JARVIS, MultiOn leverages cutting-edge AI technology to interact with your browser to perform tasks for you in real-time, from ordering you dinner, booking flights, scheduling, finding information online, to even filling out forms. And the best part? MultiOn is now integrated directly within LangChain as a Toolkit, making it even easier to automate your everyday web tasks & build custom agents and applications that can take actions on the web.\n\nSeamless Integration with LangChain\n\nWith MultiOn directly integrated into LangChain, the power of Autonomous Web AI Agents is now at the fingertips of all users.\n\nThe integration unlocks numerous advantages. It provides LangChain users with an AI-powered tool that can automate a variety of everyday web tasks, from information retrieval to interaction with web services on their behalf. This integration not only enhances the functionality of LangChain but also takes the Action ability of agents to the next level - to now interact with any website!\n\nHere is a glimpse of how you can use MultiOn within LangChain to interact with the website in just 3 Lines of Code \ud83d\udd25:\n\nImport MultiOn as a LangChain Toolkit to add it to any custom Agent:\n\n\n\n# IMPORTS from langchain import OpenAI from langchain.agents import initialize_agent, AgentType from langchain.agents.agent_toolkits import MultionToolkit import multion multion.login() # MultiOn -> Login to the MultiOn Website # Initialize Agent agent = initialize_agent( tools=MultionToolkit().get_tools(), llm=OpenAI(temperature=0), agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose = True ) print(agent.run(\"Show Beautiful Pictures of New York\"))\n\nGet more samples at the MultiOn API repository.\n\n\n\nLangChain Agent Demo:\n\nOther\n\nMultiOn Scheduler App: Schedule recurring tasks that run periodically, such as \u201cwishing happy birthday to friends on fb\u201d everyday.\n\nGroup Dinner reservation Agent: Add MultiOn to a sms group chat and ask it to help book a group dinner on Opentable\n\nJoin the MultiOn Community!\n\n\n\nWe\u2019re very enthusiastic about the potential for Autonomous Web AI Agents, and more broadly, exploring new ways to harness the power of AI to improve online experiences. We believe that Actions are key to building powerful AI applications, and we want to empower developers & the open source community to build AI that can interact with the Web by building on top of MultiOn. Please check our documentation, contribute to adding examples, and join our Discord to experience the future of web task automation!\n\nStay tuned for more updates on our journey, and don't hesitate to reach us out at info@multion.ai if you have any questions or suggestions. We're always looking to hear from users and improve MultiOn to best serve your needs \ud83d\ude80", "start_char_idx": 0, "end_char_idx": 3727, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "70ce7632-c91d-4431-a3f5-68397739189e": {"__data__": {"id_": "70ce7632-c91d-4431-a3f5-68397739189e", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_neo4j-x-langchain-new-vector-index_.txt", "file_name": "blog.langchain.dev_neo4j-x-langchain-new-vector-index_.txt", "file_type": "text/plain", "file_size": 9850, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a2a18fb6-3415-4745-9f4b-b263304d32d4", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_neo4j-x-langchain-new-vector-index_.txt", "file_name": "blog.langchain.dev_neo4j-x-langchain-new-vector-index_.txt", "file_type": "text/plain", "file_size": 9850, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "93bd99c7eaef9b3c01e5f03d59d44fa9c5faa626a9d845296a9ceacb6a657995", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c48cc975-3ac0-4be5-83e4-c519dab13668", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_multion-x-langchain-powering-next-gen-web-automation-navigation-with-ai_.txt", "file_name": "blog.langchain.dev_multion-x-langchain-powering-next-gen-web-automation-navigation-with-ai_.txt", "file_type": "text/plain", "file_size": 3739, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "ca88e1c77e86294b059a9cb6f750a120dede93b6cf13547ae65543bac4c32e67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f40aae8-2ca5-4c8b-8e30-1eb25c595274", "node_type": "1", "metadata": {}, "hash": "88d306ffeb818abd2424a84aa6a6ed3436e9ab10977deed853bef04d2d6c0597", "class_name": "RelatedNodeInfo"}}, "hash": "eb956a0c87b7cd37fbe87e7a6b93b7da0c4725d7f34085d572824d4fdc5b19cc", "text": "URL: https://blog.langchain.dev/neo4j-x-langchain-new-vector-index/\nTitle: Neo4j x LangChain: Deep dive into the new Vector index implementation\n\nLearn how to customize LangChain\u2019s wrapper of Neo4j vector index\n\nEditor's Note: This post was written in collaboration with the Neo4j team. We've been working closely with them on their new vector index and we're really impressed with its ability to efficiently perform semantic search over unstructured text or other embedded data modalities, unlocking support for RAG applications and more customization.\n\nNeo4j was and is an excellent fit for handling structured information, but it struggled a bit with semantic search due to its brute-force approach. However, the struggle is in the past as Neo4j has introduced a new vector index in version 5.11 designed to efficiently perform semantic search over unstructured text or other embedded data modalities. The newly added vector index makes Neo4j a great fit for most RAG applications as it now works great with both structured and unstructured data.\n\nImage by author.\n\nThis blog post is designed to walk you through all the customization options available in the Neo4j Vector Index implementation in LangChain.\n\nThe code is available on GitHub.\n\nNeo4j Environment setup\n\nYou need to setup a Neo4j 5.11 or greater to follow along with the examples in this blog post. The easiest way is to start a free instance on Neo4j Aura, which offers cloud instances of Neo4j database. Alternatively, you can also setup a local instance of the Neo4j database by downloading the Neo4j Desktop application and creating a local database instance.\n\nExample dataset\n\nFor the purpose of this blog post, we will use the WikipediaLoader to fetch text from the Witcher page.\n\nfrom langchain.document_loaders import WikipediaLoader from langchain.text_splitter import CharacterTextSplitter # Read the wikipedia article raw_documents = WikipediaLoader(query=\"The Witcher\").load() # Define chunking strategy text_splitter = CharacterTextSplitter.from_tiktoken_encoder( chunk_size=1000, chunk_overlap=20 ) # Chunk the document documents = text_splitter.split_documents(raw_documents) # Remove the summary for d in documents: del d.metadata[\"summary\"]\n\nNeo4j Vector index customization\n\nEach text chunk is stored in Neo4j as a single isolated node.\n\nGraph schema of imported documents.\n\nBy default, Neo4j vector index implementation in LangChain represents the documents using the Chunk node label, where the text property stores the text of the document, and the embedding property holds the vector representation of the text. The implementation allows you to customize the node label, text and embedding property names.\n\nneo4j_db = Neo4jVector.from_documents( documents, OpenAIEmbeddings(), url=url, username=username, password=password, database=\"neo4j\", # neo4j by default index_name=\"wikipedia\", # vector by default node_label=\"WikipediaArticle\", # Chunk by default text_node_property=\"info\", # text by default embedding_node_property=\"vector\", # embedding by default create_id_index=True, # True by default )\n\nIn this example, we have specified that we want to store text chunks under the WikipediaArticle node label, where the info property is used to store text, and the vector property holds the text embedding representation. If you run the above examples, you should see the following information in the database.\n\nNode information.\n\nAs mentioned, we define the info property to contain the text information, while the vector property is used to store the embedding. Any other properties like the source and title are treated as document metadata.\n\nBy default, we also create a unique node property constraint on the id property of the specified node label for faster imports. If you don\u2019t want to create a unique constraint, you can set the create_id_index to false. You can verify that the constraint has been created by using the following Cypher statement:\n\nneo4j_db.query(\"SHOW CONSTRAINTS\") #[{'id': 4, # 'name': 'constraint_e5da4d45', # 'type': 'UNIQUENESS', # 'entityType': 'NODE', # 'labelsOrTypes': ['WikipediaArticle'], # 'properties': ['id'], # 'ownedIndex': 'constraint_e5da4d45', # 'propertyType': None}]\n\nAs you would expect, we also create a vector index that will allow us to perform fast ANN searches.\n\nneo4j_db.query( \"\"\"SHOW INDEXES YIELD name, type, labelsOrTypes, properties, options WHERE type = 'VECTOR' \"\"\" ) #[{'name': 'wikipedia', # 'type': 'VECTOR', # 'labelsOrTypes': ['WikipediaArticle'], # 'properties': ['vector'], # 'options': {'indexProvider': 'vector-1.0', # 'indexConfig':", "start_char_idx": 0, "end_char_idx": 4592, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f40aae8-2ca5-4c8b-8e30-1eb25c595274": {"__data__": {"id_": "4f40aae8-2ca5-4c8b-8e30-1eb25c595274", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_neo4j-x-langchain-new-vector-index_.txt", "file_name": "blog.langchain.dev_neo4j-x-langchain-new-vector-index_.txt", "file_type": "text/plain", "file_size": 9850, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a2a18fb6-3415-4745-9f4b-b263304d32d4", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_neo4j-x-langchain-new-vector-index_.txt", "file_name": "blog.langchain.dev_neo4j-x-langchain-new-vector-index_.txt", "file_type": "text/plain", "file_size": 9850, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "93bd99c7eaef9b3c01e5f03d59d44fa9c5faa626a9d845296a9ceacb6a657995", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70ce7632-c91d-4431-a3f5-68397739189e", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_neo4j-x-langchain-new-vector-index_.txt", "file_name": "blog.langchain.dev_neo4j-x-langchain-new-vector-index_.txt", "file_type": "text/plain", "file_size": 9850, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "eb956a0c87b7cd37fbe87e7a6b93b7da0c4725d7f34085d572824d4fdc5b19cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b60a9dcf-19b3-4df1-ad5b-228c50b8032c", "node_type": "1", "metadata": {}, "hash": "683f936b2c3f6e07ad83c9eadc352a458fd470188900e0f8e7e0312f8ce9b18c", "class_name": "RelatedNodeInfo"}}, "hash": "88d306ffeb818abd2424a84aa6a6ed3436e9ab10977deed853bef04d2d6c0597", "text": "# 'options': {'indexProvider': 'vector-1.0', # 'indexConfig': {'vector.dimensions': 1536, # 'vector.similarity_function': 'cosine'}}}]\n\nThe LangChain implementation created a vector index named wikipedia , which indexes the vector property of WikipediaArticle nodes. Additionally, the provided configuration informs us that the vector embedding dimension is 1536 and uses the cosine similarity function.\n\nLoading additional documents\n\nYou can use the add_documents method to load additional documents into an instantiated vector index.\n\nneo4j_db.add_documents( [ Document( page_content=\"LangChain is the coolest library since the Library of Alexandria\", metadata={\"author\": \"Tomaz\", \"confidence\": 1.0} ) ], ids=[\"langchain\"], )\n\nLangChain allows you to provide document ids to the add_document method, which can be used to sync information across different system and make it easier to update or delete relevant text chunks.\n\nLoading existing index\n\nIf you have an existing vector index in Neo4j with populated data, you can use the from_existing_method to connect to it.\n\nexisting_index = Neo4jVector.from_existing_index( OpenAIEmbeddings(), url=url, username=username, password=password, index_name=\"wikipedia\", text_node_property=\"info\", # Need to define if it is not default )\n\nFirst, the from_existing_method checks if the index with the provided name actually exists in the database. If it exists, it can retrieve the node label and embedding node property from index configuration map, which means that you don\u2019t have to manually set those.\n\nprint(existing_index.node_label) # WikipediaArticle print(existing_index.embedding_node_property) # vector\n\nHowever, the index information does not contain the text node property information. Therefore, if you use any property besides the default one ( text ), specify it using the text_node_property parameter.\n\nCustom retrieval queries\n\nSince Neo4j is a native graph database, the vector index implementation in LangChain allows customization and enrichment of the returned information. However, this feature is intended for more advanced users as you are responsible for custom data loading as well as retrieval.\n\nThe retrieval_query parameter allows you to collect, transform, or calculate any additional graph information you want to return from the similarity search. To better understand it, we can look at the actual implementation in the code.\n\nread_query = ( \"CALL db.index.vector.queryNodes($index, $k, $embedding) \" \"YIELD node, score \" ) + retrieval_query\n\nFrom the code, we can observe that the vector similarity search is hardcoded. However, we then have the option to add any intermediate steps and return additional information. The retrieval query must return the following three columns:\n\ntext (String): This is usually the textual data that is associated with the node that has been retrieved. This could be the main content of the node, a name, a description, or any other text-based information.\n\nscore (Float): This represents the similarity score between the query vector and the vector associated with the returned node. The score quantifies how similar the query is to the returned nodes, often on a scale from 0 to 1\n\nmetadata (Dictionary): This is a more flexible column that can contain additional information about the node or the search. It can be a dictionary (or map) that includes various attributes or properties that give more context to the returned node.\n\nWe will add a relationship to a WikipediaArticle node to demonstrate this functionality.\n\nexisting_index.query( \"\"\"MATCH (w:WikipediaArticle {id:'langchain'}) MERGE (w)<-[:EDITED_BY]-(:Person {name:\"Galileo\"}) \"\"\" )\n\nWe have added an EDITED_BY relationship to the WikipediaArticle node with the given id. Let\u2019s now test out a custom retrieval option.\n\nretrieval_query = \"\"\" OPTIONAL MATCH (node)<-[:EDITED_BY]-(p) WITH node, score, collect(p) AS editors RETURN node.info AS text, score, node {.*, vector: Null, info: Null, editors: editors} AS metadata \"\"\" existing_index_return = Neo4jVector.from_existing_index( OpenAIEmbeddings(), url=url, username=username, password=password, database=\"neo4j\", index_name=\"wikipedia\", text_node_property=\"info\", retrieval_query=retrieval_query, )\n\nI won\u2019t go too much into the specifics of Cypher. You can use many resources to learn the basic syntax and more like the Neo4j Graph Academy. To construct a valid retrieval query, you must know that the relevant node from the vector similarity search is available under the node reference variable, while the similarity metric value is available under the score reference.\n\nLet\u2019s try it out.\n\nexisting_index_return.similarity_search( \"What do you know about LangChain?\", k=1) #[ # Document(\"page_content=\"\"LangChain is the coolest library since the Library of Alexandria\", # \"metadata=\"{ #", "start_char_idx": 4531, "end_char_idx": 9349, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b60a9dcf-19b3-4df1-ad5b-228c50b8032c": {"__data__": {"id_": "b60a9dcf-19b3-4df1-ad5b-228c50b8032c", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_neo4j-x-langchain-new-vector-index_.txt", "file_name": "blog.langchain.dev_neo4j-x-langchain-new-vector-index_.txt", "file_type": "text/plain", "file_size": 9850, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a2a18fb6-3415-4745-9f4b-b263304d32d4", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_neo4j-x-langchain-new-vector-index_.txt", "file_name": "blog.langchain.dev_neo4j-x-langchain-new-vector-index_.txt", "file_type": "text/plain", "file_size": 9850, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "93bd99c7eaef9b3c01e5f03d59d44fa9c5faa626a9d845296a9ceacb6a657995", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f40aae8-2ca5-4c8b-8e30-1eb25c595274", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_neo4j-x-langchain-new-vector-index_.txt", "file_name": "blog.langchain.dev_neo4j-x-langchain-new-vector-index_.txt", "file_type": "text/plain", "file_size": 9850, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "88d306ffeb818abd2424a84aa6a6ed3436e9ab10977deed853bef04d2d6c0597", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "697fc1ef-e443-472a-9121-97a2a620f624", "node_type": "1", "metadata": {}, "hash": "f58deea3ec2d3d72ce078a38ef18ec45a33cc4ad8fa05ee3eec29f306c12f7ae", "class_name": "RelatedNodeInfo"}}, "hash": "683f936b2c3f6e07ad83c9eadc352a458fd470188900e0f8e7e0312f8ce9b18c", "text": "is the coolest library since the Library of Alexandria\", # \"metadata=\"{ # \"author\":\"Tomaz\", # \"confidence\":1.0, # \"id\":\"langchain\", # \"editors\":[ # { # \"name\":\"Galileo\" # } # ] # }\")\" #]\n\nYou can observe that the metadata information contains the editor property, which was calculated from graph information.\n\nSummary\n\nThe newly added vector index implementation in Neo4j allows it to support RAG applications that rely on both structured and unstructured data, making it a perfect fit for highly-complex and connected datasets.\n\nThe code is available on GitHub.", "start_char_idx": 9276, "end_char_idx": 9838, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "697fc1ef-e443-472a-9121-97a2a620f624": {"__data__": {"id_": "697fc1ef-e443-472a-9121-97a2a620f624", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_neum-x-langchain_.txt", "file_name": "blog.langchain.dev_neum-x-langchain_.txt", "file_type": "text/plain", "file_size": 6284, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1eee1785-8b22-4586-bd15-a49a1e1c88cb", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_neum-x-langchain_.txt", "file_name": "blog.langchain.dev_neum-x-langchain_.txt", "file_type": "text/plain", "file_size": 6284, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "d664f1bd323a09f511d43b052931ca440349c5313ab52106f45c7af3233b6e57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b60a9dcf-19b3-4df1-ad5b-228c50b8032c", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_neo4j-x-langchain-new-vector-index_.txt", "file_name": "blog.langchain.dev_neo4j-x-langchain-new-vector-index_.txt", "file_type": "text/plain", "file_size": 9850, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "683f936b2c3f6e07ad83c9eadc352a458fd470188900e0f8e7e0312f8ce9b18c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3238c40c-e598-4c35-b4bc-a5b780a73d63", "node_type": "1", "metadata": {}, "hash": "ff4294d707c61bcce599a9624315880517ed7ffb5d84071a27b047dbe797c8ee", "class_name": "RelatedNodeInfo"}}, "hash": "f58deea3ec2d3d72ce078a38ef18ec45a33cc4ad8fa05ee3eec29f306c12f7ae", "text": "URL: https://blog.langchain.dev/neum-x-langchain/\nTitle: NeumAI x LangChain: Efficiently maintaining context in sync for AI applications\n\nEditors Note: This post was written by the NeumAI team and cross-posted from their blog. Keeping source data relevant and up-to-date efficiently is a challenge many builders are facing. It's especially painful for teams that are building on top of datasources constantly changing like team documentation (a use-case we see a lot of). Following up on our blog yesterday about making ingestion pipelines more production ready, we're really excited to highlight this because it continues in that vein. It adds scheduling and orchestration onto the ingestion pipeline, part of which is powered by LangChain text splitters.\n\nLast week, we released a blogpost about doing Q&A with thousands of documents and how Neum AI can help developers build large-scale AI apps to support that scenario. In this post, we want to dive deeper into a common problem with building large scale AI applications: Keeping context up to date in a cost-effectively way.\n\nIntro\n\nLet\u2019s set up some context first (see what we did there ;)). Data is the most important part when building AI applications. If the data you are training the model with is of low quality, then your model with perform poorly. If the data you are using for your prompts is low quality, then your model responses will not be accurate. There are many more examples on why data is important but it is really the fundamental part for bringing accuracy to our AI models.\n\nSpecifically here, let\u2019s delve in context. Many have done chatbots where a massive prompt is passed to the model. This can become problematic for a couple of reasons.\n\nYou might reach a context limit depending on the model you use The more tokens you pass the more costly your operation becomes\n\nAnd so, people have started to include context in the prompt that is fetched depending on the user\u2019s query so as to only pass a subset of relevant information to the model for it to perform accurately. This is also called Retrieval Augmented Generation (RAG). Those who have built this know what I\u2019m talking about but if you aren\u2019t you can check these two blog posts by Pinecone and LangChain for more information.\n\nSource: Pinecone docs\n\nOne problem not too many seem to talk about is how relevant this context is.\n\nImagine you are creating a chatbot over a constantly-changing data source like a restaurant\u2019s menu or some team documentation. You can easily build a chatbot with some of the libraries and tools explained in the previous post - like LangChain, Pinecone, etc. I won\u2019t go into too many details but at a high level it goes something like this:\n\nGet your source data Vectorize it using some embedding model (This is crucial so that whenever we bring context to the prompt, the \u201csearch\u201d based on the user query is done semantically and fast) Bring the context to the prompt of your model (like GPT-4 for example) and run the model. Output to the user\n\nThis poses a trivial question. What if your source data changes?\n\nIt could be that the restaurant is no longer offering an item from the menu. That an internal documents or wiki was just updated with some new content.\n\nWill our chatbot respond with high accuracy?\n\nChances are, no. Unless you have a way to give your AI model, up to date context, it probably won\u2019t know that the pepperoni pizza is no longer available or that the documentation for onboarding a new team member to the team changed. It will respond with whatever context had been stored before in the vector store (or even without any context!)\n\nChatGPT response with no context\n\n\u200d\n\nEnter Neum\n\nWith Neum we automatically synchronize your source data with your vector store. This means that whenever an AI application wants to query the vector db for semantic search or bringing context to an AI model, the information will always be updated. It is important to note that the quality of your model also depends on how you vectorize the data. At Neum, we leverage different LangChain tools to partition the source data depending on the use case.\u200d\n\nNeum pipeline builder example for syncing your data between Notion and Pinecone\n\n\u200d\n\nThese are amongst the top things that are needed when building this synchronization for your LLM data.\n\nSetting up the infrastructure required to sync the sources Setting up your scheduler or real-time pipelines to update the data Handling errors if something goes wrong at any given point And most importantly, efficiently vectorizing to reduce costs\n\nNow, let\u2019s briefly talk about costs.\n\nOpenAI embeddings pricing model currently is $0.0001/1k tokens. That might not look like much but at large scale, it translates roughly to 10k per 1TB of data. If your source data is not large, you might get away with it by constantly vectorizing and storing your data", "start_char_idx": 0, "end_char_idx": 4866, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3238c40c-e598-4c35-b4bc-a5b780a73d63": {"__data__": {"id_": "3238c40c-e598-4c35-b4bc-a5b780a73d63", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_neum-x-langchain_.txt", "file_name": "blog.langchain.dev_neum-x-langchain_.txt", "file_type": "text/plain", "file_size": 6284, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1eee1785-8b22-4586-bd15-a49a1e1c88cb", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_neum-x-langchain_.txt", "file_name": "blog.langchain.dev_neum-x-langchain_.txt", "file_type": "text/plain", "file_size": 6284, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "d664f1bd323a09f511d43b052931ca440349c5313ab52106f45c7af3233b6e57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "697fc1ef-e443-472a-9121-97a2a620f624", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_neum-x-langchain_.txt", "file_name": "blog.langchain.dev_neum-x-langchain_.txt", "file_type": "text/plain", "file_size": 6284, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "f58deea3ec2d3d72ce078a38ef18ec45a33cc4ad8fa05ee3eec29f306c12f7ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eee5711e-eb8c-4895-a2a3-31315a11ae0f", "node_type": "1", "metadata": {}, "hash": "f6e63ba79983e7a3af6824f9792b5972d6fab9dddb4e51efb48491863630f2a5", "class_name": "RelatedNodeInfo"}}, "hash": "ff4294d707c61bcce599a9624315880517ed7ffb5d84071a27b047dbe797c8ee", "text": "source data is not large, you might get away with it by constantly vectorizing and storing your data in the vector store.\n\nBut what if you have lots of documents? What if you have millions and millions of rows in your database? Vectorizing everything all the time will not only be inefficient but very costly!\n\nAt Neum, we\u2019ve developed tech to help detect differences and only vectorize the necessary information, thus keeping the context up-to-date but in an efficient and cost-saving way.\n\nSee it to believe it\n\nTo prove this we created a sample chatbot for our Notion workspace that is updated automatically as the Notion is updated with more content. It allows users to as questions and get up-to-date responses if something changed internally. The sample is built with Vercel as frontend and Pinecone as the vector store. Internally, Neum leverages LangChain for its text splitter tools.\n\nBehind the scenes, Neum is not only ensuring that updates are extracted, embedded and loaded into Pinecone, but also makes sure that we are only updating data that needs to be. If a section of the Notion workspace didn\u2019t change, we don\u2019t re-embed it. If a section changed, then it is re-embedded. This approach delivers a better user experience by having up to date data that is also more cost effective by only using resources where needed.\n\nTake a look at the 2min video below for an in-depth look of how it works!\n\nYou can reach out to founders@tryneum.com if interested in these topics!", "start_char_idx": 4766, "end_char_idx": 6250, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eee5711e-eb8c-4895-a2a3-31315a11ae0f": {"__data__": {"id_": "eee5711e-eb8c-4895-a2a3-31315a11ae0f", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_opaqueprompts-x-langchain-enhance-the-privacy-of-your-langchain-application-with-just-one-code-change_.txt", "file_name": "blog.langchain.dev_opaqueprompts-x-langchain-enhance-the-privacy-of-your-langchain-application-with-just-one-code-change_.txt", "file_type": "text/plain", "file_size": 7310, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fee773e4-ee3a-4447-bc94-b604cf9231b9", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_opaqueprompts-x-langchain-enhance-the-privacy-of-your-langchain-application-with-just-one-code-change_.txt", "file_name": "blog.langchain.dev_opaqueprompts-x-langchain-enhance-the-privacy-of-your-langchain-application-with-just-one-code-change_.txt", "file_type": "text/plain", "file_size": 7310, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "44b29f2d54962f0df650f38ce70f0b9e3bdd80cfb4bad9908375efc4d28291b4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3238c40c-e598-4c35-b4bc-a5b780a73d63", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_neum-x-langchain_.txt", "file_name": "blog.langchain.dev_neum-x-langchain_.txt", "file_type": "text/plain", "file_size": 6284, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "ff4294d707c61bcce599a9624315880517ed7ffb5d84071a27b047dbe797c8ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af026887-3667-4b25-a09b-a89a11e5ca0a", "node_type": "1", "metadata": {}, "hash": "a683940b7ac749b7a88b42b7cba1594ec7d828b036442e754a523dcede3955de", "class_name": "RelatedNodeInfo"}}, "hash": "f6e63ba79983e7a3af6824f9792b5972d6fab9dddb4e51efb48491863630f2a5", "text": "URL: https://blog.langchain.dev/opaqueprompts-x-langchain-enhance-the-privacy-of-your-langchain-application-with-just-one-code-change/\nTitle: OpaquePrompts x LangChain: Enhance the privacy of your LangChain application with just one code change\n\nEditor's Note: This blog post was written in collaboration with the Opaque team. As more apps get into production, we've been hearing more teams talk about solutions for data privacy. Opaque's seamless integration with LangChain ensures personal information in your users\u2019 prompts will be hidden from the LLM provider with just a few lines of code.\n\nWe\u2019ve been hearing growing feedback from our users that they want to keep their data private from LLM providers, whether it be OpenAI, Anthropic, Cohere, or others, for a number of reasons:\n\nConcerns about data retention\n\nConcerns about the LLM provider seeing the input data\n\nConcerns about the provider using user inputs to continually train the LLM\n\nConcerns about the LLM leaking data the model was trained on\n\nThe same is true for LLM application builders at companies of all sizes\u2014from enterprise to small startups\u2014across a variety of verticals. One startup we talked to is building a knowledge management solution that summarizes stored documents, but a potential customer, a law firm, doesn\u2019t trust third-party providers with their legal documents. Another is building an application to generate targeted advertisements based off user data, but must strictly control how personal user information is shared and used by third-party providers. A large bank wants to automate risk assessment, which, in its manual form, requires meticulous analysis of sensitive documents whose contents cannot be shared with third-party providers in its plaintext form.\n\nAll these use cases and more have one common theme: an LLM application developer wants to leverage an LLM to operate on sensitive data, but cannot do so because of concerns about or restrictions on the LLM provider\u2019s ability to see, process, and store the sensitive data. This is where OpaquePrompts comes in.\n\nAn introduction to OpaquePrompts\n\nOpaquePrompts serves as a privacy layer around your LLM of choice. With OpaquePrompts, you can:\n\nAutomatically identify sensitive tokens in your prompts with natural language processing (NLP)-based machine learning\n\nin your prompts with natural language processing (NLP)-based machine learning Pre-process LLM inputs to hide sensitive inputs in your prompts from LLM providers via a sanitization mechanism\n\nin your prompts from LLM providers via a sanitization mechanism For example, in the prompt, every instance of the name John Smith will be deterministically replaced with PERSON_1 .\n\nwill be deterministically replaced with . Post-process LLM responses to replace all sanitized instances with the original sensitive information\n\nto replace all sanitized instances with the original sensitive information For example, in the LLM response, all instances of PERSON_1 will be replaced with John Smith .\n\nwill be replaced with . Leverage the power of confidential computing to ensure that not even the OpaquePrompts service sees the underlying prompt\n\nto ensure that not even the OpaquePrompts service sees the underlying prompt OpaquePrompts runs in an attestable trusted execution environment, meaning that you can cryptographically verify that not even Opaque can see any input to OpaquePrompts.\n\nMore on OpaquePrompts architecture and security guarantees can be found in the documentation.\n\nMake your application privacy-preserving by modifying just one line of code in your LangChain application\n\nby modifying just one line of code in your LangChain application See an example here.\n\nAn application built with OpaquePrompts works as follows:\n\nThe OpaquePrompts service takes in a constructed prompt. Using a state-of-the-art model, OpaquePrompts identifies sensitive information in the prompt. OpaquePrompts sanitizes the prompt by encrypting all identified personal information before returning the sanitized prompt to the LLM application. The LLM application sends the sanitized prompt to its LLM provider of choice. The LLM application receives a response from the LLM provider, which contains the post-sanitization identifiers. The LLM application sends the response to OpaquePrompts, which de-sanitizes the response by decrypting previously encrypted personal information. The LLM application returns the de-sanitized response to the user. From the user\u2019s perspective, the response appears as if the original prompt were sent directly to the LLM.\n\nUsing GIFs, we compare LLM application workflows with and without OpaquePrompts. Without OpaquePrompts, the prompt goes directly from LLM application to the model provider, all in the clear.\n\nWith OpaquePrompts, the prompt first gets securely sanitized by the OpaquePrompts service (and the service doesn\u2019t see the contents of the prompt) before making its way to the LLM provider for a response.\n\nModifying a", "start_char_idx": 0, "end_char_idx": 4968, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "af026887-3667-4b25-a09b-a89a11e5ca0a": {"__data__": {"id_": "af026887-3667-4b25-a09b-a89a11e5ca0a", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_opaqueprompts-x-langchain-enhance-the-privacy-of-your-langchain-application-with-just-one-code-change_.txt", "file_name": "blog.langchain.dev_opaqueprompts-x-langchain-enhance-the-privacy-of-your-langchain-application-with-just-one-code-change_.txt", "file_type": "text/plain", "file_size": 7310, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fee773e4-ee3a-4447-bc94-b604cf9231b9", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_opaqueprompts-x-langchain-enhance-the-privacy-of-your-langchain-application-with-just-one-code-change_.txt", "file_name": "blog.langchain.dev_opaqueprompts-x-langchain-enhance-the-privacy-of-your-langchain-application-with-just-one-code-change_.txt", "file_type": "text/plain", "file_size": 7310, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "44b29f2d54962f0df650f38ce70f0b9e3bdd80cfb4bad9908375efc4d28291b4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eee5711e-eb8c-4895-a2a3-31315a11ae0f", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_opaqueprompts-x-langchain-enhance-the-privacy-of-your-langchain-application-with-just-one-code-change_.txt", "file_name": "blog.langchain.dev_opaqueprompts-x-langchain-enhance-the-privacy-of-your-langchain-application-with-just-one-code-change_.txt", "file_type": "text/plain", "file_size": 7310, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "f6e63ba79983e7a3af6824f9792b5972d6fab9dddb4e51efb48491863630f2a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5bdf1930-b446-42d2-b806-4396d68a5c2c", "node_type": "1", "metadata": {}, "hash": "d494e2c99b97a9a6c17dda6dd75b0f9251cca3dba4ca2474b485367a3c07e295", "class_name": "RelatedNodeInfo"}}, "hash": "a683940b7ac749b7a88b42b7cba1594ec7d828b036442e754a523dcede3955de", "text": "of the prompt) before making its way to the LLM provider for a response.\n\nModifying a chatbot built with LangChain to incorporate OpaquePrompts\n\nBelow, we walk through how we modified an existing GPT-based chat application built with LangChain to hide sensitive information from prompts sent to OpenAI.\n\nThe server-side with a /chat endpoint of a vanilla chat application looks something like the following.\n\n# Full source code can be found here: <https://github.com/opaque-systems/opaqueprompts-chat-server> class ChatRequest(BaseModel): history: Optional[list[str]] prompt: str class ChatResponse(BaseModel): response: str async def chat( chat_request: ChatRequest, ) -> ChatResponse: \"\"\" Defines an endpoint that takes in a prompt and sends it to GPT Parameters ---------- chat_request : ChatRequest The request body, which contains the history of the conversation and the prompt to be completed. Returns ------- ChatResponse The response body, which contains GPT's response to the prompt. \"\"\" # Actual template and build_memory logic are omitted and can be found in the # repo linked below prompt = PromptTemplate.from_template(CHAT_TEMPLATE) memory = build_memory(chat_request.history) chain = LLMChain( prompt=prompt, llm=OpenAI(), memory=memory, ) return ChatResponse(response=chain.run(chat_request.prompt))\n\nTo use OpaquePrompts, once we retrieve an API token from the OpaquePrompts website, all we have to do is wrap the llm passed into LLMChain with OpaquePrompts :\n\nchain = LLMChain( prompt=prompt, # llm=OpenAI(), llm=OpaquePrompts(base_llm=OpenAI()), memory=memory, )\n\nYou can play with a working implementation of a chatbot built with LangChain and OpaquePrompts on the OpaquePrompts website, and find the full source code from which we derived the example above on GitHub. Note that the source code also includes logic for authentication and for displaying intermediate (i.e., the sanitized prompt and sanitized response) steps.\n\nConclusion\n\nWith OpaquePrompts, you can bootstrap your existing LangChain-based application to add privacy for your users. With your OpaquePrompts + LangChain application, any personal information in your users\u2019 prompts will be hidden from the LLM provider, ensuring that you, as the LLM application developer, do not have to worry about the provider\u2019s data retention or processing policies. Take a look at the documentation or try out OpaquePrompts Chat today!", "start_char_idx": 4883, "end_char_idx": 7290, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5bdf1930-b446-42d2-b806-4396d68a5c2c": {"__data__": {"id_": "5bdf1930-b446-42d2-b806-4396d68a5c2c", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt", "file_name": "blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt", "file_type": "text/plain", "file_size": 14087, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "295d2839-53d5-44dc-a40b-4fdcff61c08d", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt", "file_name": "blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt", "file_type": "text/plain", "file_size": 14087, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "6e6b6d67611c6c503db8f49125ca86e4f529847a998fb6a82c277812277ce60f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af026887-3667-4b25-a09b-a89a11e5ca0a", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_opaqueprompts-x-langchain-enhance-the-privacy-of-your-langchain-application-with-just-one-code-change_.txt", "file_name": "blog.langchain.dev_opaqueprompts-x-langchain-enhance-the-privacy-of-your-langchain-application-with-just-one-code-change_.txt", "file_type": "text/plain", "file_size": 7310, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "a683940b7ac749b7a88b42b7cba1594ec7d828b036442e754a523dcede3955de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "551b68f8-c29f-4abf-9b6e-ab446c536438", "node_type": "1", "metadata": {}, "hash": "3bc5484e388497d4f93446d5cfc721e4f3cae4247d3478e016f0b202de8ab4c3", "class_name": "RelatedNodeInfo"}}, "hash": "d494e2c99b97a9a6c17dda6dd75b0f9251cca3dba4ca2474b485367a3c07e295", "text": "URL: https://blog.langchain.dev/peering-into-the-soul-of-ai-decision-making-with-langsmith/\nTitle: Peering Into the Soul of AI Decision-Making with LangSmith\n\nEditor's Note: This post was written by Paul Thomson from Commandbar. They've been awesome partners as they brought their application into production with LangSmith, and we're excited to share their story getting there.\n\nDo you ever wonder why you\u2019re getting unhinged responses from ChatGPT sometimes? Or why the heck Midjourney is giving your creations 7 weird fingers? As intelligent as AI is supposed to be, it does produce some pretty unintelligent responses sometimes.\n\nNow, if you\u2019re using GPT to write your next \u201clet \u2018em down easy breakup message\u201d, the stakes are low - it doesn\u2019t really matter. But if a core product feature is leveraging AI and your customers depend on super-intelligent perfection, you\u2019re going to want some security and assurances that the outputs are up to scratch. Enter, LangSmith.\n\nSince the launch of HelpHub, we were trying to do things on hard mode when it came to iterating and improving functionality. That is, of course, until the LangChain team tantalized us onto their LangSmith beta. What we didn\u2019t expect was how immediate the downstream improvements were to our flagship AI-powered product.\n\nBut is LangSmith robust enough for us to rely on entirely for our LLM-powered QA? Or is it just another nice-to-have feature for our ENG team?\n\nIf you\u2019re at the intersection of product, LLMs, and user experience, we\u2019ve just walked so you can run. Time to read on.\n\nWhat Is LangSmith?\n\nLangSmith is a framework built on the shoulders of LangChain. It\u2019s designed to track the inner workings of LLMs and AI agents within your product.\n\nThose LLM inner-workings can be categorized into 4 main buckets - each with its own flair of usefulness. Here\u2019s a breakdown of how they all work in unison and what you can expect.\n\n\n\nDebugging:\n\nWhen your LLM starts throwing curveballs instead of answers, you don't just want to sit there catching them. With LangSmith, you can roll up your sleeves and play detective. We use the debugging tools to dive into perplexing agent loops, frustratingly slow chains, and to scrutinize prompts like they're suspects in a lineup.\n\nTesting:\n\nTesting LLM applications without LangSmith is like trying to assemble IKEA furniture without the manual: sure, you could wing it, but do you really want to risk it? Baked into LangSmith is the option to utilize existing datasets or create new ones, and run them against your chains. Visual feedback on outputs and accuracy metrics are presented within the interface, streamlining the testing process for our eng team (we really like this).\n\nEvaluating:\n\nBeyond mere testing, evaluation in LangSmith delves into the performance nuances of LLM runs. While the built-in evaluators offer a preliminary analysis, the true power lies in guiding your focus towards crucial examples (more on how we do that later). As your datasets grow, LangSmith ensures you never miss a beat, making evaluations both comprehensive and insightful. Because \"good enough\" isn't in your vocabulary, right?\n\nMonitoring:\n\nThink of LangSmith's monitoring as your AI\u2019s babysitter: always vigilant, never distracted, and ready to report every little mischief. It'll give you the play-by-play, ensure everything's in order, and notify you if things get out of hand. We even went a step ahead and piped these flags directly into Slack giving us almost realtime monitoring when our users hit a deadend with chat conversations.\n\nhttps://twitter.com/zhanghaili0610\n\nLangChain vs LangSmith: What\u2019s the difference?\n\nWhile LangChain is the muscle doing the heavy lifting with Chains, Prompts, and Agents, understanding the 'why' behind the decisions LLMs make is a maze we often found ourselves lost in. That's where LangSmith shines, acting as an AI compass built into LangChain, guiding us through the intricate decision pathways and results that our chatbot generates.\n\n\"LangChain's (the company's) goal is to make it as easy as possible to develop LLM applications\"\n\nsaid Harrison Chase, co-founder and CEO of LangChain.\n\n\"To that end, we realized pretty early that what was needed - and missing - wasn't just an open source tool like LangChain, but also a complementary platform for managing these new types of applications. To that end, we built LangSmith - which is usable with or without LangChain and let's users easily debug, monitor, test, evaluate, and now (with the recently launched Hub) share and collaborate on their LLM applications.\u201d\n\n\n\nWhat Are LangSmith Traces?\n\nTraces in the world of LangSmith are analogous to logs when programming; they allow us", "start_char_idx": 0, "end_char_idx": 4689, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "551b68f8-c29f-4abf-9b6e-ab446c536438": {"__data__": {"id_": "551b68f8-c29f-4abf-9b6e-ab446c536438", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt", "file_name": "blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt", "file_type": "text/plain", "file_size": 14087, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "295d2839-53d5-44dc-a40b-4fdcff61c08d", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt", "file_name": "blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt", "file_type": "text/plain", "file_size": 14087, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "6e6b6d67611c6c503db8f49125ca86e4f529847a998fb6a82c277812277ce60f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5bdf1930-b446-42d2-b806-4396d68a5c2c", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt", "file_name": "blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt", "file_type": "text/plain", "file_size": 14087, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "d494e2c99b97a9a6c17dda6dd75b0f9251cca3dba4ca2474b485367a3c07e295", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8de519fa-e912-479f-936d-1e0df2cad1f9", "node_type": "1", "metadata": {}, "hash": "764f7120d2662e94a0d975b9da01dfc3d3e89e889b5d1478728f9bb94925e0f3", "class_name": "RelatedNodeInfo"}}, "hash": "3bc5484e388497d4f93446d5cfc721e4f3cae4247d3478e016f0b202de8ab4c3", "text": "in the world of LangSmith are analogous to logs when programming; they allow us to easily see what text came in and out of chains and LLMs. Think of them as detailed breadcrumbs illuminating the AI's journey. Each trace, like a footprint on a sandy beach, represents a pivotal AI decision. Traces don't merely depict the path taken; they shed light on the underlying thought process and actions taken at each juncture.\n\nHere\u2019s what one of our traces looks like inside LangSmith:\n\nAll the individual traces are consolidated into datasets:\n\n\n\nDo You Really Need To Use LangSmith?\n\nWhen generative AI works, it feels like watching a viral \u201csatisfying video\u201d montage - so delightful. But when it doesn\u2019t, it sucks, and sometimes it sucks real bad.\n\nTake it from us, as we integrated AI more heavily and widely across our products, we\u2019ve been more conscious than ever that the quality of the outputs matches the quality and trust that our customers have in our product. G2 review flex here.\n\nTruth is, until we powered up LangSmith, we truly had no way of postmorteming responses from OpenAI or testing how prompt changes, or even upgrading to a new model like GPT-4, would affect the answers.\n\nHow We Use LangSmith at CommandBar: AI-Powered User Assistance\n\nWe\u2019ve covered a lot of ground so far on how LangSmith works. From here on out, we\u2019re ripping the covers off and showing you what\u2019s under the hood of our HelpHub <> LangSmith setup. To give you a little context, first, let\u2019s dive into what HelpHub is.\n\nHelpHub is a GPT-Powered chatbot for any site. It syncs with any public URL or your existing help center to assist users in getting instantaneous support while circumventing customer service teams or manually screening docs.\n\nWhile we utilize our own search with ElasticSearch, we rely on LangChain to merge those search results into meaningful prompts for HelpHub. This synergy allows us to search hundreds of docs and sites in milliseconds, source relevant context, compute reasoning, and deliver an answer to our users (with citations!) almost instantly.\n\nIt\u2019s through this core integration with LangChain that we\u2019re able to capture traces via LangSmith. primarily to finetune and optimize our chatbot\u2019s functionality and sandbox future improvements for our users.\n\nAccurate AI Responses Are A Necessity, Not A Nice-To-Have\n\nWe pride ourselves on offering precise and relevant answers to user queries and that has always been a strong USP for us. However, with the aforementioned challenges of unhinged AI-generated responses not always aligning with user expectations, once we flicked on LangSmith, we took our prototyping and QA from mediocre guesswork to David Blane quality witchcraft.\n\nSince its integration, LangSmith has traced over X0 million tokens for HelpHub (about XM tokens a week!).\n\nReal-world Example of LangSmith In Our Production\n\nBelow is an example from Gus, one of our mighty talented LangSmith connoisseurs, caught in one of our traces.\n\nWhat he\u2019s referring to in the screenshot is the fact that each prompt from HelpHub should reference the source document that it\u2019s referencing when giving users an answer. We do this primarily to legitimize the LLMs response and give our HelpHub customer\u2019s peace of mind that their end users are in fact getting to the help resource they need (instead of an LLM just hallucinating and giving any response it wants.)\n\nFrom here, we went into LangSmith and saw that the LLM actually returned no source, even though we asked it to. Ideally, the source should be returned on the first line in the \u201cOutput\u201d section above the actual answer.\n\nWe updated our prompt to be more firm when asking for the sources:\n\nPreviously the snippet in the prompt responsible for this was: Return the source ID with the most relevance to your answer.\n\nWe update that piece of the prompt to: ALWAYS return the source ID with the most relevance to your answer prior to answering the question .\n\nWe then tested everything using LangSmith evals, to make sure that it fixes the issue before pushing to production.\n\nou can now clearly see the citations coming through with the responses in the traces, and we\u2019re good to ship the changes to the prompt to prod.\n\nThe Verdict: Are We Betting The House On LangSmith?\n\nWhen a product like LangSmith comes along, it can feel really natural to default to the path of least resistance and offhand all responsibility. As we start to add additional functionality to HelpHub, there\u2019s an inherent risk that GPT is going to lead users astray, and that\u2019s just not an option we\u2019re willing to entertain.\n\nSo, in short, yes, we are putting a lot of trust right now in LangSmith and scaling our prototyping and debugging rapidly. The systems we\u2019re building internally have already been instrumental", "start_char_idx": 4610, "end_char_idx": 9379, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8de519fa-e912-479f-936d-1e0df2cad1f9": {"__data__": {"id_": "8de519fa-e912-479f-936d-1e0df2cad1f9", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt", "file_name": "blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt", "file_type": "text/plain", "file_size": 14087, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "295d2839-53d5-44dc-a40b-4fdcff61c08d", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt", "file_name": "blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt", "file_type": "text/plain", "file_size": 14087, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "6e6b6d67611c6c503db8f49125ca86e4f529847a998fb6a82c277812277ce60f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "551b68f8-c29f-4abf-9b6e-ab446c536438", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt", "file_name": "blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt", "file_type": "text/plain", "file_size": 14087, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "3bc5484e388497d4f93446d5cfc721e4f3cae4247d3478e016f0b202de8ab4c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d69a9195-cfb0-4605-8d1c-4cb506908f46", "node_type": "1", "metadata": {}, "hash": "33f63a52cfdd262c3b1e2b7d29472643651a9b29d9aa43b15f4b5041b9d2a4a6", "class_name": "RelatedNodeInfo"}}, "hash": "764f7120d2662e94a0d975b9da01dfc3d3e89e889b5d1478728f9bb94925e0f3", "text": "and scaling our prototyping and debugging rapidly. The systems we\u2019re building internally have already been instrumental in improving user experience, and as you\u2019ve read earlier, many of these insights and improvements have come directly from those real-time traces from users chatting with HelpHub in the wild.\n\nLeveraging User Feedback For Improvements:\n\nWe believe that every piece of user feedback, whether positive or negative, is a goldmine of insights. And with LangSmith's help (plus a little ingenuity on our side), we've turned these insights into actionable improvements.\n\nHere's how our feedback loop works:\n\nReal-time Feedback Collection: As users interact with HelpHub, they have the opportunity to provide feedback on the AI-generated responses, signaling with a simple \"thumbs up\" or \"thumbs down\". In-depth Analysis with LangSmith: Instead of just collecting feedback in the form or positive or negative signals, we delve deeper (particularly for the negative signals). Using LangSmith, we\u2019re able to attribute each signal to an individual trace. In that trace, we can map the exact sequence of steps the model took to generate that response. We essentially replay GPT's thought process and LangChain\u2019s actions, giving us the insights into what went right and where it veered off track. Categorizing Into Datasets: Central to our refinement process is LangSmith's use of dynamic datasets. We maintain multiple datasets, each tailored to different query types and user interactions. These datasets are essentially compilations of identical states of our environment at the time the trace was captured. This ensures that when there's an update to the prompt or LLM, a new dataset starts to compile those traces, preventing any contamination. Automating ENG-team Signals: When a user provides feedback, say a thumbs down, it's immediately flagged to our team via Slack. We built this little snippet to help the team screen traces and prioritize the ones that need attention right away. Iterating Quickly: We rigorously review the feedback, analyze the corresponding traces, and then, based on our insights, make informed adjustments to the model's role, prompts, or configurations to try and curb whatever jankiness was happening. This iterative process ensures our AI chatbot is continually refining its understanding, resonating more with user needs, and exceeding expectations over time.\n\nBy combining granular AI insights through LangSmith with user feedback, we\u2019ve created a pretty tight loop of perpetual improvement with HelpHub. This was such an important unlock for us as we build in tactical functionality to our AI.\n\n\n\nAdvice for Product Teams Considering LangSmith\n\nWhen you're in the thick of product development, the whirlwind of AI and LLMs can be overwhelming. We've been there, boots on the ground, making sense of it all. From our journey with LangSmith and HelpHub, here's some hard-earned wisdom for fellow product teams.\n\nStart with Data, and Start Now:\n\nAI thrives on data. But don\u2019t wait for the 'perfect' moment or the 'perfect' dataset. Start collecting now. Setting up LangSmith takes literally 5 minutes if you\u2019re already using LangChain. Every bit of data, every interaction, adds a layer of understanding. But, a word to the wise: quality matters. Make sure the data reflects real-world scenarios, ensuring your AI resonates with genuine user needs.\n\nDive Deep with Traces: Don't just skim the surface. Use LangSmith's trace feature to dive deep into AI decision-making. Every trace is a lesson, a chance to improve. Experiment with Prompts: One of LangSmith's standout features is its ability to test a new prompt across multiple examples without manual entry each time. This makes it incredibly efficient to iterate on your setup, ensuring you get the desired output from the AI. Note, in addition, the Playground is also an amazing tool to dig around with for testing prompts and adjustments to traces too. Lean on the Community: There's a whole community of LangSmith users out there. Swap stories, share challenges, and celebrate successes. You're not alone on this journey. Stay on Your Toes: AI doesn\u2019t stand still, and neither should you. Keep an eye on LangSmith's updates. New features? Dive in. Test, iterate, refine.\n\nConclusion\n\nAfter diving deep with LangSmith's traces, experimenting with prompts, testing, and iterating on our LLM environment, here's the real talk: LangSmith isn't just a tool for us - it's become a critical inclusion in our stack. We've moved from crossing our fingers and toes hoping our AI works to knowing exactly how and why.\n\nSo, to our fellow AI product people trailblazers, dive into LangSmith. You\u2019d be silly not to if you\u2019re already using LangChain!", "start_char_idx": 9260, "end_char_idx": 13995, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d69a9195-cfb0-4605-8d1c-4cb506908f46": {"__data__": {"id_": "d69a9195-cfb0-4605-8d1c-4cb506908f46", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_realchar-x-langsmith-ai-companions_.txt", "file_name": "blog.langchain.dev_realchar-x-langsmith-ai-companions_.txt", "file_type": "text/plain", "file_size": 3131, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "05861ae7-c95f-4e12-aa50-ec6f2863cfa7", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_realchar-x-langsmith-ai-companions_.txt", "file_name": "blog.langchain.dev_realchar-x-langsmith-ai-companions_.txt", "file_type": "text/plain", "file_size": 3131, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "948005c5160e89e345fa7209781b967cf8b3a92a47d0c4865f493ab84f4a094e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8de519fa-e912-479f-936d-1e0df2cad1f9", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt", "file_name": "blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt", "file_type": "text/plain", "file_size": 14087, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "764f7120d2662e94a0d975b9da01dfc3d3e89e889b5d1478728f9bb94925e0f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21a243d7-fae5-49e5-8236-c36ad36971b5", "node_type": "1", "metadata": {}, "hash": "1502ebce179f546f0d8aa9004250e0efe79527e26599462a48863dca34b8b41a", "class_name": "RelatedNodeInfo"}}, "hash": "33f63a52cfdd262c3b1e2b7d29472643651a9b29d9aa43b15f4b5041b9d2a4a6", "text": "URL: https://blog.langchain.dev/realchar-x-langsmith-ai-companions/\nTitle: RealChar x LangSmith: Using Open Source tools to create an AI companion\n\nEditor\u2019s Note: This blog post was written in collaboration with RealChar, an early LangSmith BETA user. They moved fast and created something really, really sophisticated and really, really fun to use\u2013all with open source tools.\n\nWe're also very excited about AI characters and companions internally, which is part of the reason we're excited to highlight RealChar. As seen by the meteoric rise of platforms like CharacterAI, allowing people to converse with different personas can be really fun.\n\nRealChar may be the most complete and most exciting OSS AI character framework out there. Besides impressive underlying technology, it also offers a really polished UI and UX. They were one of the top trending GitHub repos for basically all of last week, and we'd highly recommend that you check it out if you haven't already.\n\nWe (RealChar team) are pleased to share our experience using LangSmith and working with LangChain team.\n\nIn case you don\u2019t know, RealChar is an open source project to let you create, customize and talk to your AI character/companion in realtime (all in one codebase). We offer users natural and seamless conversations with AI on all the common platforms (mobile, web, terminal and desktop soon). We built RealChar leveraging some of best open source tools in the Generative AI/LLM space, including LangChain.\n\nJust a fun demo: asking AI Elon about whether he is afraid of losing in the much anticipated cage fight. Full version here.\n\nRealChar received a ton of attention and usage from the community after releasing it just a week ago, and our site has undergo significant traffic. With conversations piling up and logs get cluttered very quickly, we found LangSmith to be a perfect tool for us to monitor and observe the traffic.\n\nIt\u2019s also easy to filter logs easily based on various conditions, to allow us track issues more accurately. For example, we can easily see all the errors when interacting with the Language Model, which has helped us understand and maintain our reliability better.\n\nLangSmith also allows us to identify important conversations and add to dataset easily. This is then helpful for us to evaluate and safe checking the prompts going forward, using the Evaluation features of LangSmith.\n\nThe UI of LangSmith is also top-notch and easy to work with. It largely replaced our monitoring tools previously built in-house.\n\nAll these features are almost free to get as we already use LangChain. As soon as the API Key are set up in LangSmith, only a few environment variables are needed:\n\nLANGCHAIN_TRACING_V2=true LANGCHAIN_ENDPOINT=https://api.smith.langchain.com LANGCHAIN_API_KEY=YOUR_LANGCHAIN_API_KEY LANGCHAIN_PROJECT=YOUR_LANGCHAIN_PROJECT\n\nOverall, we see LangSmith as a great tool for Analytics, Observability, and Evaluation, all in one place. It\u2019s very useful for a production-level application with large volume of traffic like RealChar.\n\n/content/media/5101573/253656635-5de0b023-6cf3-4947-84cb-596f429d109e.mp4", "start_char_idx": 0, "end_char_idx": 3121, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "21a243d7-fae5-49e5-8236-c36ad36971b5": {"__data__": {"id_": "21a243d7-fae5-49e5-8236-c36ad36971b5", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_streamlit-llm-hackathon-kickoff-and-projects-wed-love-to-see-2_.txt", "file_name": "blog.langchain.dev_streamlit-llm-hackathon-kickoff-and-projects-wed-love-to-see-2_.txt", "file_type": "text/plain", "file_size": 3128, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4dbac9a-42a7-4e29-8602-358df8e42956", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_streamlit-llm-hackathon-kickoff-and-projects-wed-love-to-see-2_.txt", "file_name": "blog.langchain.dev_streamlit-llm-hackathon-kickoff-and-projects-wed-love-to-see-2_.txt", "file_type": "text/plain", "file_size": 3128, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "b8c2a68f76cd4073f82c04ef553904000ed6666bd1b64b65df54c9ecfa38208e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d69a9195-cfb0-4605-8d1c-4cb506908f46", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_realchar-x-langsmith-ai-companions_.txt", "file_name": "blog.langchain.dev_realchar-x-langsmith-ai-companions_.txt", "file_type": "text/plain", "file_size": 3131, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "33f63a52cfdd262c3b1e2b7d29472643651a9b29d9aa43b15f4b5041b9d2a4a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4060ea8c-9755-4421-9c1c-e6e2f13ba1a3", "node_type": "1", "metadata": {}, "hash": "f614f766a1c42ab27eea924cba4f6d4da3b06fda23193112172f8d582a862886", "class_name": "RelatedNodeInfo"}}, "hash": "1502ebce179f546f0d8aa9004250e0efe79527e26599462a48863dca34b8b41a", "text": "URL: https://blog.langchain.dev/streamlit-llm-hackathon-kickoff-and-projects-wed-love-to-see-2/\nTitle: Streamlit LLM Hackathon Kickoff (and projects we\u2019d love to see)\n\nStreamlit\u2019s LLM Hackathon kicks off today and we\u2019re thrilled to be partnering with them to bring it to life. We\u2019ve been building with the Streamlit team since LangChain\u2019s inception because it\u2019s the easiest place to get started building LLM apps and we can\u2019t wait to see what everyone builds this week.\n\nAs you prepare to get going, we thought we\u2019d share some resources and ideas for apps that we\u2019d love to see (and share out with the broader community!).\n\nGetting Started with LangChain and Streamlit\n\nHere\u2019s a repo including reference implementations of several LangChain agents as Streamlit apps including:\n\na search enabled chatbot that remembers chat history (app here)\n\na chat app that allows the user to add feedback on responses using Streamlit feedback and link to the traces in LangSmith (we\u2019ve been using this one a lot lately)\n\na chatbot capable of answering queries by referring custom documents (app here)\n\na chatbot which can communicate with your database (app here)\n\nlots more\n\nWe would LOVE to see teams contributing to this this week\u2013PRs very welcome! \ud83d\ude80\n\nHere\u2019s a LangSmith cookbook on building a Streamlit Chat UI with LangSmith. It shows you step-by-step how to create a ChatGPT-like web app in Streamlit that supports:\n\nstreaming\n\ncustom instructions\n\napp feedback (including a template that lets you log simple \ud83d\udc4d\ud83d\udc4e scores to runs in LangSmith to make user feedback easier to incorporate.\n\nand more\n\nHere\u2019s some LLM app inspiration from the Streamlit blog, including:\n\nHere\u2019s our Getting Started documentation (including a scenario on how to create an agent with tools).\n\nLangChain Prompt Hub\n\nWe recently launched LangChain Hub, a home for submitting, discovering, inspecting, and remixing prompts. It\u2019s still (very) early days\u2013we see it as not only a useful tool for helping developers build faster, but also as a way for the entire community to get collectively smarter on prompting overall.\n\nWe hope you\u2019ll contribute prompts and try each other\u2019s out\u2013here\u2019s how: https://docs.smith.langchain.com/hub/quickstart\n\nWe\u2019ll be tweeting and blogging about our favorites prompts\u2013from most useful to most creative\u2013so make sure you share them with us and the community on Twitter!\n\nPlease share feedback along the way\u2013anything from bugs to big ideas welcome.\n\nProjects we\u2019d love to see (and share with the LangChain community!)\n\napplications that use open source models\n\ninnovative retrieval tactics\n\nmost practical agents\n\nmost adventurous agents\n\napps that connect unique data source/format(s)\n\nWe\u2019ll be tweeting out our favorite projects throughout the week and collecting them for a blog post to close out the Hackathon. There may even be some (unofficial) LangChain prizes/swag for teams doing this stuff! \ud83d\udc40\n\nTag us on Twitter (@langchainai), Send us a note at hello@langchain.dev, or ping us in the Steamlit Discord (I\u2019m hwchase17).\n\nWe can\u2019t wait to see what you build!", "start_char_idx": 0, "end_char_idx": 3058, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4060ea8c-9755-4421-9c1c-e6e2f13ba1a3": {"__data__": {"id_": "4060ea8c-9755-4421-9c1c-e6e2f13ba1a3", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_student-hacker-in-residence-fall-23_.txt", "file_name": "blog.langchain.dev_student-hacker-in-residence-fall-23_.txt", "file_type": "text/plain", "file_size": 2237, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b35a746-2334-4af3-b700-9e7451d7e037", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_student-hacker-in-residence-fall-23_.txt", "file_name": "blog.langchain.dev_student-hacker-in-residence-fall-23_.txt", "file_type": "text/plain", "file_size": 2237, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "4807ca8889f732a8aa62c267babc13a72019deed926030c892dff34edf99a458", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21a243d7-fae5-49e5-8236-c36ad36971b5", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_streamlit-llm-hackathon-kickoff-and-projects-wed-love-to-see-2_.txt", "file_name": "blog.langchain.dev_streamlit-llm-hackathon-kickoff-and-projects-wed-love-to-see-2_.txt", "file_type": "text/plain", "file_size": 3128, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "1502ebce179f546f0d8aa9004250e0efe79527e26599462a48863dca34b8b41a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7c31deb-e039-4a95-a056-61f085f667ae", "node_type": "1", "metadata": {}, "hash": "029a57eb2bdbe00448087c37950b9574d76c170bdea619266c4628c122acd353", "class_name": "RelatedNodeInfo"}}, "hash": "f614f766a1c42ab27eea924cba4f6d4da3b06fda23193112172f8d582a862886", "text": "URL: https://blog.langchain.dev/student-hacker-in-residence-fall-23/\nTitle: Announcing our Student Hacker in Residence Program, Fall '23 Semester\n\nToday, we're opening up applications for our inaugural student hacker in residence program.\n\nWe're looking for 3-5 students to work alongside the LangChain team this semester to build LLM applications that are creative, useful, thought-proving, and probably a little weird.\n\nHow it works\n\nMostly just build. We'll work together to pick the right project. From there, your work will be largely self-guided.\n\nWe'll work together to pick the right project. From there, your work will be largely self-guided. Build with the LangChain team. We'll will be available to unblock and collaborate as you need us. You'll share feedback and ideas that will help us make LangChain better.\n\nWe'll will be available to unblock and collaborate as you need us. You'll share feedback and ideas that will help us make LangChain better. Get your app app in front of our community. We'll also collaborate on a blog post about your experience building that can help teach and inspire other builders.\n\nWe'll also collaborate on a blog post about your experience building that can help teach and inspire other builders. Meet other builders. You'll meet and trade tips with not only Student Hackers in Residence from other schools, but also professional developers and builders in our broader community.\n\nYou'll meet and trade tips with not only Student Hackers in Residence from other schools, but also professional developers and builders in our broader community. Stipend. $7,500 for 8 weeks (expectation is a minimum of 10 hours a week)\n\nApply\n\nWe're looking for full-time students that have built and shipped demos and apps before. The application will ask for an example of an application you're proud of and an idea for something you'd like to build.\n\nApply here.\n\nIf you have any questions about the program or candidacy, email brie@langchain.dev.\n\nProject Ideas\n\nThis is a non-exhaustive list of the kinds of applications we hope our student hackers will build. Some will be apps that our community can use and others might be put to use by our team internally. We're also interested in apps you'd propose!", "start_char_idx": 0, "end_char_idx": 2237, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7c31deb-e039-4a95-a056-61f085f667ae": {"__data__": {"id_": "e7c31deb-e039-4a95-a056-61f085f667ae", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model_.txt", "file_name": "blog.langchain.dev_summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model_.txt", "file_type": "text/plain", "file_size": 10301, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c4650e72-2e50-44fa-8278-a4e922d08f99", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model_.txt", "file_name": "blog.langchain.dev_summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model_.txt", "file_type": "text/plain", "file_size": 10301, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "95a7decef3be7043c8675572aede2b359afdc8385e1f76fca96db67a16126d20", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4060ea8c-9755-4421-9c1c-e6e2f13ba1a3", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_student-hacker-in-residence-fall-23_.txt", "file_name": "blog.langchain.dev_student-hacker-in-residence-fall-23_.txt", "file_type": "text/plain", "file_size": 2237, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "f614f766a1c42ab27eea924cba4f6d4da3b06fda23193112172f8d582a862886", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab5ee918-c0b4-4cf7-8b11-d92b6c737281", "node_type": "1", "metadata": {}, "hash": "c0f11a78612eee9223c6204d508a7b04cd6af83914894d357cb4d3e3803e7559", "class_name": "RelatedNodeInfo"}}, "hash": "029a57eb2bdbe00448087c37950b9574d76c170bdea619266c4628c122acd353", "text": "URL: https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/\nTitle: Summarizing and Querying Data from Excel Spreadsheets Using eparse and a Large Language Model\n\nEditor's Note: This post was written by Chris Pappalardo, a Senior Director at Alvarez & Marsal, a leading global professional services firm. The standard processes for building with LLM work well for documents that contain mostly text, but do not work as well for documents that contain tabular data (like spreadsheets). We wrote about our latest thinking on Q&A over csvs on the blog a couple weeks ago, and we loved reading Chris's exploration of working with csvs and LangChain using agents, chains, RAG, and metadata. Lots of great learnings in here!\n\nWhen I first sat down to write eparse, the objective was to create a library that could crawl and parse a large set of Excel files and extract information in context into storage for later consumption. To this end, we were fairly successful \u2013 eparse can extract sub-tabular information using a rules-based search algorithm and store labeled cells as rows in a database. Assuming the user has a good idea of what is contained in the source files, SQL queries or the eparse CLI can be used to retrieve specific data.\n\nHowever, document Extraction, Transformation, and Loading (\u201cETL\u201d) activities are becoming more generative AI-oriented to facilitate activities like document summarization and Retrieval-Augmented Generation (\u201cRAG\u201d). Given that most documents in question mostly contain text, which Large Language Models (\u201cLLMs\u201d) are well suited for, many of the ETL tools were built for this case. A typical \u201cquickstart\u201d workflow for these purposes is as follows:\n\nFigure 1 - Typical AI-oriented ETL Workflow (source: langchain.com).\n\nThe process begins with using an ETL tool set like unstructured, which identifies the document type, extracts content as text, cleans the text, and returns one or more text elements. A second library, in this case langchain, will then \u201cchunk\u201d the text elements into one or more documents that are then stored, usually in a vectorstore such as Chroma. Finally, an LLM can be used to query the vectorstore to answer questions or summarize the content of the document.\n\nThis process works well for documents that contain mostly text. It does not work well for documents that contain mostly tabular data, such as spreadsheets.\n\nTo better understand this problem, let\u2019s consider an example. In the eparse code repository there is a unit test data file called eparse_unit_test_data.xlsx which contains the following sub-tables, each with different types of financial data:\n\nFigure 2 - Financial Statement Data by Date with Duplicate Columns\n\nFigure 3 - Principal Repayment Schedules for Multiple Unidentified Debt Instruments\n\nFor this demonstration, I wrote a Gradio app to display the extracted and chunked text data so it is easier to figure out what the libraries are doing behind the scenes. If we use unstructured and langchain without any modifications, the ETL workflow would produce text chunks from this file that look as follows:\n\nFigure 4 - Extracted Data from Figure 2 Spreadsheet Table in Gradio\n\nUnstructured produces a single text element which LangChain chunks up into 14 pieces, with the 3rd piece (\u201c3 \u2013 Document\u201d) containing the first sub-table I depicted above. Each cell in this table is a separate line, and the 3rd piece contains about 40 lines, which is not the entire table.\n\nWhen I first tried to ask an LLM to summarize the document using the vectorstore, I received a context window overrun error due to the number of tokens (loosely words) exceeding the LLM\u2019s context window size (in this case 2k tokens). This is a common problem with working with LLMs, which I will touch on later in the article. So, to handle that problem, I used a larger context window LLM running on a bigger server and extended the API timeout to 10 minutes. This time we get a decent result:\n\nFigure 5 - Summarization Using a Large Context LLM with a Default Implementation\n\nWith default implementations, the LLM understood some aspects of the file but did not get a good general sense of the content. Also, the amounts are off, the loan amount appears to be 10x higher than the amount in the file, ignores the other loans, and the LLM is misunderstanding the unformatted Excel date value (a 44,621 day maturity would be 122+ years).\n\nHow does eparse perform on the same task?\n\neparse does things a little differently. Instead of passing entire sheets to LangChain, eparse will find and pass sub-tables, which appears", "start_char_idx": 0, "end_char_idx": 4634, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab5ee918-c0b4-4cf7-8b11-d92b6c737281": {"__data__": {"id_": "ab5ee918-c0b4-4cf7-8b11-d92b6c737281", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model_.txt", "file_name": "blog.langchain.dev_summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model_.txt", "file_type": "text/plain", "file_size": 10301, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c4650e72-2e50-44fa-8278-a4e922d08f99", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model_.txt", "file_name": "blog.langchain.dev_summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model_.txt", "file_type": "text/plain", "file_size": 10301, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "95a7decef3be7043c8675572aede2b359afdc8385e1f76fca96db67a16126d20", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7c31deb-e039-4a95-a056-61f085f667ae", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model_.txt", "file_name": "blog.langchain.dev_summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model_.txt", "file_type": "text/plain", "file_size": 10301, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "029a57eb2bdbe00448087c37950b9574d76c170bdea619266c4628c122acd353", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "433da125-97f8-4ad6-9806-4592a1ae5936", "node_type": "1", "metadata": {}, "hash": "ab0b4bcd3c3e5b2de7e451270d13ea5fe24019fbf5d4dcba51b7ba4aaf7ed482", "class_name": "RelatedNodeInfo"}}, "hash": "c0f11a78612eee9223c6204d508a7b04cd6af83914894d357cb4d3e3803e7559", "text": "of passing entire sheets to LangChain, eparse will find and pass sub-tables, which appears to produce better segmentation in LangChain. Using eparse, LangChain returns 9 document chunks, with the 2nd piece (\u201c2 \u2013 Document\u201d) containing the entire first sub-table. Asking the LLM to summarize the spreadsheet using these vectors produces a more comprehensive view of what is contained in the spreadsheet, including the nuances of the sub-tables, and without any erroneous data.\n\nFigure 6 - Summarization Using eparse and Sub-table Chunking\n\nHowever, the LLM gets sidetracked by pedantic things like row structure on a single table, and still gets basic questions about amounts incorrect:\n\nAnd the dates are still in the wrong format:\n\nA better way.\n\nTo recap, these are the issues with feeding Excel files to an LLM using default implementations of unstructured, eparse, and LangChain and the current state of those tools:\n\nExcel sheets are passed as a single table and default chunking schemes break up logical collections\n\nLarger chunks strain constraints such as context window size, GPU memory, and timeout settings\n\nBroken logical collections and default retrieval schemes produce incomplete summaries\n\nDiscrete value lookup performance by the LLM on vectorized data is poor\n\nDefault data cleaning does not handle certain things like Excel numeric date encoding\n\nThe basic problem with summarization is that it is a reduction from many things to one statement. The default configuration for a single document retrieval Q&A application is to find 4 similar parts of the document and \u201cstuff\u201d them into the context window before asking for a summary.\n\nTo improve retrieval and summarization performance on spreadsheets, we need to consider other retrieval strategies. Damien Benveniste recently posted the following graphic on LinkedIn that addresses handling summaries of multiple documents that are too large to fit into the context window:\n\nFigure 7 - Damien Benveniste, TheAiEdge.io\n\nAdding options for chain type, search type, and k-documents settings to my Gradio app, I am now able to test each of these strategies in different configurations against different LLMs:\n\nFigure 8 - Adding Options for Chain Type, Search Type, and K-documents\n\nSetting chain type to \u201cmap reduce\u201d (the second strategy in Figure 7) and increasing the number of retrieved documents produces a much better result:\n\nFigure 9 - Map-reduced Summarization with 10 Table Elements\n\nThe LLM hits on all major themes in the various extracted sub-tables, acknowledges the instrument type in the document (debt), and even mentions the amortization schedule. All sans nonsensical data elements.\n\nTurning on chain verbosity, we can get an idea of what is happening behind the scenes (apologies for the small text):\n\nFigure 10 \u2013 The Final Prompt in the Map Reduce Chain\n\nEach extracted sub-table is being summarized by the LLM before it is injected into a final prompt for a collective summary at the end, just like the map-reduce diagram depicts. Expanding the K-document size ensures that smaller nuances of the file are considered.\n\nYou may be wondering about the \u201crefine\u201d strategy and perhaps wonder what happened to the small context model we started with. I tried various strategies and combinations with the smaller model including refine and, while I was able to eventually deal with the context window limitation, that model with and without the refine strategy just did not deliver quality responses. I believe this result is the combination of a worse foundational model and 8-bit quantization. Bottom line is that it pays to have at least one LLM running on outsized hardware with a solid foundation to test against.\n\nWhat about specific data retrieval?\n\nThe solution to the problem of extracting specific data from spreadsheet tables using an LLM will involve the Agent design pattern, where LLMs are taught to use functions that they can call. The demonstration of agents is beyond the scope of this article. However, we recently added something to eparse that will assist in this effort that I am excited to share.\n\nIn version 0.7.0 of eparse we introduced utility functions and a new interface to seamlessly transition from HTML tables to an eparse data interface backed by Sqlite. What this means is that users can interface their LLMs to structured table data captured by the ETL process, which is stored as metadata in the objects that are uploaded to vector storage. For example, the following HTML table was generated as a by-product from eparsing the unit test spreadsheet:\n\nFigure 11 - HTML Tabular Metadata\n\nTo facilitate an ETL pipeline powered by eparse, a drop-in replacement of the unstructured auto partitioner and the Excel partitioner are provided starting with v0.7.1 (see the README for more details on how to incorporate these functions into your project):\n\nFigure 12 - Custom Excel Partitioner for Unstructured Using eparse\n\nUsing", "start_char_idx": 4544, "end_char_idx": 9477, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "433da125-97f8-4ad6-9806-4592a1ae5936": {"__data__": {"id_": "433da125-97f8-4ad6-9806-4592a1ae5936", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model_.txt", "file_name": "blog.langchain.dev_summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model_.txt", "file_type": "text/plain", "file_size": 10301, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c4650e72-2e50-44fa-8278-a4e922d08f99", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model_.txt", "file_name": "blog.langchain.dev_summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model_.txt", "file_type": "text/plain", "file_size": 10301, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "95a7decef3be7043c8675572aede2b359afdc8385e1f76fca96db67a16126d20", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab5ee918-c0b4-4cf7-8b11-d92b6c737281", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model_.txt", "file_name": "blog.langchain.dev_summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model_.txt", "file_type": "text/plain", "file_size": 10301, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "c0f11a78612eee9223c6204d508a7b04cd6af83914894d357cb4d3e3803e7559", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93d5cda4-eb80-49f3-b3a3-7b709e8ad9cb", "node_type": "1", "metadata": {}, "hash": "55dd050410ffb82ff87cfdf02ba4dfd6ee9e52f592d99967bd778ef14069d030", "class_name": "RelatedNodeInfo"}}, "hash": "ab0b4bcd3c3e5b2de7e451270d13ea5fe24019fbf5d4dcba51b7ba4aaf7ed482", "text": "into your project):\n\nFigure 12 - Custom Excel Partitioner for Unstructured Using eparse\n\nUsing HTML tabular data in an LLM chain with agent tools is as easy as instantiating the following new HTML interface and then using it like any other database ORM:\n\nFigure 13 - eparse HTML Tabular Data Interface\n\nAnd handling conversion of numeric Excel formatting data?\n\nThe solution to the problem of handling things like recasting Excel numeric date information into the proper format would best be handled by a custom cleaning or staging brick using the unstructured library. A discussion of cleaning bricks and how to apply them is here.\n\nConclusion\n\nIn conclusion, extracting information from Excel spreadsheets presents unique problems not contemplated by many ETL systems and the typical LLM tool sets. Key points to consider when designing your own solutions include:", "start_char_idx": 9383, "end_char_idx": 10249, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93d5cda4-eb80-49f3-b3a3-7b709e8ad9cb": {"__data__": {"id_": "93d5cda4-eb80-49f3-b3a3-7b709e8ad9cb", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_syncing-data-sources-to-vector-stores_.txt", "file_name": "blog.langchain.dev_syncing-data-sources-to-vector-stores_.txt", "file_type": "text/plain", "file_size": 6318, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "69c3976a-7737-4d6e-a73b-2ee14795a4d6", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_syncing-data-sources-to-vector-stores_.txt", "file_name": "blog.langchain.dev_syncing-data-sources-to-vector-stores_.txt", "file_type": "text/plain", "file_size": 6318, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "6b01349a8a599348ca9a6c4dd08c0092424162afec08028744263d04384db4cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "433da125-97f8-4ad6-9806-4592a1ae5936", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model_.txt", "file_name": "blog.langchain.dev_summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model_.txt", "file_type": "text/plain", "file_size": 10301, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "ab0b4bcd3c3e5b2de7e451270d13ea5fe24019fbf5d4dcba51b7ba4aaf7ed482", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ddd9c354-aaee-4172-85d5-49d8297500bc", "node_type": "1", "metadata": {}, "hash": "9cfe5c07e5c4c873475d765ad30d63348dd362f89443e837d4d7053b97727c1b", "class_name": "RelatedNodeInfo"}}, "hash": "55dd050410ffb82ff87cfdf02ba4dfd6ee9e52f592d99967bd778ef14069d030", "text": "URL: https://blog.langchain.dev/syncing-data-sources-to-vector-stores/\nTitle: Syncing data sources to vector stores\n\nMost complex and knowledge-intensive LLM applications require runtime data retrieval for Retrieval Augmented Generation (RAG). A core component of the typical RAG stack is a vector store, which is used to power document retrieval.\n\nUsing a vector store requires setting up an indexing pipeline to load data from sources (a website, a file, etc.), transform the data into documents, embed those documents, and insert the embeddings and documents into the vector store.\n\nIf your data sources or processing steps change, the data needs to be re-indexed. If this happens regularly, and the changes are incremental, it becomes valuable to de-duplicate the content being indexed with the content already in the vector store. This avoids spending time and money on redundant work. It also becomes important to set up vector store cleanup processes to remove stale data from your vector store.\n\nLangChain Indexing API\n\nThe new LangChain Indexing API makes it easy to load and keep in sync documents from any source into a vector store. Specifically, it helps:\n\nAvoid writing duplicated content into the vector store\n\nAvoid re-writing unchanged content\n\nAvoid re-computing embeddings over unchanged content\n\nCrucially, the indexing API will work even with documents that have gone through several transformation steps (e.g., via text chunking) with respect to the original source documents.\n\nHow it works\n\nLangChain indexing makes use of a record manager ( RecordManager ) that keeps track of document writes into a vector store.\n\nWhen indexing content, hashes are computed for each document, and the following information is stored in the record manager:\n\nthe document hash (hash of both page content and metadata)\n\nwrite time\n\nthe source id -- each document should include information in its metadata to allow us to determine the ultimate source of this document\n\nCleanup modes\n\nWhen re-indexing documents into a vector store, it's possible that some existing documents in the vector store should be deleted. If you\u2019ve made changes to how documents are processed before insertion or source documents have changed, you\u2019ll want to remove any existing documents that come from the same source as the new documents being indexed. If some source documents have been deleted, you\u2019ll want to delete all existing documents in the vector store and replace them with the re-indexed documents.\n\nThe indexing API cleanup modes let you pick the behavior you want:\n\nFor more detailed documentation of the API and its limitations, check out the docs: https://python.langchain.com/docs/modules/data_connection/indexing\n\nSeeing it in action\n\nFirst let\u2019s initialize our vector store. We\u2019ll demo with the ElasticsearchStore , since it satisfies the pre-requisites of supporting insertion and deletion. See the Requirements docs section for more on vector store requirements.\n\n# !pip install openai elasticsearch from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import ElasticsearchStore collection_name = \"test_index\" # Set env var OPENAI_API_KEY embedding = OpenAIEmbeddings() # Run an Elasticsearch instance locally: # !docker run -p 9200:9200 -e \"discovery.type=single-node\" -e \"xpack.security.enabled=false\" -e \"xpack.security.http.ssl.enabled=false\" docker.elastic.co/elasticsearch/elasticsearch:8.9.0 vector_store = ElasticsearchStore( collection_name, es_url=\"<http://localhost:9200>\", embedding=embedding )\n\nAnd now we\u2019ll initialize and create a schema for our record manager, for which we\u2019ll just use a SQLite table:\n\nfrom langchain.indexes import SQLRecordManager namespace = f\"elasticsearch/{collection_name}\" record_manager = SQLRecordManager( namespace, db_url=\"sqlite:///record_manager_cache.sql\" ) record_manager.create_schema()\n\nSuppose we want to index the reuters.com front page. We can load and split the url contents with:\n\n# !pip install beautifulsoup4 tiktoken import bs4 from langchain.document_loaders import RecursiveUrlLoader from langchain.text_splitter import RecursiveCharacterTextSplitter raw_docs = RecursiveUrlLoader( \"<https://www.reuters.com>\", max_depth=0, extractor=lambda x: BeautifulSoup(x, \"lxml\").text ).load() processed_docs = RecursiveCharacterTextSplitter.from_tiktoken_encoder( chunk_size=200 ).split_documents(raw_docs)\n\nAnd now we\u2019re ready to index! Suppose when we first index only the first 10 documents are on the front page:\n\nfrom langchain.indexes import index index( processed_docs[:10], record_manager, vector_store, cleanup=\"full\", source_id_key=\"source\" )\n\n{'num_added': 10, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}\n\nAnd if we index an hour later, maybe 2 of the documents have changed:\n\nindex( process_docs[2:10] +", "start_char_idx": 0, "end_char_idx": 4803, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ddd9c354-aaee-4172-85d5-49d8297500bc": {"__data__": {"id_": "ddd9c354-aaee-4172-85d5-49d8297500bc", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_syncing-data-sources-to-vector-stores_.txt", "file_name": "blog.langchain.dev_syncing-data-sources-to-vector-stores_.txt", "file_type": "text/plain", "file_size": 6318, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "69c3976a-7737-4d6e-a73b-2ee14795a4d6", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_syncing-data-sources-to-vector-stores_.txt", "file_name": "blog.langchain.dev_syncing-data-sources-to-vector-stores_.txt", "file_type": "text/plain", "file_size": 6318, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "6b01349a8a599348ca9a6c4dd08c0092424162afec08028744263d04384db4cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93d5cda4-eb80-49f3-b3a3-7b709e8ad9cb", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_syncing-data-sources-to-vector-stores_.txt", "file_name": "blog.langchain.dev_syncing-data-sources-to-vector-stores_.txt", "file_type": "text/plain", "file_size": 6318, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "55dd050410ffb82ff87cfdf02ba4dfd6ee9e52f592d99967bd778ef14069d030", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03ffeae0-4aac-4f98-b2dc-68d972f3bb7d", "node_type": "1", "metadata": {}, "hash": "8bddfde18cb879f794a3ab0ee5f94a63d04202064d5dfffab7d76e8ba4237a2e", "class_name": "RelatedNodeInfo"}}, "hash": "9cfe5c07e5c4c873475d765ad30d63348dd362f89443e837d4d7053b97727c1b", "text": "maybe 2 of the documents have changed:\n\nindex( process_docs[2:10] + processed_docs[-2:], record_manager, vector_store, cleanup=\"full\", source_id_key=\"source\", )\n\n{'num_added': 2, 'num_updated': 0, 'num_skipped': 8, 'num_deleted': 2}\n\nLooking at the output, we can see that while 10 documents were indexed the actual work we did was 2 additions and 2 deletions \u2014 we added the new documents, removed the old ones and skipped all the duplicate ones.\n\nFor more in-depth examples, head to: https://python.langchain.com/docs/modules/data_connection/indexing\n\nChatLangChain + Indexing API\n\nWe\u2019ve recently revamped the https://github.com/langchain-ai/chat-langchain chatbot for questions about LangChain. As part of the revamp, we revived the hosted version https://chat.langchain.com and set up a daily indexing job using the new API to make sure the chatbot is up to date with the latest LangChain developments.\n\nDoing this was very straightforward \u2014 all we had to do was:\n\nSet up a Supabase Postgres database to be used as a record manager, Update our ingestion script to use the indexing API instead of inserting documents to the vector store directly, Set up a scheduled Github Action to run the ingestion script daily. You can check out the GHA workflow here.\n\nConclusion\n\nAs you move your apps from prototype to production, be able to re-indexing efficiently and keep documents in your vector in sync with their source becomes very important. LangChain's new indexing API provides a clean and scalable way to do this.\n\nTry it out and let us know what you think!", "start_char_idx": 4736, "end_char_idx": 6296, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "03ffeae0-4aac-4f98-b2dc-68d972f3bb7d": {"__data__": {"id_": "03ffeae0-4aac-4f98-b2dc-68d972f3bb7d", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_ted-ai-hackathon-kickoff_.txt", "file_name": "blog.langchain.dev_ted-ai-hackathon-kickoff_.txt", "file_type": "text/plain", "file_size": 2495, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb70198e-5d74-49ba-9662-c7cb492efe26", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_ted-ai-hackathon-kickoff_.txt", "file_name": "blog.langchain.dev_ted-ai-hackathon-kickoff_.txt", "file_type": "text/plain", "file_size": 2495, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "2cdb788e4f904d79698e558b62023d602434cf1caa0ad36bd81948fcc49c7a65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ddd9c354-aaee-4172-85d5-49d8297500bc", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_syncing-data-sources-to-vector-stores_.txt", "file_name": "blog.langchain.dev_syncing-data-sources-to-vector-stores_.txt", "file_type": "text/plain", "file_size": 6318, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "9cfe5c07e5c4c873475d765ad30d63348dd362f89443e837d4d7053b97727c1b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6fcd670-40e9-49de-bc71-e4da4c99b2b1", "node_type": "1", "metadata": {}, "hash": "cdacc1ba5f9c178a10c734271f9e8f3ad819f1b73bc5ee79d9158285404139ee", "class_name": "RelatedNodeInfo"}}, "hash": "8bddfde18cb879f794a3ab0ee5f94a63d04202064d5dfffab7d76e8ba4237a2e", "text": "URL: https://blog.langchain.dev/ted-ai-hackathon-kickoff/\nTitle: TED AI Hackathon Kickoff (and projects we\u2019d love to see)\n\nThe TED AI Hackathon kicks off October 14 and we\u2019re excited to be their partner in bringing it to life! TED has a long history of inspiring and educating great thinkers and builders and we can\u2019t wait to see what everyone builds.\n\nYou can register here and here's a video walkthrough of the event.\n\nWe\u2019re anticipating (and hoping!) there are many of you who might be building with LLMs for the first time. So, we thought we\u2019d share some resources to help you get started and ideas for apps that we\u2019d love to see (and share out with the broader community!). We\u2019ll also be sponsoring a prize for the winner of the best LLM application category!\n\nGetting Started with LangChain\n\nHere\u2019s how to install LangChain, set up your environment, and start building. It\u2019s as simple as: 'pip install langchain'\n\nHere\u2019s our Quickstart guide, designed to familiarize yourself with the LangChain framework through building your first LangChain application.\n\nHere\u2019s a link to use-case specific walkthroughs for question and answering, interacting with APIs, chatbots, extraction, code understanding, summarization, tagging, web scraping, and more!\n\nYouTube tutorials: Here\u2019s a list of tutorials created by our community that we think are especially helpful.\n\nInspiration: Check out our gallery of some of our favorite applications built on top of LangChain.\n\nYou can also get your feet wet by contributing to our open source repo. Here\u2019s a list of good first issues to try.\n\nLangChain Prompt Hub\n\nWe recently launched LangChain Hub, a home for submitting, discovering, inspecting, and remixing prompts. It\u2019s still (very) early days\u2013we see it as not only a useful tool for helping developers build faster, but also as a way for the entire community to get collectively smarter on prompting overall.\n\n\n\nWe hope you\u2019ll contribute prompts and try each other\u2019s out\u2013here\u2019s how.\n\n\n\nProjects we\u2019d love to see (and share with the LangChain community!)\n\napplications that use open source models\n\ninnovative retrieval tactics\n\npractical agents\n\nadventurous agents\n\napps that connect unique data sources/format(s)\n\n\n\nWe\u2019ll be tweeting out our favorite projects throughout the week and collecting them for a blog post to close out the Hackathon.\n\nTag us on Twitter (@langchainai) or send us a note at hello@langchain.dev. We can\u2019t wait to see what you build!", "start_char_idx": 0, "end_char_idx": 2449, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c6fcd670-40e9-49de-bc71-e4da4c99b2b1": {"__data__": {"id_": "c6fcd670-40e9-49de-bc71-e4da4c99b2b1", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_name": "blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_type": "text/plain", "file_size": 29160, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "036cc20e-6afc-45ea-95aa-e2dc7a370631", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_name": "blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_type": "text/plain", "file_size": 29160, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "c873ed4ea17b29b71707b9f71bbf33c65f813f7de9224536fc24e769d4cbf7c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03ffeae0-4aac-4f98-b2dc-68d972f3bb7d", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_ted-ai-hackathon-kickoff_.txt", "file_name": "blog.langchain.dev_ted-ai-hackathon-kickoff_.txt", "file_type": "text/plain", "file_size": 2495, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "8bddfde18cb879f794a3ab0ee5f94a63d04202064d5dfffab7d76e8ba4237a2e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b83140e-8002-480d-9201-fc1432699b27", "node_type": "1", "metadata": {}, "hash": "27d17518a47bba2f2508260985b4c1ef8b9165228c6c52f74f20e22562ca7ad3", "class_name": "RelatedNodeInfo"}}, "hash": "cdacc1ba5f9c178a10c734271f9e8f3ad819f1b73bc5ee79d9158285404139ee", "text": "URL: https://blog.langchain.dev/timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications/\nTitle: Timescale Vector x LangChain: Making PostgreSQL A Better Vector Database for AI Applications\n\nEditor's Note: This post was written in collaboration with the Timescale Vector team. Their integration with LangChain supports PostgreSQL as your vector database for faster similarity search, time-based context retrieval for RAG, and self-querying capabilities. And they're offering a free 90 day trial!\n\nIntroducing the Timescale Vector integration for LangChain. Timescale Vector enables LangChain developers to build better AI applications with PostgreSQL as their vector database: with faster vector similarity search, efficient time-based search filtering, and the operational simplicity of a single, easy-to-use cloud PostgreSQL database for not only vector embeddings, but an AI application\u2019s relational and time-series data too.\n\n\n\nPostgreSQL is the world\u2019s most loved database, according to the Stack Overflow 2023 Developer Survey. And for a good reason: it\u2019s been battle-hardened by production use for over three decades, it\u2019s robust and reliable, and it has a rich ecosystem of tools, drivers, and connectors.\n\nAnd while pgvector, the open-source extension for vector data on PostgreSQL, is a wonderful extension (and is offered as part of Timescale Vector), it is just one piece of the puzzle in providing a production-grade experience for AI application developers on PostgreSQL. After speaking with numerous developers at nimble startups and established industry giants, we saw the need to enhance pgvector to cater to the performance and operational needs of developers building AI applications.\n\nHere\u2019s the TL;DR on how Timescale Vector helps you build better AI applications with LangChain:\n\nFaster similarity search on millions of vectors: Thanks to the introduction of a new search index inspired by the DiskANN algorithm, Timescale Vector achieves 243% faster search speed at ~99 % recall than Weaviate, a specialized database, and outperforms all existing PostgreSQL search indexes by between 39.39% and 1,590.33% on a dataset of one million OpenAI embeddings. Plus, enabling product quantization yields a 10x index space savings compared to pgvector. Timescale Vector also offers pgvector\u2019s Hierarchical Navigable Small Worlds (HNSW) and Inverted File Flat (IVFFlat) indexing algorithms.\n\nThanks to the introduction of a new search index inspired by the DiskANN algorithm, Timescale Vector achieves 243% faster search speed at ~99 % recall than Weaviate, a specialized database, and outperforms all existing PostgreSQL search indexes by between 39.39% and 1,590.33% on a dataset of one million OpenAI embeddings. Plus, enabling product quantization yields a 10x index space savings compared to pgvector. Timescale Vector also offers pgvector\u2019s Hierarchical Navigable Small Worlds (HNSW) and Inverted File Flat (IVFFlat) indexing algorithms. Similarity search with efficient time-based filtering: Timescale Vector optimizes time-based vector search, leveraging the automatic time-based partitioning and indexing of Timescale\u2019s hypertables to efficiently find recent embeddings, constrain vector search by a time range or document age, and store and retrieve large language model (LLM) response and chat history with ease. Time-based semantic search also enables you to use Retrieval Augmented Generation (RAG) with time-based context retrieval to give users more useful LLM responses.\n\nTimescale Vector optimizes time-based vector search, leveraging the automatic time-based partitioning and indexing of Timescale\u2019s hypertables to efficiently find recent embeddings, constrain vector search by a time range or document age, and store and retrieve large language model (LLM) response and chat history with ease. Time-based semantic search also enables you to use Retrieval Augmented Generation (RAG) with time-based context retrieval to give users more useful LLM responses. Simplified AI infra stack: By combining vector embeddings, relational data, and time-series data in one PostgreSQL database, Timescale Vector eliminates the operational complexity that comes with managing multiple database systems at scale.\n\nBy combining vector embeddings, relational data, and time-series data in one PostgreSQL database, Timescale Vector eliminates the operational complexity that comes with managing multiple database systems at scale. Simplified metadata handling and multi-attribute filtering: You can leverage all PostgreSQL data types to store and filter metadata, and JOIN vector search results with relational data for more contextually relevant responses. In future releases, Timescale Vector will also support rich multi-attribute filtering, enabling even faster similarity searches when filtering on metadata.\n\n\n\nOn top of these innovations for vector workloads, Timescale Vector provides a robust, production-ready PostgreSQL platform with flexible pricing, enterprise-grade security, and free expert support.\n\n\n\nIn the rest of this post, we\u2019ll dive deeper", "start_char_idx": 0, "end_char_idx": 5109, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b83140e-8002-480d-9201-fc1432699b27": {"__data__": {"id_": "6b83140e-8002-480d-9201-fc1432699b27", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_name": "blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_type": "text/plain", "file_size": 29160, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "036cc20e-6afc-45ea-95aa-e2dc7a370631", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_name": "blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_type": "text/plain", "file_size": 29160, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "c873ed4ea17b29b71707b9f71bbf33c65f813f7de9224536fc24e769d4cbf7c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6fcd670-40e9-49de-bc71-e4da4c99b2b1", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_name": "blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_type": "text/plain", "file_size": 29160, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "cdacc1ba5f9c178a10c734271f9e8f3ad819f1b73bc5ee79d9158285404139ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f832ed9-9715-4e6c-a976-836c387b101a", "node_type": "1", "metadata": {}, "hash": "47421a2e5a6c04ef48f5d4c17034104453f3392de9b05a7a535f6cf0fe5b9adb", "class_name": "RelatedNodeInfo"}}, "hash": "27d17518a47bba2f2508260985b4c1ef8b9165228c6c52f74f20e22562ca7ad3", "text": "enterprise-grade security, and free expert support.\n\n\n\nIn the rest of this post, we\u2019ll dive deeper (with code!) into the unique capabilities Timescale Vector enables for developers wanting to use PostgreSQL as their vector database with LangChain:\n\n\n\nFaster similarity search with DiskANN, HNSW and IVFFlat index types.\n\nEfficient similarity search when filtering vectors by time.\n\nRetrieval Augmented Generation (RAG) with time-based context retrieval.\n\nAdvanced self-querying capabilities.\n\n\n\n(If you\u2019d prefer to jump into the code, explore this tutorial).\n\n\n\n\ud83c\udf89 LangChain Users Get 3 Months Free of Timescale Vector\n\nTimescale is giving LangChain users an extended 90-day trial of Timescale Vector. This makes it easy to test and develop your applications with Timescale Vector, as you won\u2019t be charged for any cloud PostgreSQL databases you spin up during your trial period. Try Timescale Vector for free today.\n\n\n\nFaster Vector Similarity Search in PostgreSQL\n\nTimescale Vector speeds up Approximate Nearest Neighbor (ANN) search on large scale vector datasets, enhancing pgvector with a state-of-the-art ANN index inspired by the DiskANN algorithm. Timescale Vector also offers pgvector\u2019s HNSW and IVFFlat indexing algorithms as well, giving developers the flexibility to choose the right index for their use case.\n\n\n\nOur performance benchmarks using the ANN benchmarks suite show that Timescale Vector achieves between 39.43% and 1,590.33% faster search speed at ~99 % recall than all existing PostgreSQL search indexes and 243.77% faster search speed than specialized vector databases like Weaviate, on a dataset of one million OpenAI embeddings. You can read more about the performance benchmark methodology and results here.\n\n\n\nCaption: Timescale Vector\u2019s new index outperforms specialized vector database Weaviate by 243% and all existing PostgreSQL index types when performing approximate nearest neighbor searches at 99% recall on 1 million OpenAI embeddings.\n\n\n\nUsing Timescale Vector\u2019s DiskANN, HNSW, or IVFFLAT indexes in LangChain is incredibly straightforward.\n\n\n\nSimply create a Timescale Vector vector store as shown below:\n\nfrom langchain.vectorstores.timescalevector import TimescaleVector # Create a Timescale Vector instance from the collection of documents db = TimescaleVector.from_documents( embedding=embeddings, documents=docs, collection_name=COLLECTION_NAME, service_url=SERVICE_URL, )\n\n\n\n\n\nAnd then run:\n\n# create an index # by default this will create a Timescale Vector (DiskANN) index db.create_index()\n\nThis will create a timescale-vector index with the default parameters.\n\n\n\nWe should point out that the term \u201cindex\u201d is a bit overloaded. For many vector databases, an index is the thing that stores your data (in relational databases this is often called a table), but in the PostgreSQL world an index is something that speeds up search, and we are using the latter meaning here.\n\n\n\nWe can also specify the exact parameters for index creation in the `create_index` command as follows:\n\n# create an timescale vector index (DiskANN) with specified parameters db.create_index(index_type=\"tsv\", max_alpha=1.0, num_neighbors=50)\n\nAdvantages to this Timescale Vector\u2019s new DiskANN-inspired vector search index include the following:\n\n\n\nFaster vector search at high accuracy in PostgreSQL.\n\nOptimized for running on disks, not only in memory use.\n\nQuantization optimization compatible with PostgreSQL, reducing the vector size and consequently shrinking the index size (by 10x in some cases!) and expediting searches.\n\nEfficient hybrid search or filtering additional dimensions.\n\n\n\nFor more on DiskANN and how Timescale Vector\u2019s new index works, see this blog post.\n\n\n\nPgvector is packaged as part of Timescale Vector, so you can also access pgvector\u2019s HNSW and IVFFLAT indexing algorithms in your LangChain applications. The ability to conveniently create database indexes from your LangChain application code makes it easy to create different indexes and compare their performance.\n\n\n\n# Create an HNSW index. # Note: you don't need to specify m and ef_construction parameters as we set smart defaults. db.create_index(index_type=\"hnsw\", m=16, ef_construction=64) # Create an IVFFLAT index # Note:you don't need to specify num_lists and num_records parameters as we set smart defaults. db.create_index(index_type=\"ivfflat\", num_lists=20, num_records=1000)\n\nAdd Efficient Time-Based Search Functionality to Your LangChain AI Application\n\nTimescale Vector optimizes time-based vector search, leveraging the automatic time-based partitioning and indexing of Timescale\u2019s hypertables to efficiently search vectors by time and similarity.\n\n\n\nTime is often an important metadata component for vector embeddings. Sources of embeddings, like documents, images, and web pages, often have a timestamp associated with them, for example,", "start_char_idx": 5011, "end_char_idx": 9866, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f832ed9-9715-4e6c-a976-836c387b101a": {"__data__": {"id_": "5f832ed9-9715-4e6c-a976-836c387b101a", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_name": "blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_type": "text/plain", "file_size": 29160, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "036cc20e-6afc-45ea-95aa-e2dc7a370631", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_name": "blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_type": "text/plain", "file_size": 29160, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "c873ed4ea17b29b71707b9f71bbf33c65f813f7de9224536fc24e769d4cbf7c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b83140e-8002-480d-9201-fc1432699b27", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_name": "blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_type": "text/plain", "file_size": 29160, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "27d17518a47bba2f2508260985b4c1ef8b9165228c6c52f74f20e22562ca7ad3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e64faa6d-6cfe-4fab-8fdb-7661991005b3", "node_type": "1", "metadata": {}, "hash": "1d8fdce97883f0fea9881457131461885e226f984ded57b95d830c48097382f5", "class_name": "RelatedNodeInfo"}}, "hash": "47421a2e5a6c04ef48f5d4c17034104453f3392de9b05a7a535f6cf0fe5b9adb", "text": "like documents, images, and web pages, often have a timestamp associated with them, for example, their creation date, publishing date, or the date they were last updated, to name but a few.\n\n\n\nWe can take advantage of this time metadata in our collections of vector embeddings to enrich the quality and applicability of search results by retrieving vectors that are not just semantically similar but also pertinent to a specific time frame.\n\n\n\nHere are some examples where time-based retrieval of vectors can improve your LangChain applications:\n\n\n\n\n\nChat history: Storing and retrieving LLM response history. For example, chatbot chat history.\n\nStoring and retrieving LLM response history. For example, chatbot chat history. Finding recent embeddings: Finding the most recent embeddings similar to a query vector. For example, finding the most recent news, documents, or social media posts related to elections.\n\nFinding the most recent embeddings similar to a query vector. For example, finding the most recent news, documents, or social media posts related to elections. Search within a time range: Constraining similarity search to only vectors within a relevant time range. For example, asking time-based questions about a knowledge base (\u201cWhat new features were added between January and March 2023?\u201d).\n\n\n\nLet\u2019s look at an example of performing time-based searches on a git log dataset. In a git log, each entry has a timestamp, an author, and some information about the commit.\n\n\n\nTo illustrate how to use TimescaleVector's time-based vector search functionality, we'll ask questions about the git log history for TimescaleDB. Each git commit entry has a timestamp associated with it, as well as a message and other metadata (e.g., author).\n\nLoad text and extract metadata\n\nFirst, we load in the git log using LangChain\u2019s JSON Loader.\n\n\n\n# Load data from JSON file and extract metadata loader = JSONLoader( file_path=FILE_PATH, jq_schema='.commit_history[]', text_content=False, metadata_func=extract_metadata ) documents = loader.load()\n\nNotice how we provide a function named `extract_metadata` as an argument to the JSONLoader. This function enables us to store not just the contents of the JSON in vectorized form but metadata about an embedding. It is in this function that we\u2019ll specify the timestamp of the git log entry to be used in our time-based vector search.\n\nCreate time-based identifiers for Documents\n\nFor time-based search in LangChain, Timescale Vector uses the \u2018datetime\u2019 portion of a UUID v1 to place vectors in the correct time partition. Timescale Vector\u2019s Python client library provides a simple-to-use function named `uuid_from_time` to create a UUID v1 from a Python `datetime` object, which you can then pass to the Timescale Vector vector store constructor as we\u2019ll see in the code snippet further down. Here\u2019s how we use the `uuid_from_time` helper functions:\n\n\n\nfrom timescale_vector import client # Function to take in a date string in the past and return a uuid v1 def create_uuid(date_string: str): if date_string is None: return None time_format = '%a %b %d %H:%M:%S %Y %z' datetime_obj = datetime.strptime(date_string, time_format) uuid = client.uuid_from_time(datetime_obj) return str(uuid)\n\nHere\u2019s the `extract_metdata()` function we pass to the JSONLoader specifying the fields we want in the metadata for each vector embedding in our vector collection:\n\n\n\n# Metadata extraction function to extract metadata from a JSON record def extract_metadata(record: dict, metadata: dict) -> dict: record_name, record_email = split_name(record[\"author\"]) metadata[\"id\"] = create_uuid(record[\"date\"]) metadata[\"date\"] = create_date(record[\"date\"]) metadata[\"author_name\"] = record_name metadata[\"author_email\"] = record_email metadata[\"commit_hash\"] = record[\"commit\"] return metadata\n\nNote: The code above references two helper functions to get things in the right format (`split_name()` and `create_date()`) which we\u2019ve omitted for brevity. The full code is included in the tutorial linked in the Resources section at the end of this post.\n\n\n\nNext, we'll create a Timescale Vector instance from the collection of documents we loaded into the JSONLoader above.\n\nLoad vectors and metadata into Timescale Vector\n\nFinally, we'll create the Timescale Vector instance from the set of documents we loaded in.\n\n\n\nTo take advantage of Timescale Vector\u2019s efficient time-based search, we need to specify the `time_partition_interval` argument when creating a Timescale Vector vector store. This argument represents the length of each interval for partitioning the data by time. Each partition will consist of data that falls within the specified length of time.\n\n\n\nIn the example below, we use seven days for simplicity, but you can pick whatever value makes sense for your use case\u2014for example, if you query recent vectors frequently, you might", "start_char_idx": 9770, "end_char_idx": 14638, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e64faa6d-6cfe-4fab-8fdb-7661991005b3": {"__data__": {"id_": "e64faa6d-6cfe-4fab-8fdb-7661991005b3", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_name": "blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_type": "text/plain", "file_size": 29160, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "036cc20e-6afc-45ea-95aa-e2dc7a370631", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_name": "blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_type": "text/plain", "file_size": 29160, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "c873ed4ea17b29b71707b9f71bbf33c65f813f7de9224536fc24e769d4cbf7c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f832ed9-9715-4e6c-a976-836c387b101a", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_name": "blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_type": "text/plain", "file_size": 29160, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "47421a2e5a6c04ef48f5d4c17034104453f3392de9b05a7a535f6cf0fe5b9adb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d88b7964-bcd2-4799-879a-d5def701f526", "node_type": "1", "metadata": {}, "hash": "ce801d543df0912f07bc96053ca03ae006be428f3297946a01b931feabb6295b", "class_name": "RelatedNodeInfo"}}, "hash": "1d8fdce97883f0fea9881457131461885e226f984ded57b95d830c48097382f5", "text": "whatever value makes sense for your use case\u2014for example, if you query recent vectors frequently, you might want to use a smaller time delta (ike one day), or if you query vectors over a decade-long time period, then you might want to use a larger time delta like six months or one year. As a rule of thumb, common queries should touch only a couple of partitions and at the same time your full dataset should fit within a 1000 partitions, but don\u2019t stress too much \u2013 the system is not very sensitive to this value.\n\n\n\nWe specify the `ids` argument to be a list of the UUID v1s we created in the pre-processing step above and stored in the ID field of our metadata. We do this because we want the time part of our UUIDs to reflect past dates. If we want the current date and time to be associated with our document, we can remove the `id` argument, and UUIDs will be automatically created with the current date and time.\n\n\n\n# Define collection name COLLECTION_NAME = \"timescale_commits\" embeddings = OpenAIEmbeddings() # Create a Timescale Vector instance from the collection of documents db = TimescaleVector.from_documents( embedding=embeddings, ids = [doc.metadata[\"id\"] for doc in docs], documents=docs, collection_name=COLLECTION_NAME, service_url=SERVICE_URL, time_partition_interval=timedelta(days = 7),)\n\nEfficient similarity search with time filters\n\nNow that we\u2019ve loaded our vector data and metadata into a Timescale Vector vector store, and enabled automatic time-based partitioning on the table our vectors and metadata are stored in, we can query our vector store with time-based filters as follows:\n\n\n\nstart_dt = datetime(2023, 8, 1, 22, 10, 35) end_dt = datetime(2023, 8, 30, 22, 10, 35) query = \"What's new with TimescaleDB functions?\"\n\ndocs_with_score = db.similarity_search_with_score(query, start_date=start_dt, end_date=end_dt)\n\n-------------------------------------------------------------------------------- Score: 0.17487859725952148 Date: Tue Aug 29 18:13:24 2023 +0200 {\"commit\": \" e4facda540286b0affba47ccc63959fefe2a7b26\", \"author\": \"Sven Klemm<sven@timescale.com>\", \"date\": \"Tue Aug 29 18:13:24 2023 +0200\", \"change summary\": \"Add compatibility layer for _timescaledb_internal functions\", \"change details\": \"With timescaledb 2.12 ...\"} -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- Score: 0.17487859725952148 Date: Tue Aug 29 18:13:24 2023 +0200 {\"commit\": \" e4facda540286b0affba47ccc63959fefe2a7b26\", \"author\": \"Sven Klemm<sven@timescale.com>\", \"date\": \"Tue Aug 29 18:13:24 2023 +0200\", \"change summary\": \"Add compatibility layer for _timescaledb_internal functions\", \"change details\": \"With timescaledb 2.12 all the functions... \"} -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- Score: 0.18100780248641968 Date: Sun Aug 20 22:47:10 2023 +0200 {\"commit\": \" 0a66bdb8d36a1879246bd652e4c28500c4b951ab\", \"author\": \"Sven Klemm<sven@timescale.com>\", \"date\": \"Sun Aug 20 22:47:10 2023 +0200\", \"change summary\": \"Move functions to _timescaledb_functions schema\", \"change details\": \"To increase schema security we do ...\"} --------------------------------------------------------------------------------\n\nSuccess! Notice how only vectors with timestamps within the specified start and end date ranges of 1 August 2023 and 30 August 2023 are included in the results.\n\n\n\nWe can also specify a time filter with a provided start date and time delta later:\n\nstart_dt = datetime(2023, 8, 1, 22, 10, 35) td = timedelta(days=7) query = \"What's new with TimescaleDB functions?\" docs_with_score = db.similarity_search_with_score(query, start_date=start_dt, time_delta=td)\n\nAnd specify a time filter within a provided end_date and time delta earlier:\n\nend_dt = datetime(2023, 8, 30, 22, 10, 35) td = timedelta(days=7) query = \"What's new with TimescaleDB functions?\" docs_with_score =", "start_char_idx": 14531, "end_char_idx": 18556, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d88b7964-bcd2-4799-879a-d5def701f526": {"__data__": {"id_": "d88b7964-bcd2-4799-879a-d5def701f526", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_name": "blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_type": "text/plain", "file_size": 29160, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "036cc20e-6afc-45ea-95aa-e2dc7a370631", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_name": "blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_type": "text/plain", "file_size": 29160, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "c873ed4ea17b29b71707b9f71bbf33c65f813f7de9224536fc24e769d4cbf7c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e64faa6d-6cfe-4fab-8fdb-7661991005b3", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_name": "blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_type": "text/plain", "file_size": 29160, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "1d8fdce97883f0fea9881457131461885e226f984ded57b95d830c48097382f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2bdf1086-5b5e-487f-af65-985cd564a3d8", "node_type": "1", "metadata": {}, "hash": "91bb3d881643fe0f27bc3c9c985527db30c8a40877c7b14cc993560bb5c11b0b", "class_name": "RelatedNodeInfo"}}, "hash": "ce801d543df0912f07bc96053ca03ae006be428f3297946a01b931feabb6295b", "text": "query = \"What's new with TimescaleDB functions?\" docs_with_score = db.similarity_search_with_score(query, end_date=end_dt, time_delta=td)\n\nWhat\u2019s happening behind the scenes\n\nHere\u2019s some intuition for why Timescale Vector\u2019s time-based partitioning speeds up ANN queries with time-based filters:\n\n\n\nTimescale Vector partitions the data by time and creates ANN indexes on each partition individually. Then, during search, we perform a three-step process:\n\n\n\n\n\nStep 1: filter our partitions that don\u2019t match the time predicate.\n\nStep 2: perform the similarity search on all matching partitions.\n\nStep 3: combine all the results from each partition in step 2, rerank, and filter out results by time.\n\n\n\nTimescale Vector leverages TimescaleDB\u2019s hypertables, which automatically partition vectors and associated metadata by a timestamp. This enables efficient querying on vectors by both similarity to a query vector and time, as partitions not in the time window of the query are ignored, making the search a lot more efficient by filtering out whole swaths of data in one go.\n\nPowering Retrieval Augmented Generation With Time-Based Context Retrieval in LangChain Applications with Timescale Vector\n\nLet\u2019s put everything together and look at how to use the Timescale Vector to power Retrieval Augmented Generation (RAG) on the git log dataset we examined above.\n\n\n\nTimescale Vector helps with time-based context retrieval, where we want to find the most relevant vectors within a specified time range to use as context for answering a user query. Let's take a look at an example below, using Timescale Vector as a retriever.\n\n\n\nFirst we create a retriever from the TimescaleVector store.\n\n# Set timescale vector as a retriever and specify start and end dates via kwargs retriever = db.as_retriever(search_kwargs={\"start_date\": start_dt, \"end_date\": end_dt})\n\nWhen creating the retriever, we can constrain the search to a relevant time range by passing our time filter parameters for Timescale Vector as `search_kwargs`.\n\n\n\nThen we\u2019ll create a RetrievalQA chain from a Stuff chain, by passing our retriever and the LLM we want to use to generate a response:\n\nfrom langchain.chat_models import ChatOpenAI llm = ChatOpenAI(temperature = 0.1, model = 'gpt-3.5-turbo-16k') from langchain.chains import RetrievalQA qa_stuff = RetrievalQA.from_chain_type( llm=llm, chain_type=\"stuff\", retriever=retriever, verbose=True, )\n\nThen we can query the RetrievalQA chain and it will use the retriever backed by Timescale Vector to answer your the query with the most relevant documents within the time ranged specified:\n\nquery = \"What's new with the timescaledb functions? Tell me when these changes were made.\" response = qa_stuff.run(query) print(response)\n\n\n\n\n\n> Entering new RetrievalQA chain...\n\n\n\n> Finished chain.\n\nThe following changes were made to the timescaledb functions:\n\n1. \"Add compatibility layer for _timescaledb_internal functions\" - This change was made on Tue Aug 29 18:13:24 2023 +0200.\n\n2. \"Move functions to _timescaledb_functions schema\" - This change was made on Sun Aug 20 22:47:10 2023 +0200.\n\n3. \"Move utility functions to _timescaledb_functions schema\" - This change was made on Tue Aug 22 12:01:19 2023 +0200.\n\n4. \"Move partitioning functions to _timescaledb_functions schema\" - This change was made on Tue Aug 29 10:49:47 2023 +0200.\n\nSuccess! Note that the context the LLM uses to compose an answer are from retrieved documents only within the specified date range.\n\n\n\nThis is a simple example of a powerful concept \u2013 using time-based context retrieval in your RAG applications can help provide more relevant answers to your users. This time-based context retrieval can be helpful to any dataset with a natural language and time component. Timescale Vector uniquely enables this thanks to its efficient time-based similarity search capabilities and taking advantage of it in your LangChain applications is easy thanks to the Timescale Vector integration.\n\nAdvanced LangChain Self-Querying Capabilities With Timescale Vector\n\nTimescale Vector also supports one of LangChain\u2019s coolest features: the Self-Querying retriever.\n\n\n\nHere\u2019s how it works: We create a retriever from the Timescale Vector vector store and feed it a natural language query with a query statement and filters (single or composite). The retriever then uses a query constructing LLM chain to write a SQL query and applies it to the underlying PostgreSQL database in the Timescale Vector vector store.\n\n\n\nWith Timescale Vector, you can ask queries with", "start_char_idx": 18490, "end_char_idx": 23022, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2bdf1086-5b5e-487f-af65-985cd564a3d8": {"__data__": {"id_": "2bdf1086-5b5e-487f-af65-985cd564a3d8", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_name": "blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_type": "text/plain", "file_size": 29160, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "036cc20e-6afc-45ea-95aa-e2dc7a370631", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_name": "blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_type": "text/plain", "file_size": 29160, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "c873ed4ea17b29b71707b9f71bbf33c65f813f7de9224536fc24e769d4cbf7c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d88b7964-bcd2-4799-879a-d5def701f526", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_name": "blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_type": "text/plain", "file_size": 29160, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "ce801d543df0912f07bc96053ca03ae006be428f3297946a01b931feabb6295b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a588886-6fd7-47b7-9c4f-bed2282189b8", "node_type": "1", "metadata": {}, "hash": "b974da409a217e8567290659deb7ba0f438a7975205ade536a3426614ad33a2e", "class_name": "RelatedNodeInfo"}}, "hash": "91bb3d881643fe0f27bc3c9c985527db30c8a40877c7b14cc993560bb5c11b0b", "text": "PostgreSQL database in the Timescale Vector vector store.\n\n\n\nWith Timescale Vector, you can ask queries with limit, metadata, and time-based filters using the self-query retriever. Let\u2019s take a look at an example of using self-querying on a git log dataset.\n\n\n\nFirst, we instantiate our TimescaleVector vector store:\n\nCOLLECTION_NAME = \"timescale_commits\" vectorstore = TimescaleVector( embedding_function=OpenAIEmbeddings(), collection_name=COLLECTION_NAME, service_url=SERVICE_URL, )\n\n\n\n\n\nSecond, let\u2019s create the self-query retriever from an LLM, passing the following parameters to it:\n\n`llm`: the LLM we want our self-query retriever to use to construct the queries.\n\n`vectorstore`: our TimescaleVector vectorstore, instantiated above.\n\n`document_content_description`: a description of the content associated with our vector embeddings. In this case, it is information about the git log entries.\n\n`metadata_field_info`: a list of AttributeInfo objects, which give the LLM information about the metadata fields in our collection of vectors.\n\n`enable_limit`: setting this to \u2018true\u2019 enables us to ask questions with an implied limit, which helps constrain the number of results returned.\n\nfrom langchain.llms import OpenAI from langchain.retrievers.self_query.base import SelfQueryRetriever from langchain.chains.query_constructor.base import AttributeInfo # Give LLM info about the metadata fields metadata_field_info = [ AttributeInfo( name=\"id\", description=\"A UUID v1 generated from the date of the commit\", type=\"uuid\", ), AttributeInfo( name=\"date\", description=\"The date of the commit in timestamptz format\", type=\"timestamptz\", ), AttributeInfo( name=\"author_name\", description=\"The name of the author of the commit\", type=\"string\", ), AttributeInfo( name=\"author_email\", description=\"The email address of the author of the commit\", type=\"string\", ) ] document_content_description = \"The git log commit summary containing the commit hash, author, date of commit, change summary and change details\" # Instantiate the self-query retriever from an LLM llm = OpenAI(temperature=0) retriever = SelfQueryRetriever.from_llm( llm, vectorstore, document_content_description, metadata_field_info, enable_limit=True, verbose=True )\n\nNow for the fun part, let\u2019s query our self-query retriever.\n\nSelf-querying example: Query and metadata filter\n\nHere\u2019s an example of a simple metadata filter specified in natural language. In this case, we\u2019re asking for commits added by a specific person:\n\nretriever.get_relevant_documents(\"What commits about timescaledb_functions did Sven Klemm add?\")\n\nHere\u2019s the verbose output of the LLM chain, showing what query parameters the natural language query got translated into:\n\nquery='timescaledb_functions' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='author_name', value='Sven Klemm') limit=None\n\n\n\n\n\nSelf-querying example:: Time-based filter\n\nHere\u2019s an example of a question that uses a time-based filter. The query to fetch the results to answer this question will take advantage of Timescale Vector\u2019s efficient time-based partitioning.\n\n\n\n# This example specifies a time-based filter retriever.get_relevant_documents(\"What commits were added in July 2023?\")\n\nHere\u2019s the verbose explanation from LangChain\u2019s self-query retriever about the SQL query parameters that the natural language query gets translated into:\n\nquery=' ' filter=Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GTE: 'gte'>, attribute='date', value='2023-07-01T00:00:00Z'), Comparison(comparator=<Comparator.LTE: 'lte'>, attribute='date', value='2023-07-31T23:59:59Z')]) limit=None\n\n\n\n\n\nAnd here\u2019s a snippet of the results:\n\n[Document(page_content='{\"commit\": \" 5cf354e2469ee7e43248bed382a4b49fc7ccfecd\", \"author\": \"Markus Engel<engel@sero-systems.de>\", \"date\": \"Mon Jul 31 11:28:25 2023 +0200\",... Document(page_content='{\"commit\": \" 88aaf23ae37fe7f47252b87325eb570aa417c607\", \"author\": \"noctarius aka Christoph Engelbert<me@noctarius.com>\", \"date\": \"Wed Jul 12 14:53:40 2023 +0200\", . . . Document(page_content='{\"commit\": \"", "start_char_idx": 22914, "end_char_idx": 27000, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a588886-6fd7-47b7-9c4f-bed2282189b8": {"__data__": {"id_": "7a588886-6fd7-47b7-9c4f-bed2282189b8", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_name": "blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_type": "text/plain", "file_size": 29160, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "036cc20e-6afc-45ea-95aa-e2dc7a370631", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_name": "blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_type": "text/plain", "file_size": 29160, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "c873ed4ea17b29b71707b9f71bbf33c65f813f7de9224536fc24e769d4cbf7c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2bdf1086-5b5e-487f-af65-985cd564a3d8", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_name": "blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_type": "text/plain", "file_size": 29160, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "91bb3d881643fe0f27bc3c9c985527db30c8a40877c7b14cc993560bb5c11b0b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb4b951c-f4ea-4d8e-b3e4-0a3ca0e988db", "node_type": "1", "metadata": {}, "hash": "d00c02822e092d53c8508bdd4b2891e97cfb1f7a021c4823616c865c1c6a9a0d", "class_name": "RelatedNodeInfo"}}, "hash": "b974da409a217e8567290659deb7ba0f438a7975205ade536a3426614ad33a2e", "text": "2023 +0200\", . . . Document(page_content='{\"commit\": \" d5268c36fbd23fa2a93c0371998286e8688247bb\", \"author\": \"Alexander Kuzmenkov<36882414+akuzm@users.noreply.github.com>\", \"date\": \"Fri Jul 28 13:35:05 2023 +0200\",...\n\n\n\n\n\nNote how you can specify a query, filter, and composite filter (filters with AND, OR) in natural language and the self-query retriever will translate that query into SQL and perform the search on the Timescale Vector (PostgreSQL) vector store.\n\n\n\nThis illustrates the power of the self-query retriever. You can use it to perform complex searches over your vector store without you or your users having to write any SQL directly.\n\nWhat\u2019s more, you can combine the self-query retriever with the LangChain RetrievalQA chain to power an RAG application!\n\nResources and next steps\n\nNow that you\u2019ve learned how Timescale Vector can help you power AI applications with PostgreSQL, it\u2019s your turn to dive in. Take the next step in your learning journey by following one of the tutorials or reading one of the blog posts in the following resources set:\n\nUp and Running Tutorial : learn how to use Timescale Vector as a LangChain vector store and perform time-based similarity search on vectors.\n\nlearn how to use Timescale Vector as a LangChain vector store and perform time-based similarity search on vectors. Self-query retriever tutorial: learn how to use Timescale Vector as a self-query retriever.\n\nlearn how to use Timescale Vector as a self-query retriever. Timescale Vector explainer : learn more about the internals of Timescale Vector\n\n: learn more about the internals of Timescale Vector Timescale Vector website: learn more about Timescale Vector and Timescale\u2019s AI Launch Week.\n\n\n\n\ud83c\udf89 And a reminder: LangChain Users get Timescale Vector free for 3 Months\n\nTimescale is giving LangChain users an extended 90-day trial of Timescale Vector. This makes it easy to test and develop your applications with Timescale Vector, as you won\u2019t be charged for any cloud PostgreSQL databases you spin up during your trial period. Try Timescale Vector for free today.", "start_char_idx": 26946, "end_char_idx": 29022, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cb4b951c-f4ea-4d8e-b3e4-0a3ca0e988db": {"__data__": {"id_": "cb4b951c-f4ea-4d8e-b3e4-0a3ca0e988db", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_titantakeoff-x-langchain-supercharged-local-inference-for-llms-2_.txt", "file_name": "blog.langchain.dev_titantakeoff-x-langchain-supercharged-local-inference-for-llms-2_.txt", "file_type": "text/plain", "file_size": 4865, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ca0e92d5-47c1-4dc2-b585-920412c51542", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_titantakeoff-x-langchain-supercharged-local-inference-for-llms-2_.txt", "file_name": "blog.langchain.dev_titantakeoff-x-langchain-supercharged-local-inference-for-llms-2_.txt", "file_type": "text/plain", "file_size": 4865, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "ba09750dffd7ff47ef3961e2a4d1dfa537b57f2da3f5fa0b928d34cff404c91a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a588886-6fd7-47b7-9c4f-bed2282189b8", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_name": "blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_type": "text/plain", "file_size": 29160, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "b974da409a217e8567290659deb7ba0f438a7975205ade536a3426614ad33a2e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b430f89c-e810-4466-b7e9-8dfaf4cc8064", "node_type": "1", "metadata": {}, "hash": "a6452d020af232299de008a73679c3371963a89d72d8f4811c6bf3453fe6d51c", "class_name": "RelatedNodeInfo"}}, "hash": "d00c02822e092d53c8508bdd4b2891e97cfb1f7a021c4823616c865c1c6a9a0d", "text": "URL: https://blog.langchain.dev/titantakeoff-x-langchain-supercharged-local-inference-for-llms-2/\nTitle: TitanTakeoff x LangChain: Supercharged Local Inference for LLMs\n\nEditor's Note: This post was written in collaboration with the TitanML team. The integration between their NLP development platform + LangChain makes inference LLMs super easy!\n\nChallenges\n\nWith the release of many open source large language models over the past year, developers are increasingly keen to jump on the bandwagon and deploy their own LLMs. However, without specialised knowledge, developers who are experimenting with deploying LLMs on their own hardware may face unexpected technical difficulties. The recent scramble for powerful GPUs has also made it significantly harder to secure sufficient GPU allocation to deploy the best model at the desired latency and scale.\n\nDevelopers are faced with an unappealing choice between suboptimal applications due to compromises on model size and quality, or costly deployments because of manual optimisations and reliance on expensive GPUs, not to mention wasted time dealing with boring and one-off technical eccentricities.\n\nTitan Takeoff Server\n\nFalcon-7B-instruct model running on a CPU with Titan Takeoff Server.\n\nThat being said, deploying your own models locally doesn\u2019t have to be difficult and frustrating. The Titan Takeoff Server offers a simple solution for the local deployment of open-source Large Language Models (LLMs) even on memory-constrained CPUs. With it, users gain the benefits of on-premises inferencing \u2014 reduced latency, enhanced data security, cost savings in the long run, and unparalleled flexibility in model customization and integration without additional complexity, not to mention the ability to deploy larger and more powerful models on memory-constrained hardware.\n\nTitan Takeoff Server offers significant performance benefits for deployment and inferencing of LLMs.\n\nWith its lightning fast inference speeds and support on low cost, readily available devices, the Titan Takeoff Server is suitable for developers who need to constantly deploy, test and refine their LLMs. Through the use of state of the art memory compression techniques, the Titan Takeoff Server offers a 10x improvement in throughput, a 3-4x improvement in latency and a 4\u201320x cost saving through the use of smaller GPUs, in comparison with the base model implementation. In an era where control and efficiency are paramount, the Titan Takeoff Server stands out as an optimal solution for deploying and inferencing LLMs.\n\nSeamless Integration with LangChain\n\nWith the recent integration of Titan Takeoff into LangChain, users will be able to inference their LLMs with minimal setup and coding overhead. You can view a short demonstration of how to use the LangChain integration with Titan Takeoff:\n\nDemo of the Titan Takeoff X LangChain integration\n\nHere is how you can start deploying and inferencing your LLMs in these simple steps. Install the Iris CLI, which will allow you to run the Titan Takeoff Server\n\npip install titan-iris\n\nStart the Takeoff Server, specifying the model name on HuggingFace, as well as the device if you\u2019re using a GPU. This will pull the model from the HuggingFace server, allowing you to inference the model locally.\n\niris takeoff --model tiiuae/falcon-7b-instruct --device cuda\n\nThe Takeoff server is now ready. You can then initialise the LLM object by providing it with a custom port (if not running the Takeoff server on the default port 8000) or other generation parameters such as temperature. There is also an option to specify a streaming flag.\n\n\n\n\n\nllm = TitanTakeoff(port=5000, temperature=0.8, streaming=True)output = llm(\"What is the weather in London in August?\")print(output) # Output: The weather in London in August can vary, with some sunny days and occasional rain showers. The average temperature is around 20-25\u00b0C (68-77\u00b0F).\n\nWith these simple steps, you have made your first inference call to an LLM with the Titan Takeoff Server running right on your local machine. For more examples on using the Takeoff x LangChain integration, view our guide here.\n\nConclusion\n\nThe integration of Titan\u2019s Takeoff server with LangChain marks a transformative phase in the development and deployment of language model-powered applications. As developers and enterprises seek faster, more efficient, and cost-effective ways to leverage the capabilities of LLMs, solutions like this pave the way for a smarter, seamless, and supercharged future.\n\nAbout TitanML\n\nTitanML is an NLP development platform and service focusing on the deployability of LLMs. Our Takeoff Server is a hyper-optimised LLM inference server that \u2018just works\u2019, making it the fastest and simplest way to experiment with and deploy LLMs locally.\n\nOur documentation and Discord community", "start_char_idx": 0, "end_char_idx": 4820, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b430f89c-e810-4466-b7e9-8dfaf4cc8064": {"__data__": {"id_": "b430f89c-e810-4466-b7e9-8dfaf4cc8064", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_titantakeoff-x-langchain-supercharged-local-inference-for-llms-2_.txt", "file_name": "blog.langchain.dev_titantakeoff-x-langchain-supercharged-local-inference-for-llms-2_.txt", "file_type": "text/plain", "file_size": 4865, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ca0e92d5-47c1-4dc2-b585-920412c51542", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_titantakeoff-x-langchain-supercharged-local-inference-for-llms-2_.txt", "file_name": "blog.langchain.dev_titantakeoff-x-langchain-supercharged-local-inference-for-llms-2_.txt", "file_type": "text/plain", "file_size": 4865, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "ba09750dffd7ff47ef3961e2a4d1dfa537b57f2da3f5fa0b928d34cff404c91a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb4b951c-f4ea-4d8e-b3e4-0a3ca0e988db", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_titantakeoff-x-langchain-supercharged-local-inference-for-llms-2_.txt", "file_name": "blog.langchain.dev_titantakeoff-x-langchain-supercharged-local-inference-for-llms-2_.txt", "file_type": "text/plain", "file_size": 4865, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "d00c02822e092d53c8508bdd4b2891e97cfb1f7a021c4823616c865c1c6a9a0d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1876db37-d180-46c0-8bc7-4397949ae4bd", "node_type": "1", "metadata": {}, "hash": "78e9942924b1c1dd11025e8008120f6851a8628c70116e9280c15e560691b56a", "class_name": "RelatedNodeInfo"}}, "hash": "a6452d020af232299de008a73679c3371963a89d72d8f4811c6bf3453fe6d51c", "text": "the fastest and simplest way to experiment with and deploy LLMs locally.\n\nOur documentation and Discord community are here to support you.", "start_char_idx": 4707, "end_char_idx": 4845, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1876db37-d180-46c0-8bc7-4397949ae4bd": {"__data__": {"id_": "1876db37-d180-46c0-8bc7-4397949ae4bd", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_using-langsmith-to-support-fine-tuning-of-open-source-llms_.txt", "file_name": "blog.langchain.dev_using-langsmith-to-support-fine-tuning-of-open-source-llms_.txt", "file_type": "text/plain", "file_size": 13638, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aff38af1-2b57-4a15-9ad4-916f9a964e9e", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_using-langsmith-to-support-fine-tuning-of-open-source-llms_.txt", "file_name": "blog.langchain.dev_using-langsmith-to-support-fine-tuning-of-open-source-llms_.txt", "file_type": "text/plain", "file_size": 13638, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "9f87e54b97bd6d7a3f920c76f724dd40cecdce92289e44109efcaec31d0d5b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b430f89c-e810-4466-b7e9-8dfaf4cc8064", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_titantakeoff-x-langchain-supercharged-local-inference-for-llms-2_.txt", "file_name": "blog.langchain.dev_titantakeoff-x-langchain-supercharged-local-inference-for-llms-2_.txt", "file_type": "text/plain", "file_size": 4865, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "a6452d020af232299de008a73679c3371963a89d72d8f4811c6bf3453fe6d51c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dae2bd5e-f99f-4fac-9202-d719e49a56f5", "node_type": "1", "metadata": {}, "hash": "4f2ff19d1ca1b9c329fdf3f5aa2b79da50a068c676142bd4184384a4d1805e50", "class_name": "RelatedNodeInfo"}}, "hash": "78e9942924b1c1dd11025e8008120f6851a8628c70116e9280c15e560691b56a", "text": "URL: https://blog.langchain.dev/using-langsmith-to-support-fine-tuning-of-open-source-llms/\nTitle: Using LangSmith to Support Fine-tuning\n\nSummary\n\nWe created a guide for fine-tuning and evaluating LLMs using LangSmith for dataset management and evaluation. We did this both with an open source LLM on CoLab and HuggingFace for model training, as well as OpenAI's new finetuning service. As a test case, we fine-tuned LLaMA2-7b-chat and gpt-3.5-turbo for an extraction task (knowledge graph triple extraction) using training data exported from LangSmith and also evaluated the results using LangSmith. The CoLab guide is here.\n\nContext\n\nInterest in fine-tuning has grown rapidly over the past few weeks. This can largely be attributed to two causes.\n\nFirst, the open source LLM ecosystem has grown remarkably, progressing from open source LLMs that lagged the state-of-the-art (SOTA) by a wide margin to near-SOTA (e.g., Llama-2) LLMs that can be run on consumer laptops in ~1 year! The drivers of this progress include increasingly large corpus of training data (x-axis, below) and fine-tuning (y-axis) for instruction-following and better-human-aligned responses. Performant open source base models offer benefits such as cost savings (e.g., for token-intensive tasks), privacy, and - with fine tuning - the opportunity to exceed SOTA LLMs with much smaller open source for highly specific tasks.\n\nSecond, the leading LLM provider, OpenAI, has released fine-tuning support for gpt-3.5-turbo (and other models). Previously, fine-tuning was only available for older models. These models were not nearly as capable as newer models, meaning even after fine-tuning, they often weren't competitive with GPT-3.5 and few-shot examples. Now that newer models can be fine-tuned, many expect this to change.\n\nSome have argued that organizations may opt for many specialist fine-tuned LLMs derived from open source base models over a single massive generalist model. With this and libraries such as HuggingFace to support fine-tuning in mind, you may be curious about when and how to fine-tune. This guide provides an overview and shows how LangSmith can support the process.\n\nWhen to fine-tune\n\nLLMs can learn new knowledge in at least two ways: weight updates (e.g., pre-training or fine-tuning) or prompting (e.g., retrieval augmented generation, RAG). Model weights are like long-term memory whereas the prompt is like short-term memory. This OpenAI cookbook has a useful analogy: When you fine-tune a model, it's like studying for an exam one week away. When you insert knowledge into the prompt (e.g., via retrieval), it's like taking an exam with open notes.\n\nWith this in mind, fine-tuning is not advised for teaching an LLM new knowledge or factual recall; this talk from John Schulman of OpenAI notes that fine-tuning can increase hallucinations. Fine-tuning is better suited for teaching specialized tasks, but it should be considered relative to prompting or RAG. As discussed here, fine-tuning can be helpful for well-defined tasks with ample examples and / or LLMs that lack the in-context learning capacity for few-shot prompting. This Anyscale blog summarizes these points well: fine-tuning is for form, not facts.\n\nHow to fine-tune\n\nThere\u2019s a number of helpful LLaMA fine-tuning recipes that have been released for tasks such as chat using a subset of the OpenAssistant corpus in HuggingFace. Notably, these work on a single CoLab GPU, which makes the workflow accessible. However, two of the largest remaining pain points in fine-tuning are dataset collection / cleaning and evaluation. Below we show how LangSmith can be used to help address both of these (green, below).\n\nTask\n\nTasks like classification / tagging or extraction are well-suited for fine-tuning; Anyscale reported promising results (exceeding GPT4) by fine-tuning LLaMA 7B and 13B LLMs on extraction, text-to-SQL, and QA. As a test case, we chose extraction of knowledge triples of the form (subject, relation, object) from text: the subject and object are entities and the relation is a property or connection between them. Triples can then be used to build knowledge graphs, databases that store information about entities and their relationships. We built an public Streamlit app for triple extraction from user input text to explore the capacity of LLMs (GPT3.5 or 4) to extract triples with function calling. In parallel, we fine-tuned", "start_char_idx": 0, "end_char_idx": 4415, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dae2bd5e-f99f-4fac-9202-d719e49a56f5": {"__data__": {"id_": "dae2bd5e-f99f-4fac-9202-d719e49a56f5", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_using-langsmith-to-support-fine-tuning-of-open-source-llms_.txt", "file_name": "blog.langchain.dev_using-langsmith-to-support-fine-tuning-of-open-source-llms_.txt", "file_type": "text/plain", "file_size": 13638, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aff38af1-2b57-4a15-9ad4-916f9a964e9e", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_using-langsmith-to-support-fine-tuning-of-open-source-llms_.txt", "file_name": "blog.langchain.dev_using-langsmith-to-support-fine-tuning-of-open-source-llms_.txt", "file_type": "text/plain", "file_size": 13638, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "9f87e54b97bd6d7a3f920c76f724dd40cecdce92289e44109efcaec31d0d5b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1876db37-d180-46c0-8bc7-4397949ae4bd", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_using-langsmith-to-support-fine-tuning-of-open-source-llms_.txt", "file_name": "blog.langchain.dev_using-langsmith-to-support-fine-tuning-of-open-source-llms_.txt", "file_type": "text/plain", "file_size": 13638, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "78e9942924b1c1dd11025e8008120f6851a8628c70116e9280c15e560691b56a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a9ef591-d1a1-492c-8fc5-36cc23356bce", "node_type": "1", "metadata": {}, "hash": "ce005dc37f7309859717977bc7e2f3efe455713789ea3144430bb7c8ed92acdc", "class_name": "RelatedNodeInfo"}}, "hash": "4f2ff19d1ca1b9c329fdf3f5aa2b79da50a068c676142bd4184384a4d1805e50", "text": "or 4) to extract triples with function calling. In parallel, we fine-tuned LLaMA2-7b-chat and GPT-3.5 for this task using public datasets.\n\nDataset\n\nDataset collection and cleaning is often a challenging task in training LLMs. When a project is set up in LangSmith, generations are automatically logged, making it easy to get a large quantity of data. LangSmith offers a queryable interface so you can use user feedback filters, tags, and other metrics to select for poor quality cases, correct the outputs in the app, and save to datasets that you can use to improve the results of your model (see below).\n\nAs an example, we created LangSmith train and test datasets from knowledge graph triples in the public BenchIE and CarbIE datasets; we converted them to a shared JSON format with each triplet represented as {s: subject, object: object, relation: relationship} and randomly split the combined data to into a train set of ~1500 labeled sentences and a test set of 100 sentences. The CoLab shows how to easily load LangSmith datasets. Once loaded, we create instructions for fine-tuning using the system prompt below and LLaMA instruction tokens (as done here):\n\n\"you are a model tasked with extracting knowledge graph triples from given text. \" \"The triples consist of:\n\n\" \"- \\\"s\\\": the subject, which is the main entity the statement is about.\n\n\" \"- \\\"object\\\": the object, which is the entity or concept that the subject is related to.\n\n\" \"- \\\"relation\\\": the relationship between the subject and the object. \" \"Given an input sentence, output the triples that represent the knowledge contained in the sentence.\"\n\nQuantization\n\nAs shown in the excellent guide here, we fine-tune a 7B parameter LLaMA chat model. We want to do this on a single GPU (HuggingFace guide here), which presents a challenge: if each parameter is 32 bits, a 7B parameter LLaMA2 model will occupy 28GB, which exceeds the VRAM of a T4 (16GB). To address this, we quantize the model parameters, which means binning the values (e.g., to 16 values in the case of 4 bit quantization), which reduces the memory required to store the model (7B * 4 bits / parameter =3.5GB) ~8-fold.\n\nLoRA and qLoRA\n\nWith the model in memory, we still need a way to fine-tune within the constraint of the remaining GPU resources. For this, parameter-efficient fine-tuning (PEFT) is a common approach: LoRA freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the model architecture (see here), reducing the number of trainable parameters for fine-tuning (e.g., ~1% of the total model). qLoRA extends this by freezing quantized weights. During fine-tuning frozen weights are de-quantized for forward and backward passes, but only the (small set of) LoRA adapters are saved in memory, reducing fine-tuned model footprint.\n\nTraining\n\nWe started with a pre-trained LLaMA-7b chat model llama-2-7b-chat-hf and fine-tuned on the ~1500 instructions in CoLab on an A100. For training configuration, we used LLaMA fine-tuning parameters from here: BitsAndBytes loads the base model with 4-bit precision but forward and backward passes are in fp16. We use Supervised Fine-Tuning (SFT) for fine-tuning on our instructions, which is quite fast (< 15 minutes) on an A100 for this small data volume.\n\nOpenAI Finetuning\n\nTo fine-tune OpenAI's GPT-3.5-turbo chat model, we selected 50 examples from the training dataset and converted them to a list of chat messages in the expected format:\n\n{ \"messages\": [ { \"role\": \"user\", \"content\": \"Extract triplets from the following sentence:\n\n\n\n{sentence}\"}, { \"role\": \"assistant\", \"content\": \"{triples}\" }, ... ] }\n\nSince the base model is so broadly capable, we don't need much data to achieve the desired behavior. The training data is meant to steer the model to always generate the correct format and style rather than to teach it substantial information. As we will see in the evaluation section below, the 50 training examples were sufficient to get the model to predict triplets in the correct format each time.\n\nWe uploaded the fine-tuning data to via the openai SDK and used the resulting model directly in LangChain's ChatOpenAI class. The fine-tuned model can be used directly:\n\nfrom langchain.chat_models import ChatOpenAI llm =", "start_char_idx": 4341, "end_char_idx": 8617, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a9ef591-d1a1-492c-8fc5-36cc23356bce": {"__data__": {"id_": "0a9ef591-d1a1-492c-8fc5-36cc23356bce", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_using-langsmith-to-support-fine-tuning-of-open-source-llms_.txt", "file_name": "blog.langchain.dev_using-langsmith-to-support-fine-tuning-of-open-source-llms_.txt", "file_type": "text/plain", "file_size": 13638, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aff38af1-2b57-4a15-9ad4-916f9a964e9e", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_using-langsmith-to-support-fine-tuning-of-open-source-llms_.txt", "file_name": "blog.langchain.dev_using-langsmith-to-support-fine-tuning-of-open-source-llms_.txt", "file_type": "text/plain", "file_size": 13638, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "9f87e54b97bd6d7a3f920c76f724dd40cecdce92289e44109efcaec31d0d5b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dae2bd5e-f99f-4fac-9202-d719e49a56f5", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_using-langsmith-to-support-fine-tuning-of-open-source-llms_.txt", "file_name": "blog.langchain.dev_using-langsmith-to-support-fine-tuning-of-open-source-llms_.txt", "file_type": "text/plain", "file_size": 13638, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "4f2ff19d1ca1b9c329fdf3f5aa2b79da50a068c676142bd4184384a4d1805e50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8075beab-b865-454f-b9ec-9d7e69c7757d", "node_type": "1", "metadata": {}, "hash": "483d8fc30347e69334a17c92dc92f227e5770095959cc44ae0f061d04f89224f", "class_name": "RelatedNodeInfo"}}, "hash": "ce005dc37f7309859717977bc7e2f3efe455713789ea3144430bb7c8ed92acdc", "text": "model can be used directly:\n\nfrom langchain.chat_models import ChatOpenAI llm = ChatOpenAI(model=\"ft:gpt-3.5-turbo-0613:{openaiOrg}::{modelId}\")\n\nThe whole process took only a few minutes and required no code changes in our chain, apart from removing the need to add in few-shot examples in the prompt template. We then benchmarked this model against the others to quantify its performance. You can see an example of the whole process in our CoLab notebook.\n\nEvaluation\n\nWe evaluated each model using LangSmith, applying an LLM (GPT-4) evaluator to grade each prediction, which is instructed to identify factual discrepancies between the labels and the predicted triplets. This penalized results when it predicts triplets that are not present in the label or when the prediction fails to include a triplet, but it will be lenient if the exact wording of the object or relation differs in a non-meaningful way. The evaluator grades results on a scale from 0-100. We ran the evaluations in CoLab to easily configure our custom evaluator and chains to test.\n\nThe table below shows the evaluation results for the llama base chat model and the fine-tuned variant. For comparison, we also benchmarked 3 chains using OpenAI\u2019s chat models: gpt-3.5-turbo using a few-shot prompt, a gpt-3.5-turbo model fine-tuned on 50 training data points, and a few-shot gpt-4 chain:\n\nFew-shot prompting of GPT-4 performs the best\n\nFine-tuned GPT-3.5 is the runner up\n\nFine-tuned LLaMA-7b-chat exceeds the performance of GPT-3.5\n\nAnd fine-tuned LLaMA-7b-chat is ~29% better than baseline LLaMA-7b-chat\n\nFine-tuned llama-2 Base llama-2 chat model few-shot gpt-3.5-turbo finetuned gpt-3.5-turbo few-shot gpt-4 Score 49% 38% 40% 56% 59%\n\nAnalysis\n\nWe use LangSmith to characterize common failure modes and better understand where the fine-tuned model behaves better than the base model. By filtering for low-scored predictions, we can see cases where baseline LLaMA-7b-chat comically hallucinates: in the below case the LLM thinks that the subject is Homer Simpson and casually answers outside the scope of the desired format (here). The fine-tuned LLaMA-7b-chat (here) gets much closer to the reference (ground truth).\n\nExample of the base model hallucinating by inferring too much information from its pretrained knowledge.\n\nEven with few-shot examples, the baseline LLaMA-7b-chat has a tendency to answer in an informal chit-chat style (here).\n\nThe base llama model often injects narrative even when not asked.\n\nIn contrast, the fine-tuned model tends to generate text (here) that is aligned with the desired answer format and refrains from adding unwanted dialog.\n\nThe fine-tuned model still had instances where it failed to generate the desired content. In the example below (link) the model repeated the instructions instead of generating the extracted results. This could be fixed with various approach such as different decoding parameters (or using logit biasing), more instructions, a larger base model (e.g., 13b), or improved instructions (prompt engineering).\n\nThe fine-tuned model occasionally become repetitive.\n\nWe can contrast the above few-shot prompted GPT-4 below (link), which is able to extract reasonable triplets without fine-tuning on the training dataset.\n\nConclusion\n\nWe can distill a few central lessons:\n\nLangSmith can help address pain points in the fine-tuning workflow, such as data collection, evaluation, and inspection of results. We show how LangSmith makes it easy to collect and load datasets as well as run evaluations and inspect specific generations.\n\nWe show how LangSmith makes it easy to collect and load datasets as well as run evaluations and inspect specific generations. RAG or few-shot prompting should be considered carefully before jumping to more challenging and costly fine-tuning. Few-shot prompting GPT-4 actually performs better than all of our fine-tuned variants.\n\nFew-shot prompting GPT-4 actually performs better than all of our fine-tuned variants. Fine-tuning small open source models on well-defined tasks can outperform much larger generalist models. Just as Anyscale and other have reported previously, we see that a fine-tuned LLaMA2-chat-7B model exceeds the performance of a much larger generalist LLM (GPT-3.5-turbo).\n\nJust as Anyscale and other have reported previously, we see that a fine-tuned", "start_char_idx": 8538, "end_char_idx": 12876, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8075beab-b865-454f-b9ec-9d7e69c7757d": {"__data__": {"id_": "8075beab-b865-454f-b9ec-9d7e69c7757d", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_using-langsmith-to-support-fine-tuning-of-open-source-llms_.txt", "file_name": "blog.langchain.dev_using-langsmith-to-support-fine-tuning-of-open-source-llms_.txt", "file_type": "text/plain", "file_size": 13638, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aff38af1-2b57-4a15-9ad4-916f9a964e9e", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_using-langsmith-to-support-fine-tuning-of-open-source-llms_.txt", "file_name": "blog.langchain.dev_using-langsmith-to-support-fine-tuning-of-open-source-llms_.txt", "file_type": "text/plain", "file_size": 13638, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "9f87e54b97bd6d7a3f920c76f724dd40cecdce92289e44109efcaec31d0d5b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a9ef591-d1a1-492c-8fc5-36cc23356bce", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_using-langsmith-to-support-fine-tuning-of-open-source-llms_.txt", "file_name": "blog.langchain.dev_using-langsmith-to-support-fine-tuning-of-open-source-llms_.txt", "file_type": "text/plain", "file_size": 13638, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "ce005dc37f7309859717977bc7e2f3efe455713789ea3144430bb7c8ed92acdc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f458fd93-2978-449c-a835-48d81edb0d57", "node_type": "1", "metadata": {}, "hash": "355af0e37c70dcdb040e4fd0e05b0ccd3eb84ef337f862b7eb665443dc2d3e59", "class_name": "RelatedNodeInfo"}}, "hash": "483d8fc30347e69334a17c92dc92f227e5770095959cc44ae0f061d04f89224f", "text": "as Anyscale and other have reported previously, we see that a fine-tuned LLaMA2-chat-7B model exceeds the performance of a much larger generalist LLM (GPT-3.5-turbo). There are numerous levers to improve fine-tuning performance, most notably careful task definition and dataset curation. Some works, such as LIMA, have reported impressive performance by fine-tuning LLaMA on as few as 1000 instruction that were selected for high quality. Further data collection / cleaning, using a larger base model (e.g., LLaMA 13B), and scaling up with GPU services for fine-tuning (Lambda Labs, Modal, Vast.ai, Mosaic, Anyscale, etc) are all foreseeable ways to improve these results.\n\nOverall, these results (and the linked CoLab) provide a quick recipe for fine-tuning open source LLMs using LangSmith a tool to help with the full workflow.", "start_char_idx": 12804, "end_char_idx": 13634, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f458fd93-2978-449c-a835-48d81edb0d57": {"__data__": {"id_": "f458fd93-2978-449c-a835-48d81edb0d57", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_villagers-x-langsmith-simulating-multi-agent-social-networks_.txt", "file_name": "blog.langchain.dev_villagers-x-langsmith-simulating-multi-agent-social-networks_.txt", "file_type": "text/plain", "file_size": 2412, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "170a97eb-81bd-449d-be31-fdb5631aef90", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_villagers-x-langsmith-simulating-multi-agent-social-networks_.txt", "file_name": "blog.langchain.dev_villagers-x-langsmith-simulating-multi-agent-social-networks_.txt", "file_type": "text/plain", "file_size": 2412, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "a7b91ffb760718b16231c894241de8404bf2cd515f85c235fbc764e84a29ea15", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8075beab-b865-454f-b9ec-9d7e69c7757d", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_using-langsmith-to-support-fine-tuning-of-open-source-llms_.txt", "file_name": "blog.langchain.dev_using-langsmith-to-support-fine-tuning-of-open-source-llms_.txt", "file_type": "text/plain", "file_size": 13638, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "483d8fc30347e69334a17c92dc92f227e5770095959cc44ae0f061d04f89224f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76b4e9d6-cbad-4dfe-b089-76a8af22d836", "node_type": "1", "metadata": {}, "hash": "237a31ee69a2836186b459766a74d5c873d0281e74b40f819c897dce838f7bf3", "class_name": "RelatedNodeInfo"}}, "hash": "355af0e37c70dcdb040e4fd0e05b0ccd3eb84ef337f862b7eb665443dc2d3e59", "text": "URL: https://blog.langchain.dev/villagers-x-langsmith-simulating-multi-agent-social-networks/\nTitle: Villagers x LangSmith: Simulating multi-agent social networks with LangSmith\n\nEditor's Note: This post was written in collaboration with Kevin Hu, Tae Hyoung Jo, John Kim, and Tejal Patwardhan from the Villagers team. Villagers came in second at a recent Anthropic hackathon. We really LOVED this project as it shows off complex prompt engineering with their multi-agent social network simulation that runs many agents in parallel. We were really excited to see how LangSmith could help the team automate traces, quickly iterate on prompts, and efficiently debug for this complex use-case!\n\n\n\nWe are excited to write about our experience building a proof-of-concept for simulated multi-agent social networks using LangSmith. Simulating language-based human interactions on social networks has shown potential across economics, politics, sociology, business, and policy applications (e.g., [1], [2]). We use the example of a text-based online community (Twitter/X) with real user personas to demonstrate how LLMs can be used to create realistic multi-agent simulations.\n\n\n\n\n\n\n\nBuilding a useful simulation requires mimicking what an actual user would do, ideally based on histories of past behavior. We built agents to simulate real Twitter users interacting online based on their tweet, retweet, quote tweet, comment, and like history. Each user is an agent with their own specific prompt based on their past history. We then tested the response of the community to various ad campaigns from brands, political statements from candidates, and social commentary from comedians. This served as a proof of concept for a new simulation platform to predict engagement, responses, and behavior modification for online social networks.\n\n\n\nOne of the major technical hurdles we encountered was debugging and prompt engineering given the number of agents that would be interacting at once. We were really excited by LangSmith, which allowed us to have automatic traces and to iterate effectively on prompts, helping build the foundation of the multi-agent network.\n\nWith LangSmith, we were able to significantly speed up development time and feel more confident about the quality of our prompts. We found it to be the easiest-to-use LLMops tool for a product that has a high magnitude of agents running in parallel.", "start_char_idx": 0, "end_char_idx": 2406, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76b4e9d6-cbad-4dfe-b089-76a8af22d836": {"__data__": {"id_": "76b4e9d6-cbad-4dfe-b089-76a8af22d836", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_week-of-8-21-langchain-release-notes_.txt", "file_name": "blog.langchain.dev_week-of-8-21-langchain-release-notes_.txt", "file_type": "text/plain", "file_size": 2935, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aec0222c-6689-4c17-843a-09c2360240a6", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_week-of-8-21-langchain-release-notes_.txt", "file_name": "blog.langchain.dev_week-of-8-21-langchain-release-notes_.txt", "file_type": "text/plain", "file_size": 2935, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "d7d668ebadd5fb37a85796f95e99f3531215ee725abaffffc1374301b3688014", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f458fd93-2978-449c-a835-48d81edb0d57", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_villagers-x-langsmith-simulating-multi-agent-social-networks_.txt", "file_name": "blog.langchain.dev_villagers-x-langsmith-simulating-multi-agent-social-networks_.txt", "file_type": "text/plain", "file_size": 2412, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "355af0e37c70dcdb040e4fd0e05b0ccd3eb84ef337f862b7eb665443dc2d3e59", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81bdae04-6371-4ce8-bd3f-5ef4d3ee23ea", "node_type": "1", "metadata": {}, "hash": "6187ab6f4d22c3e10c1c75c52bc6f549522df6e6ce5a8aa11d8d904353506d32", "class_name": "RelatedNodeInfo"}}, "hash": "237a31ee69a2836186b459766a74d5c873d0281e74b40f819c897dce838f7bf3", "text": "URL: https://blog.langchain.dev/week-of-8-21-langchain-release-notes/\nTitle: [Week of 8/21] LangChain Release Notes\n\nNew in Retrieval\n\nThere was a lot happening in the retrieval space these past two weeks, so we wanted to highlight these explicitly!\n\nNew in LangSmith\n\nThis week, we\u2019re focusing on cookbooks as part of our effort to help more developers build end-to-end applications.\n\nUse the run_on_dataset helper to benchmark aggregate metrics and check against a threshold\n\nto benchmark aggregate metrics and check against a threshold Write individual unit tests to make assertions on every row in a dataset\n\nto make assertions on every row in a dataset Make user scores more actionable , with optional comments and corrections\n\n, with optional comments and corrections Evaluate your apps via LLM-based preference scoring\n\nvia LLM-based preference scoring Use LangSmith to test your RAG system and make prompt tweaks to improve the chain's performance to improve overall consistency of your LLM applications\n\nand make prompt tweaks to improve the chain's performance to improve overall consistency of your LLM applications if there are other recipes you\u2019d like to see, tell us about them @hello@langchain.dev\n\nMonitoring Charts: Each project now has a monitor tab that allows you to track important metrics over time including trace count, success rate, and latency. We will be adding more metrics very soon!\n\nNew in Open Source\n\nAdded Fallbacks to the LangChain Expression Language (LCEL): a better way to handle LLM API failures in production-ready LLM applications\n\na better way to handle LLM API failures in production-ready LLM applications Caching Embeddings: Embeddings can be stored or temporarily cached to avoid needing to recompute them.\n\nEmbeddings can be stored or temporarily cached to avoid needing to recompute them. ChatLangChain Improvements: We're benchmarking a bunch of retrieval and agent methods for our \"chat langchain\" app! Interact with the new beta version here.\n\nWe're benchmarking a bunch of retrieval and agent methods for our \"chat langchain\" app! Interact with the new beta version here. Open Source LLM guide: covers open source LLM SOTA (overview fig below) and ways to run them locally (llama.cpp, http://ollama.ai, gpt4all).\n\ncovers open source LLM SOTA (overview fig below) and ways to run them locally (llama.cpp, http://ollama.ai, gpt4all). MultiVector Retriever: a new retrieval algorithm that enables multiple vector embeddings per document, that can be per-chunk, a summary, hypothetical questions, or more\n\na new retrieval algorithm that enables multiple vector embeddings per document, that can be per-chunk, a summary, hypothetical questions, or more OpenAI Adapter: we added an easy way to switch out our OpenAI calls for the variety of other models that LangChain supports\n\nIn case you missed it\n\nUse-cases we love\n\nWant this in your inbox every other week? Subscribe to the blog!", "start_char_idx": 0, "end_char_idx": 2931, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81bdae04-6371-4ce8-bd3f-5ef4d3ee23ea": {"__data__": {"id_": "81bdae04-6371-4ce8-bd3f-5ef4d3ee23ea", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_week-of-8-7_.txt", "file_name": "blog.langchain.dev_week-of-8-7_.txt", "file_type": "text/plain", "file_size": 3454, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6f6dae7e-5f00-4deb-958f-07a241f3289b", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_week-of-8-7_.txt", "file_name": "blog.langchain.dev_week-of-8-7_.txt", "file_type": "text/plain", "file_size": 3454, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "03a483c5fd3e4df925d0ee70afce1d8d59ffc7f2cab2661550c0d984749ee462", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76b4e9d6-cbad-4dfe-b089-76a8af22d836", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_week-of-8-21-langchain-release-notes_.txt", "file_name": "blog.langchain.dev_week-of-8-21-langchain-release-notes_.txt", "file_type": "text/plain", "file_size": 2935, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "237a31ee69a2836186b459766a74d5c873d0281e74b40f819c897dce838f7bf3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33fe22bc-688d-46a6-9ca5-d9f97976c601", "node_type": "1", "metadata": {}, "hash": "625bcecaa09e529505219b6d643fce025aa1d92381a0e65232e41c7dbda024e5", "class_name": "RelatedNodeInfo"}}, "hash": "6187ab6f4d22c3e10c1c75c52bc6f549522df6e6ce5a8aa11d8d904353506d32", "text": "URL: https://blog.langchain.dev/week-of-8-7/\nTitle: [Week of 8/7] LangChain Release Notes\n\nNew in LangSmith\n\nTeams: You can now collaborate in LangSmith! We\u2019ve released support for teams of up to five people. If you need more, get in touch at support@langchain.dev\n\nYou can now collaborate in LangSmith! We\u2019ve released support for teams of up to five people. If you need more, get in touch at support@langchain.dev Cookbooks: Learn how to get more out of LangSmith's debugging, testing, and feedback functionality with these end-to-end recipes in the LangSmith Cookbook repository. Cookbooks added so far: How to use LangSmith if you\u2019re NOT using LangChain\n\nLearn how to get more out of LangSmith's debugging, testing, and feedback functionality with these end-to-end recipes in the LangSmith Cookbook repository. Cookbooks added so far: How to use LangSmith if you\u2019re NOT using LangChain What use-cases would you like to see? Tell us at hello@langchain.dev\n\nUI updates: resizable columns in table views and enhanced navigation\n\nresizable columns in table views and enhanced navigation Performance improvements and Bug Fixes: fixed stuttering occurring on run tree navigation\n\nNew in Open Source:\n\nLangChain Expression Language: a new syntax to create chains with composition. webinar recording here. Cookbook of examples rewriting popular chains here.\n\na new syntax to create chains with composition. webinar recording here. Cookbook of examples rewriting popular chains here. Conversational Retrieval Agents: There have been several emerging trends in LLM applications over the past few months: RAG, chat interfaces, agents. Our newest functionality - conversational retrieval agents - combines them all. blog post here.\n\nThere have been several emerging trends in LLM applications over the past few months: RAG, chat interfaces, agents. Our newest functionality - conversational retrieval agents - combines them all. blog post here. Parent Document Retriever: A new retrieval algorithm that creates small chunks (to allow embeddings to have semantic meaning) and fetches the parent documents those chunks came from (to capture full context). Docs here.\n\nA new retrieval algorithm that creates small chunks (to allow embeddings to have semantic meaning) and fetches the parent documents those chunks came from (to capture full context). Docs here. Text Splitting Playground: Chunking text into appropriate splits is seemingly trivial yet very nuanced. Open sourcing a playground to help explore different text splitting strategies. GitHub here. Hosted Playground here.\n\nChunking text into appropriate splits is seemingly trivial yet very nuanced. Open sourcing a playground to help explore different text splitting strategies. GitHub here. Hosted Playground here. Use Case Documentation: We\u2019ve updated documentation to better highlight all the use cases of LangChain. Big shout out to community members Francisco Pingham and Manuel Soria for their help here!\n\nWe\u2019ve updated documentation to better highlight all the use cases of LangChain. Big shout out to community members Francisco Pingham and Manuel Soria for their help here! LangChain.js + Next.js starter template: more streaming and prompt customization developer experience improvements via popular use-cases you can clone, remix, and deploy on Vercel in a few clicks\n\nIn case you missed it\n\nUse-cases we love\n\nThank you\u2019s\n\nWant this in your inbox every other week? Sign up here.", "start_char_idx": 0, "end_char_idx": 3440, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33fe22bc-688d-46a6-9ca5-d9f97976c601": {"__data__": {"id_": "33fe22bc-688d-46a6-9ca5-d9f97976c601", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_week-of-9-18-langchain-release-notes_.txt", "file_name": "blog.langchain.dev_week-of-9-18-langchain-release-notes_.txt", "file_type": "text/plain", "file_size": 6125, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "822deb9f-11f3-49cc-9315-71846cfc6731", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_week-of-9-18-langchain-release-notes_.txt", "file_name": "blog.langchain.dev_week-of-9-18-langchain-release-notes_.txt", "file_type": "text/plain", "file_size": 6125, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "59ab949be7a51f6fcff2d6b40af3cfc2c1ce53993a12516265ee0e09d65fb9ac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81bdae04-6371-4ce8-bd3f-5ef4d3ee23ea", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_week-of-8-7_.txt", "file_name": "blog.langchain.dev_week-of-8-7_.txt", "file_type": "text/plain", "file_size": 3454, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "6187ab6f4d22c3e10c1c75c52bc6f549522df6e6ce5a8aa11d8d904353506d32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "320fcc8a-c626-4a66-927a-b524902e31dc", "node_type": "1", "metadata": {}, "hash": "16b0504428b40de9d7a91dacf3e2d0b1f9db92364a3afbc2f5f50a09254a6f98", "class_name": "RelatedNodeInfo"}}, "hash": "625bcecaa09e529505219b6d643fce025aa1d92381a0e65232e41c7dbda024e5", "text": "URL: https://blog.langchain.dev/week-of-9-18-langchain-release-notes/\nTitle: [Week of 9/18] LangChain Release Notes\n\nNew in LangSmith\n\nOrg Support in LangChain Hub: share and collaborate on prompts across your team. Easily pull in organizationally-approved prompts into your LangChain code.\n\nshare and collaborate on prompts across your team. Easily pull in organizationally-approved prompts into your LangChain code. Need access to LangSmith to collaborate on prompts with your team? Fill out this form.\n\nto LangSmith to collaborate on prompts with your team? Fill out this form. Collapsable Traces: makes it way easier to navigate errors (especially for long runs).\n\nNew Cookbooks:\n\nPrompt Tracking on Hub. Prototype and share LLM prompts. An example of how to use it within a RAG chain here.\n\nPrototype and share LLM prompts. An example of how to use it within a RAG chain here. Edit and commit directly from the playground. Jump between prototyping and debugging directly in the browser. Save and use within any custom chain. Example here.\n\nJump between prototyping and debugging directly in the browser. Save and use within any custom chain. Example here. Prompt Versioning for consistent, stable deployments: Deploy and test specific prompt versions so you can be consistent in prod without slowing down your experimentation. Cookbook here.\n\nDeploy and test specific prompt versions so you can be consistent in prod without slowing down your experimentation. Cookbook here. Custom run (trace) naming. Navigate and query traces better with customized chain names. Cookbook here.\n\n\n\nNew in Open Source\n\nRouting in LCEL: Allows for flexibility in choosing what to do, but keeps you in control over a finite number of potential paths. Python docs. JS docs.\n\nAllows for flexibility in choosing what to do, but keeps you in control over a finite number of potential paths. Python docs. JS docs. Stream, batch, async model calls: For all OpenAI, Anthropic, Azure OpenAI, AWS Bedrock, and Google VertexAI LLMs and chat models, we now take advantage of native support for streaming, batching and asynchronous calls wherever possible. To see the full feature list by model integration, see LLMs and Chat models.\n\nFor all OpenAI, Anthropic, Azure OpenAI, AWS Bedrock, and Google VertexAI LLMs and chat models, we now take advantage of native support for streaming, batching and asynchronous calls wherever possible. To see the full feature list by model integration, see LLMs and Chat models. OpenAI InstructGPT 3.5 model: very easy to use in LangChain\u2013just specify the new model gpt-3.5-turbo-instruct in the OpenAI class (we recommend using \"old\" prompts that are NOT in the ChatMessage format)\n\nvery easy to use in LangChain\u2013just specify the new model gpt-3.5-turbo-instruct in the OpenAI class (we recommend using \"old\" prompts that are NOT in the ChatMessage format) New summarization technique\u2013Chain of Density\u2013works with Anthropic, too: Produces 5 summaries iteratively, each one denser and more informative than the prior. Great overview thread by Victor Mota here. Try it out on Prompt Hub here.\n\nProduces 5 summaries iteratively, each one denser and more informative than the prior. Great overview thread by Victor Mota here. Try it out on Prompt Hub here. Customizable Agents: default agents make it easy to prototype but agents in production require more customization. So, we rewrote all 8 agent types using LangChain Expression Language and prompts from the Hub to make them more modular, understandable, and therefore more customizable. (Updated) docs here.\n\nIn case you missed it\n\nComing soon\n\nWebinars\n\nCognitive Architectures for Language Agents Webinar [10/4] : Harrison Chase (LangChain), Charles Frye (The Full Stack, prev. Weights & Biases), Ted Sumers (Princeton), and Shunyu Yao (Princeton) will discuss the recent paper on CoALA (Cognitive Architectures for Language Agents) and the implications for building with agents\n\nHarrison Chase (LangChain), Charles Frye (The Full Stack, prev. Weights & Biases), Ted Sumers (Princeton), and Shunyu Yao (Princeton) will discuss the recent paper on CoALA (Cognitive Architectures for Language Agents) and the implications for building with agents Data Privacy for LLM Applications Webinar [10/12]: we\u2019ll host a conversation with DeepsenseAI and Opaque about how to put your proprietary and/or sensitive data to use, without sacrificing privacy.\n\nHackathons\n\nTED AI Hackathon Kickoff [Oct 14]: we\u2019re offering a prize for the beset LLM app! Learn more about the Hackathon and", "start_char_idx": 0, "end_char_idx": 4533, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "320fcc8a-c626-4a66-927a-b524902e31dc": {"__data__": {"id_": "320fcc8a-c626-4a66-927a-b524902e31dc", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_week-of-9-18-langchain-release-notes_.txt", "file_name": "blog.langchain.dev_week-of-9-18-langchain-release-notes_.txt", "file_type": "text/plain", "file_size": 6125, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "822deb9f-11f3-49cc-9315-71846cfc6731", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_week-of-9-18-langchain-release-notes_.txt", "file_name": "blog.langchain.dev_week-of-9-18-langchain-release-notes_.txt", "file_type": "text/plain", "file_size": 6125, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "59ab949be7a51f6fcff2d6b40af3cfc2c1ce53993a12516265ee0e09d65fb9ac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33fe22bc-688d-46a6-9ca5-d9f97976c601", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_week-of-9-18-langchain-release-notes_.txt", "file_name": "blog.langchain.dev_week-of-9-18-langchain-release-notes_.txt", "file_type": "text/plain", "file_size": 6125, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "625bcecaa09e529505219b6d643fce025aa1d92381a0e65232e41c7dbda024e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8977978-ee4d-4897-bebb-3eebf6a37e6f", "node_type": "1", "metadata": {}, "hash": "1cff9b00003dd2f452743425a6c2254df45f95a3b4342fd82d96059d0743049e", "class_name": "RelatedNodeInfo"}}, "hash": "16b0504428b40de9d7a91dacf3e2d0b1f9db92364a3afbc2f5f50a09254a6f98", "text": "we\u2019re offering a prize for the beset LLM app! Learn more about the Hackathon and check out project ideas/resources for getting started here.\n\nConferences\n\nHarrison Chase, LangChain cofounder and CEO, is speaking about building context-aware reasoning applications with LangChain at:\n\nFavorite Prompts on LangChain Hub\n\n\n\nMade with LangChain\n\nlangchain.com/inspiration is a new gallery of our favorite applications built with LangChain. Sampling below.\n\nLangduel is an LLM-powered dialogue between any two philosophers in history to see which convictions prevail.\n\nis an LLM-powered dialogue between any two philosophers in history to see which convictions prevail. RealChar is fully open source codebase to create, customize and talk to your AI Character/Companion. Have a natural realtime conversation with your AI companion everywhere.\n\nis fully open source codebase to create, customize and talk to your AI Character/Companion. Have a natural realtime conversation with your AI companion everywhere. Tavern is ChatGPT with hundreds of your Google Drive & Notion documents, spreadsheets, and presentations.\n\nis ChatGPT with hundreds of your Google Drive & Notion documents, spreadsheets, and presentations. PTAI generates exercises personalized to you and your requests.\n\ngenerates exercises personalized to you and your requests. Bloom is a LangChain-powered learning companion using metacognitive prompting to achieve increasingly personalized UX over time. It leverages emergent theory of mind abilities in LLMs to cohere to learners and meet them where they are.\n\nBuilding or using an application you think we should add? Email us at hello@langchain.dev.", "start_char_idx": 4453, "end_char_idx": 6113, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8977978-ee4d-4897-bebb-3eebf6a37e6f": {"__data__": {"id_": "d8977978-ee4d-4897-bebb-3eebf6a37e6f", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_xata-x-langchain-new-vector-store-and-memory-store-integrations_.txt", "file_name": "blog.langchain.dev_xata-x-langchain-new-vector-store-and-memory-store-integrations_.txt", "file_type": "text/plain", "file_size": 8357, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "04fb9861-1f52-4be8-ada0-e4da2c5dd23c", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_xata-x-langchain-new-vector-store-and-memory-store-integrations_.txt", "file_name": "blog.langchain.dev_xata-x-langchain-new-vector-store-and-memory-store-integrations_.txt", "file_type": "text/plain", "file_size": 8357, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "6c9ccf6bd24b21c0cd88554b466f7e2d66116e136f5c832c3a231c7de35fbef6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "320fcc8a-c626-4a66-927a-b524902e31dc", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_week-of-9-18-langchain-release-notes_.txt", "file_name": "blog.langchain.dev_week-of-9-18-langchain-release-notes_.txt", "file_type": "text/plain", "file_size": 6125, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "16b0504428b40de9d7a91dacf3e2d0b1f9db92364a3afbc2f5f50a09254a6f98", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6726c641-6646-40a8-9de2-594a14a1f26c", "node_type": "1", "metadata": {}, "hash": "ec3e767544cc07a6a197aeae39cc25ae3a65938df118af7a834faffecc59ae8e", "class_name": "RelatedNodeInfo"}}, "hash": "1cff9b00003dd2f452743425a6c2254df45f95a3b4342fd82d96059d0743049e", "text": "URL: https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/\nTitle: Xata x LangChain: new vector store and memory store integrations\n\nEditor's Note: This post was written in collaboration with the Xata team. We're excited about their new integrations and really enjoyed their deepdive on implementation a Q&A chat bot with them.\n\nOver the past few weeks, we\u2019ve merged four Xata integrations to the LangChain repositories, and today we\u2019re happy to unveil them as part of Xata\u2019s launch week! In this blog post, we\u2019ll take a brief look at what Xata is and why it is a good data companion for AI applications. We\u2019ll also show a code example for implementing a Q&A chat bot that answers questions based on the info in a Xata database (as a vector store) and has long-term memory stored in Xata (as a memory store).\n\nWhat is Xata?\n\nXata is a database platform powered by PostgreSQL. It stores the source-of-truth data in PostgreSQL, but also replicates it automatically to Elasticsearch. This means that it offers functionality from both Postgres (ACID transactions, joins, constraints, etc.) and from Elasticsearch (BM25 full-text search, vector search, hybrid search), behind the same simple serverless API. This covers the functionality needed by the majority of AI applications and because it\u2019s based on PostgreSQL and Elasticsearch, it is reliable and scalable.\n\nXata has client SDKs for both TypeScript/JavaScript and Python and built-in integrations with platforms like GitHub, Vercel, and Netlify.\n\nIn the AI space, beside the LangChain integrations announced here, Xata offers a deep integration with OpenAI for the \u201cChatGPT on your data\u201d use case.\n\nThe integrations\n\nAs of today, the following integrations are available :\n\nXata as a vector store in LangChain. This allows one to store documents with embeddings in a Xata table and perform vector search on them. The integration takes advantage of the newly GA-ed Python SDK. The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.\n\nXata as a vector store in LangChain.js. Same as the Python integration, but for your TypeScript/JavaScript applications.\n\nXata as a memory store in LangChain. This allows storing the chat message history for AI chat sessions in Xata, making it work as \u201cmemory\u201d for LLM applications.The messages are stored in\n\nXata as a memory store in LangChain.js. Same as the Python integration, but for TypeScript/JavaScript.\n\nEach integration comes with one or two code examples in the doc pages linked above.\n\nThe four integrations already make Xata one of the most comprehensive data solutions for LangChain, and we\u2019re just getting started! For the near future, we\u2019re planning to add custom retrievers for the Xata keyword and hybrid search and the Xata Ask AI endpoint.\n\nExample: Conversational Q&A with memory\n\nWhile each LangChain integration comes with at least one minimal code example, in this blog post we\u2019ll look at a more complex example that uses Xata both as a vector store and as a memory store. The application implements the \u201cchat with your data\u201d use case, and allows for follow-up questions. The full code can be found in this repo, which you can also use as a starter-kit for LangChain + Xata applications.\n\nWhile the example application here is written in TypeScript, a similar example using the Python LangChain can be found in this Jupyter notebook.\n\nThe main part of the code looks like this:\n\nimport * as dotenv from \"dotenv\"; import { XataVectorSearch } from \"langchain/vectorstores/xata\"; import { OpenAIEmbeddings } from \"langchain/embeddings/openai\"; import { Document } from \"langchain/document\"; import { ConversationalRetrievalQAChain } from \"langchain/chains\"; import { BufferMemory } from \"langchain/memory\"; import { XataChatMessageHistory } from \"langchain/stores/message/xata\"; import { ChatOpenAI } from \"langchain/chat_models/openai\"; import { getXataClient } from \"./xata.ts\"; dotenv.config(); const client = getXataClient(); /* Create the vector store */ const table = \"docs\"; const embeddings = new OpenAIEmbeddings(); const vectorStore = new XataVectorSearch(embeddings, { client, table }); /* Add documents to the vector store */ const docs = [ new Document({ pageContent: \"Xata is a Serverless Data platform based on PostgreSQL\", }), new Document({ pageContent: \"Xata offers a built-in vector type that can be used to store and query vectors\", }), new Document({ pageContent: \"Xata includes similarity search\", }), ]; const ids = await vectorStore.addDocuments(docs); //", "start_char_idx": 0, "end_char_idx": 4582, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6726c641-6646-40a8-9de2-594a14a1f26c": {"__data__": {"id_": "6726c641-6646-40a8-9de2-594a14a1f26c", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_xata-x-langchain-new-vector-store-and-memory-store-integrations_.txt", "file_name": "blog.langchain.dev_xata-x-langchain-new-vector-store-and-memory-store-integrations_.txt", "file_type": "text/plain", "file_size": 8357, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "04fb9861-1f52-4be8-ada0-e4da2c5dd23c", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_xata-x-langchain-new-vector-store-and-memory-store-integrations_.txt", "file_name": "blog.langchain.dev_xata-x-langchain-new-vector-store-and-memory-store-integrations_.txt", "file_type": "text/plain", "file_size": 8357, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "6c9ccf6bd24b21c0cd88554b466f7e2d66116e136f5c832c3a231c7de35fbef6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8977978-ee4d-4897-bebb-3eebf6a37e6f", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_xata-x-langchain-new-vector-store-and-memory-store-integrations_.txt", "file_name": "blog.langchain.dev_xata-x-langchain-new-vector-store-and-memory-store-integrations_.txt", "file_type": "text/plain", "file_size": 8357, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "1cff9b00003dd2f452743425a6c2254df45f95a3b4342fd82d96059d0743049e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf29aa66-8795-4f15-974b-7bd09d40d29c", "node_type": "1", "metadata": {}, "hash": "047e916604382ef4f7cb5c8d4231da34abd7e17e3dada49f4063998a9bd3be3f", "class_name": "RelatedNodeInfo"}}, "hash": "ec3e767544cc07a6a197aeae39cc25ae3a65938df118af7a834faffecc59ae8e", "text": "\"Xata includes similarity search\", }), ]; const ids = await vectorStore.addDocuments(docs); // eslint-disable-next-line no-promise-executor-return await new Promise((r) => setTimeout(r, 2000)); /* Create the chat memory store */ const memory = new BufferMemory({ chatHistory: new XataChatMessageHistory({ table: \"memory\", sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation client, createTable: false, }), memoryKey: \"chat_history\", }); /* Initialize the LLM to use to answer the question */ const model = new ChatOpenAI({}); /* Create the chain */ const chain = ConversationalRetrievalQAChain.fromLLM( model, vectorStore.asRetriever(), { memory, } ); /* Ask it a question */ const question = \"What is Xata?\"; const res = await chain.call({ question }); console.log(\"Question: \", question); console.log(res); /* Ask it a follow up question */ const followUpQ = \"Can it do vector search?\"; const followUpRes = await chain.call({ question: followUpQ, }); console.log(\"Follow-up question: \", followUpQ); console.log(followUpRes); /* Clear both the vector store and the memory store */ await vectorStore.delete({ ids }); await memory.clear();\n\nLet\u2019s take it piece by piece and see what it does:\n\nFirst, we use Xata as a vector store. In this vector store, we index a few sample documents, but in a real application you can index tens of thousands of documents. These are the documents that our chatbot will use to find answers for user questions. While not shown here, it\u2019s also possible to add custom metadata columns to these documents. You can see the examples on the integration page.\n\n/* Create the vector store */ const table = \"docs\"; const embeddings = new OpenAIEmbeddings(); const vectorStore = new XataVectorSearch(embeddings, { client, table }); /* Add documents to the vector store */ const docs = [ new Document({ pageContent: \"Xata is a Serverless Data platform based on PostgreSQL\", }), new Document({ pageContent: \"Xata offers a built-in vector type that can be used to store and query vectors\", }), new Document({ pageContent: \"Xata includes similarity search\", }), ]; const ids = await vectorStore.addDocuments(docs);\n\nNext, we create a chat memory store, again based on Xata. This stores the messages exchanged by the chat bots with the users in a Xata table. Each conversation gets a session ID, which is then used to retrieve the previous messages in the conversation, so that the context is not lost.\n\n/* Create the chat memory store */ const memory = new BufferMemory({ chatHistory: new XataChatMessageHistory({ table: \"memory\", sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation client, createTable: false, }), memoryKey: \"chat_history\", });\n\nThen we initialize the client for interrogating the model, in this case the OpenAI ChatGPT API:\n\n/* Initialize the LLM to use to answer the question */ const model = new ChatOpenAI({});\n\nAnd finally, put all of them together in a conversational QA chain:\n\n/* Create the chain */ const chain = ConversationalRetrievalQAChain.fromLLM( model, vectorStore.asRetriever(), { memory, } );\n\nIf you look at the data via the Xata UI while the example is running, you will see two tables: docs and memory . The docs table is populated with the documents from the vector store, having a content column and an embedding column of type vector :\n\nThe memory table is populated with the questions and answers from the user and from the AI:\n\nContent hackathon\n\nAs part of the launch week, Xata is also organizing a content hackathon, where you can win prizes and swag by creating apps, writing blogs, recording videos, and more. See the launch week blog post for details.\n\nIf you have any questions or ideas or if you need help implementing Xata with LangChain, join us on Discord or reach out on Twitter.", "start_char_idx": 4488, "end_char_idx": 8323, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf29aa66-8795-4f15-974b-7bd09d40d29c": {"__data__": {"id_": "bf29aa66-8795-4f15-974b-7bd09d40d29c", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_zep-x-langchain-slow-chatbots_.txt", "file_name": "blog.langchain.dev_zep-x-langchain-slow-chatbots_.txt", "file_type": "text/plain", "file_size": 8618, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e97222e-ee39-45c2-b060-6e5ca5995f56", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_zep-x-langchain-slow-chatbots_.txt", "file_name": "blog.langchain.dev_zep-x-langchain-slow-chatbots_.txt", "file_type": "text/plain", "file_size": 8618, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "a0e56bd98834c2075c8c73bd60ad098c363be3308d3086c1079a4e28a8b3721f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6726c641-6646-40a8-9de2-594a14a1f26c", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_xata-x-langchain-new-vector-store-and-memory-store-integrations_.txt", "file_name": "blog.langchain.dev_xata-x-langchain-new-vector-store-and-memory-store-integrations_.txt", "file_type": "text/plain", "file_size": 8357, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "ec3e767544cc07a6a197aeae39cc25ae3a65938df118af7a834faffecc59ae8e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49b17c5e-9050-4dc1-9ca5-bd1e1ebded20", "node_type": "1", "metadata": {}, "hash": "557c006c64e5525a82748483bd199bc5253ef6179180f529caa9e2df99fa8ceb", "class_name": "RelatedNodeInfo"}}, "hash": "047e916604382ef4f7cb5c8d4231da34abd7e17e3dada49f4063998a9bd3be3f", "text": "URL: https://blog.langchain.dev/zep-x-langchain-slow-chatbots/\nTitle: Zep x LangChain: Diagnosing and Fixing Slow Chatbots\n\nEditor\u2019s Note: This blog post was written in collaboration with Zep, an early LangSmith BETA user. We've fielded a lot of questions about the latency of LangChain applications - where it comes from, how to improve. This is a FANTASTIC walkthrough of how LangSmith allows you to easily diagnose the causes of latency in your app, and how different components of the LangChain ecosystem (in this case, Zep) can be used to improve it.\n\nSummary\n\nPoor chatbot response times can result in frustrated users and customer churn. LangChain\u2019s new LangSmith service makes it simple and easy to understand what\u2019s causing latency in an LLM app. In this article, we use LangSmith to diagnose a poorly performing LangChain app and demonstrate how we improved performance by an order of magnitude using the Zep memory store.\n\n\n\nSource code for this article: https://github.com/getzep/zep-by-example/tree/main/langchain/python/langsmith-latency\n\nIf you\u2019ve ever waited several seconds for a web page to load and decided to click elsewhere, you\u2019re not alone. Much has been written about the effect of slow websites and their impact on user engagement and conversion rates. Chatbot response times are no different. While responding too quickly may shove the user into an uncanny valley, responding slowly is problematic, too. Users will become frustrated and less likely to use your service again. They may also view the chatbot as unfair and be unwilling to share personal information.\n\n\n\nUnfortunately, it's pretty easy to build a slow chatbot. Between multiple chains, high LLM latency, and enriching prompts with chat histories, summaries, and results from vector databases, a lot can impact how fast your bot responds.\n\n\n\nInstrumenting and profiling your application can be challenging and very time-consuming. LangChain\u2019s new LangSmith service does this fantastically and without any need to manually instrument your app. In this article, I will walk you through an example chatbot application that while simple, is not dissimilar to one you might be building.\n\n\n\nMy Chatbot is sooo slow\n\nI\u2019ve built a Chatbot app using LangChain, and my users are unhappy and churning.\n\nThe users would like to carry on a conversation with the bot and have it not forget the context and details of prior messages in a conversation. So, when building the bot, I used a LangChain memory class. I\u2019m also using a Retriever, backed by Chroma\u2019s vector database, to recall relevant messages from the distant past. In the future, I might also use a Retriever to ground my app with business documents. Like many Langchain developers, I\u2019ve used OpenAI\u2019s APIs for LLM completion and document embeddings.\n\nUsing the memory instance and retriever, my chain will inject the chat history into the prompt sent to the LLM. LLM context windows are limited, and large prompts cost more and take longer for an LLM to respond to. I, therefore, don\u2019t want to send the entire chat history to the LLM. Instead, I\u2019d like to limit this to the most recent messages and have my chain summarize messages in the more distant past. The prompt will look something like this:\n\n\n\nTo make the above happen, I\u2019m using LangChain\u2019s ConversationSummaryBufferMemory and VectorStoreRetrieverMemory classes.\n\nLangSmith to the rescue\n\nAs mentioned, my chat app is too slow, and my users are churning. I really want to get to the bottom of why this is happening. In the past, I\u2019d have to instrument the app in many different places to capture the timings of various operations, a time-consuming and tricky undertaking. Luckily, the LangChain team has already instrumented the LangChain codebase for us, and LangSmith makes it simple to understand the performance of your app. All I have to do is configure my LangSmith API key and add several descriptive tags to my code.\n\nFor my investigation, I put together a simple test suite using 30 messages, 20 of which are preloaded into the LangChain memory class and 10 of which I use in the experiment itself. Each run of the experiment passes these 10 messages to the chain. I do this five times so that I can understand experimental variance.\n\nThe results in LangSmith are below. Each run of 10 messages has a unique name and is the parent to several chain runs.\n\n\n\nLatency is clearly visible in the LangSmith UI with poor latency marked in red. I can easily drill down into each run of the experiment to better understand why this is occurring. All experiment runs are slow, with a mean of over 7-13s to respond to each message. One of the runs is a significant outlier.\n\n\n\nDrilling into that chain, I", "start_char_idx": 0, "end_char_idx": 4699, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49b17c5e-9050-4dc1-9ca5-bd1e1ebded20": {"__data__": {"id_": "49b17c5e-9050-4dc1-9ca5-bd1e1ebded20", "embedding": null, "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_zep-x-langchain-slow-chatbots_.txt", "file_name": "blog.langchain.dev_zep-x-langchain-slow-chatbots_.txt", "file_type": "text/plain", "file_size": 8618, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e97222e-ee39-45c2-b060-6e5ca5995f56", "node_type": "4", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_zep-x-langchain-slow-chatbots_.txt", "file_name": "blog.langchain.dev_zep-x-langchain-slow-chatbots_.txt", "file_type": "text/plain", "file_size": 8618, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "a0e56bd98834c2075c8c73bd60ad098c363be3308d3086c1079a4e28a8b3721f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf29aa66-8795-4f15-974b-7bd09d40d29c", "node_type": "1", "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_zep-x-langchain-slow-chatbots_.txt", "file_name": "blog.langchain.dev_zep-x-langchain-slow-chatbots_.txt", "file_type": "text/plain", "file_size": 8618, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}, "hash": "047e916604382ef4f7cb5c8d4231da34abd7e17e3dada49f4063998a9bd3be3f", "class_name": "RelatedNodeInfo"}}, "hash": "557c006c64e5525a82748483bd199bc5253ef6179180f529caa9e2df99fa8ceb", "text": "to each message. One of the runs is a significant outlier.\n\n\n\nDrilling into that chain, I see a poor response time from the OpenAI API, taking over 5 minutes to respond to the request. It\u2019s possible that the initial request failed and there were retries. Unfortunately, the OpenAI API can sometimes see rate-limiting and high variance in latency.\n\n\n\nLet\u2019s put aside the outlier and work to understand why each message turn of our chatbot is slow. I noticed that the majority of the time my chain spends responding to my users is the result of the ConversationSummaryBufferMemory\u2019s summarization chain. For an arbitrarily selected response turn, summarization takes almost 6s of the 7s total response time. And this occurs every single time our chatbot runs. That\u2019s not good.\n\n\n\nUsing Zep as an alternative memory service\n\nZep is an open source long-term memory store that persists, summarizes, embeds, indexes, and enriches LLM app / chatbot histories. Importantly, Zep is fast. Summarization, embedding, and message enrichment all happen asynchronously, outside of the chat loop. It also supports stateless app architectures as chat histories are not held in memory, unlike ConversationSummaryBufferMemory and many other LangChain memory classes.\n\n\n\nZep\u2019s ZepMemory and ZepRetriever classes are shipped in the LangChain codebase for Python and JS and are drop-in replacements for LangChain\u2019s native classes. Rerunning the experiment with Zep is super simple. I installed the Zep server using Zep\u2019s docker-compose setup and modified my code.\n\nI also don\u2019t need to use a separate vector database for semantically retrieving historical chat memories. Zep automatically embeds and indexes messages.\n\nThe results in LangSmith are below. The total chain runtime for each experiment run is down to ~16 seconds, almost entirely explained by OpenAI API latency. Zep generates summaries in the background, which are automatically added by the ZepMemory instance to my prompt alongside the most recent chat history.\n\n\n\nThere\u2019s an outlier chain run: OpenAI\u2019s API spikey latency strikes again!\n\nLet\u2019s take a more quantitative look at the data. First comparing the distribution of run times, p95 and p99 for each experiment. There were 5 experiments, and we ran them for both chains using the ConversationSummaryBufferMemory and ZepMemory. The chart below uses a log scale for latency. As we saw above, Zep\u2019s runtimes were an order of magnitude lower.\n\nFor good measure, I also analyzed the impact of the VectorStoreRetrieverMemory on the app\u2019s response time below.\n\n\n\nWhile certainly not as problematic as the ConversationSummaryBufferMemory, the VectorStoreRetrieverMemory and OpenAI for embeddings were still far slower than using the ZepRetriever. Zep can be configured to use a SentenceTransformer embedding model running in the Zep service, which offers far lower latency than calling out to the OpenAI embedding service.\n\n\n\nSumming it all up\n\nI\u2019ve demonstrated how to diagnose LangChain chatbot app latency issues using the new LangSmith service. The culprit here was the ConversationSummaryBufferMemory implementation, which I easily swapped out with Zep, seeing a magnitude-level improvement in latency. LangSmith is a great platform for more than just diagnosing latency issues, with tools for testing and evaluating the correctness of chains and more.\n\nExperimental Setup\n\nI ran all tests on my Apple M1 Pro 14\u201d 16GB RAM. For the Zep results, I ran the standard Zep v0.8.1 docker-compose setup with Docker configured for 4GB RAM and 4 cores.\n\n\n\nThe LLM used for all tests was OpenAI\u2019s gpt-3.5-turbo and for embeddings, I used the OpenAI text-embedding-ada-002 model. For software, I used LangChain 0.0.239, ChromaDB 0.4.2, and Python 3.11.\n\n\n\nAll tests were run 5 times consecutively. All runs started with a new vector DB collection or index created from historical conversations. There was a short cooling-off period between runs.\n\n\n\nNext Steps", "start_char_idx": 4610, "end_char_idx": 8554, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"38253cc8-12dd-4b35-8f2a-b9f2e9c40c69": {"node_ids": ["41f7e5a5-5ced-458c-8ef5-498839cafa82"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_7-24-release-notes_.txt", "file_name": "blog.langchain.dev_7-24-release-notes_.txt", "file_type": "text/plain", "file_size": 2719, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "66db197a-f5f5-4026-96c4-a651c6c1ca97": {"node_ids": ["36bf1abd-a5cd-4473-8ef3-63a224053636"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_about_.txt", "file_name": "blog.langchain.dev_about_.txt", "file_type": "text/plain", "file_size": 426, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "4f2b6468-fca3-4e90-a794-75fbd0ccfa6f": {"node_ids": ["e67d7290-6bf9-49e2-8451-74d2ca337e1c", "b72d55ca-44aa-4650-914f-3ec67ad30e38", "fe3f1d63-9531-4c4f-967a-fb91d26d77e5"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_announcing-langsmith_.txt", "file_name": "blog.langchain.dev_announcing-langsmith_.txt", "file_type": "text/plain", "file_size": 11758, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "a802dfb1-a085-4e15-84e5-a84ddae7da05": {"node_ids": ["a2aa4a30-0e28-411e-9ccb-e5fc09bfde24", "020a0fe4-9d3a-4b2a-a15b-2470686c646e", "1e2febac-0ca8-409e-aa17-26a274a3716a"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_announcing-langsmith_?ref=commandbar.ghost.io.txt", "file_name": "blog.langchain.dev_announcing-langsmith_?ref=commandbar.ghost.io.txt", "file_type": "text/plain", "file_size": 11782, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "ff022234-d71d-46ad-956b-0a001e38d97a": {"node_ids": ["9833fb8d-412e-4fa5-9c5f-9416401c8c55"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_automating-web-research_.txt", "file_name": "blog.langchain.dev_automating-web-research_.txt", "file_type": "text/plain", "file_size": 4728, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "6ef09ca3-d0d7-4d56-904b-77675ada0e6c": {"node_ids": ["876129a9-00b7-489b-9b74-e4d49a211895", "fb50f1df-d98b-4a8c-8c47-f427c196cc65", "4ac2c396-47c3-482f-8ef5-a38859334be4", "02921ceb-9fc4-4d70-8ccb-a0c9566b6a05"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt", "file_name": "blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt", "file_type": "text/plain", "file_size": 18365, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "e34fe21a-43b6-4938-a0da-94ce8e6fe887": {"node_ids": ["103355cf-a015-4bdd-94d3-aa7bb6572b16", "ebaf87ff-868a-4436-a1ae-0c3f6f586cc2", "52d09477-5764-4389-a48b-7de925680984"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_building-chat-langchain-2_.txt", "file_name": "blog.langchain.dev_building-chat-langchain-2_.txt", "file_type": "text/plain", "file_size": 11423, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "7c5eec50-2964-4601-ad30-745f2dadf028": {"node_ids": ["e22931d7-ca68-493d-9d64-d7513b071a20", "dbcb7956-3782-4047-8b85-912f13830ed6"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt", "file_name": "blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt", "file_type": "text/plain", "file_size": 4707, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "7385d65d-a81a-4ee3-ab0d-801ee406e901": {"node_ids": ["790bf101-ca54-43a5-a74e-797c33ad0195", "25595cc1-a897-4c2d-aaee-10dd5090f624", "eef8d306-c1d5-49e2-9b6a-301798ea4063", "1687731c-baa4-4e78-a2ea-03fe2b4a854d", "a60f2398-83d0-4654-92fe-d209101d02f3", "e80e809a-89a9-49c5-8db6-ecaf3c77894e"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_name": "blog.langchain.dev_chat-with-your-data-using-openai-pinecone-airbyte-langchain_.txt", "file_type": "text/plain", "file_size": 24234, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "d3c395ca-e222-4561-ab7a-274b7cb5d76d": {"node_ids": ["46a6286b-8c49-4d46-856c-ecb89c16bb59", "892ca795-9525-4782-b037-3c622852c571"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_code-interpreter-api_.txt", "file_name": "blog.langchain.dev_code-interpreter-api_.txt", "file_type": "text/plain", "file_size": 6414, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "35d71bcd-d63b-4bd6-9b22-72c0dd3093ae": {"node_ids": ["db911fa6-8dbe-44cf-804f-f102990d879a", "b1779f68-717f-429c-8002-e0586889db7b"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_conversational-retrieval-agents_.txt", "file_name": "blog.langchain.dev_conversational-retrieval-agents_.txt", "file_type": "text/plain", "file_size": 5988, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "f66c3f8b-0a4d-4cfd-a4b8-f323ab565cec": {"node_ids": ["3a79e45b-7652-45a5-b905-fcc898844cbf", "00acb849-90b0-4d58-bc95-4d544b6fc949", "a936117a-b9af-45d5-b7a4-17ac50e9d0fb", "f3c4c837-50cc-4896-9123-106298446286"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_eden-ai-x-langchain_.txt", "file_name": "blog.langchain.dev_eden-ai-x-langchain_.txt", "file_type": "text/plain", "file_size": 14536, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "6c9c9fbe-b13d-40e4-b964-83cc4cdffdbe": {"node_ids": ["ddc4174b-3544-4928-a3b1-8882fa9e9cfd", "eed170e3-014d-46e1-9cb2-1150e23634b3"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_espilla-x-langchain-retrieval-augmented-generation-rag-in-llm-powered-question-answering-pipelines_.txt", "file_name": "blog.langchain.dev_espilla-x-langchain-retrieval-augmented-generation-rag-in-llm-powered-question-answering-pipelines_.txt", "file_type": "text/plain", "file_size": 5719, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "658b3e2b-bd12-4119-9520-c8386c71a9c9": {"node_ids": ["83a93cdf-15ed-4c10-b34d-7df7b67b373a", "1e19efa8-f10c-4f24-9936-3988ecbd728f", "67c27eb4-00b7-40ab-9b44-65b72798cca1", "77883c74-6a43-4770-99ad-15367ee48e74"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt", "file_name": "blog.langchain.dev_evaluating-rag-pipelines-with-ragas-langsmith_.txt", "file_type": "text/plain", "file_size": 15613, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "9e70d970-f163-4450-a05b-569fd6cda981": {"node_ids": ["b31ec050-c902-491c-9258-6bb677d5271a", "fb621787-34b2-4d44-9b43-40ae4d51ef6f", "2a37b826-09f0-42ce-885b-bd38d77ff3e3", "e5daa82f-1059-48b0-b69e-5474ea40a678"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_exploring-genworlds_.txt", "file_name": "blog.langchain.dev_exploring-genworlds_.txt", "file_type": "text/plain", "file_size": 18004, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "0352b9e5-6c5f-4e76-8f61-8bed337644a8": {"node_ids": ["a977913b-9ed7-4a01-b05a-04cd810a45ab", "5f4e4fd6-4fc1-43b9-9c97-2ab0690244c0"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_fine-tune-your-llms-with-langsmith-and-lilac_.txt", "file_name": "blog.langchain.dev_fine-tune-your-llms-with-langsmith-and-lilac_.txt", "file_type": "text/plain", "file_size": 7600, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "f4024f7c-c873-4d85-9473-f4a3020695b4": {"node_ids": ["12a34e79-6b58-4d1a-88ba-b9366185df8d"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_goodbye-cves-hello-langchain_experimental_.txt", "file_name": "blog.langchain.dev_goodbye-cves-hello-langchain_experimental_.txt", "file_type": "text/plain", "file_size": 3141, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "529319ff-9d54-42a7-8896-5b102213bf49": {"node_ids": ["e6f4ae6f-a0f7-4fed-b655-6fc05df310f0"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_gpt-researcher-x-langchain_.txt", "file_name": "blog.langchain.dev_gpt-researcher-x-langchain_.txt", "file_type": "text/plain", "file_size": 3556, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "1da7cd2f-71b1-4b5c-be5c-7494f7c964d6": {"node_ids": ["e964de9c-1efd-430b-a3f1-6217c7ce2962", "39523747-413c-449d-bd8d-ad6ccf5f30c5", "62fd2cc8-8280-4ce1-9c3b-35523d3250ac"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_how-correct-are-llm-evaluators_.txt", "file_name": "blog.langchain.dev_how-correct-are-llm-evaluators_.txt", "file_type": "text/plain", "file_size": 8844, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "6cc5d296-aa67-4c8e-849d-31c835174760": {"node_ids": ["19b3d750-66b7-41ef-aa1e-1fcc6d1384e7", "bf154297-c15d-4b70-ad17-950be1b71696", "3aae7b83-4bc0-426c-9a88-dad4e183565a"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel_.txt", "file_name": "blog.langchain.dev_how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel_.txt", "file_type": "text/plain", "file_size": 10264, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "591d6c26-7b4a-46cf-a55e-e0a3853009b0": {"node_ids": ["6bbe09c1-6aff-4f7a-af38-59c0a67c3b69", "620e52d2-d58f-4b9b-80dc-3cf4a505692f"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_incorporating-domain-specific-knowledge-in-sql-llm-solutions_.txt", "file_name": "blog.langchain.dev_incorporating-domain-specific-knowledge-in-sql-llm-solutions_.txt", "file_type": "text/plain", "file_size": 8916, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "90b3f1ff-bfc5-4c52-aecf-6719d5d37e5d": {"node_ids": ["ad43e8ca-78d4-4915-87e2-1684d61c49b9"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_integrating-chatgpt-with-google-drive-and-notion-data_.txt", "file_name": "blog.langchain.dev_integrating-chatgpt-with-google-drive-and-notion-data_.txt", "file_type": "text/plain", "file_size": 4325, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "f270085a-4b2d-4cd3-a38d-94e567ba30ba": {"node_ids": ["2223a2d0-86e0-41ed-b897-2b41bb92c1b7", "68ac249e-80c8-451f-84b4-56b62e56769e"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_introducing-airbyte-sources-within-langchain_.txt", "file_name": "blog.langchain.dev_introducing-airbyte-sources-within-langchain_.txt", "file_type": "text/plain", "file_size": 6451, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "33f29cd2-df22-4a07-964c-35e7dffb43bd": {"node_ids": ["6049579a-0074-42a5-908f-ed134ceb3e0d"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-and-scrimba-partner-to-help-web-devs-become-ai-engineers_.txt", "file_name": "blog.langchain.dev_langchain-and-scrimba-partner-to-help-web-devs-become-ai-engineers_.txt", "file_type": "text/plain", "file_size": 2955, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "64bed31e-7c59-4b52-b7c9-3d8eb6171e69": {"node_ids": ["5ce87ded-fe42-44cf-a315-2559355c3dfb", "ab4be978-4237-47a1-b9c9-a53015888493", "23dbfd41-80ec-47ea-9de7-a08155e311b8"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-demogpt-new-era-for-gen-ai-applications_.txt", "file_name": "blog.langchain.dev_langchain-demogpt-new-era-for-gen-ai-applications_.txt", "file_type": "text/plain", "file_size": 10318, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "6b30c2db-168b-4d8b-b7ea-82a8f4f27eca": {"node_ids": ["b0e4086e-400f-4e65-8532-c7fd7353cd8f"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-docugami-webinar-lessons-from-deploying-llms-with-langsmith_.txt", "file_name": "blog.langchain.dev_langchain-docugami-webinar-lessons-from-deploying-llms-with-langsmith_.txt", "file_type": "text/plain", "file_size": 4262, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "d013c5ef-a363-4978-8dbe-b07fe972bf3f": {"node_ids": ["c48ccc50-dfc0-4e9e-b942-debce010f5a3", "ccb20e4d-511e-4573-9e0c-f87f3c45c42d"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-expression-language_.txt", "file_name": "blog.langchain.dev_langchain-expression-language_.txt", "file_type": "text/plain", "file_size": 6988, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "768dec39-fe48-4c1d-a18a-e7f054e32739": {"node_ids": ["822d6083-6ad0-4204-a82d-f62393a59aa1", "cc748c2e-0c29-4b64-b923-0f81adad1429"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_langchain-prompt-hub_.txt", "file_name": "blog.langchain.dev_langchain-prompt-hub_.txt", "file_type": "text/plain", "file_size": 9266, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "2473d46c-0ccc-4fc6-ae14-ea0a15e51b6b": {"node_ids": ["d279576f-f8a2-4c88-a2c4-5cc635028747", "fab507da-6367-4727-906c-c0f3c6588536"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_lepton-x-langchain-earning-sage_.txt", "file_name": "blog.langchain.dev_lepton-x-langchain-earning-sage_.txt", "file_type": "text/plain", "file_size": 6197, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "65409667-83da-4b4c-8725-3c8393996070": {"node_ids": ["60466d64-81fb-46a4-a9f9-105ee1f4c9da", "649a9591-69f7-49da-96cc-d5db61df64e4", "c5364800-f3bd-4e89-8e38-0333a123361e"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_llms-and-sql_.txt", "file_name": "blog.langchain.dev_llms-and-sql_.txt", "file_type": "text/plain", "file_size": 13010, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "1a43a6eb-1ccb-4acc-bc84-6e65487ec16a": {"node_ids": ["35df53c4-4d0e-4233-9fe7-93a81839b6d6", "c20ae551-1854-4eb4-821b-e2bd868bcb0d"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_llms-to-improve-documentation_.txt", "file_name": "blog.langchain.dev_llms-to-improve-documentation_.txt", "file_type": "text/plain", "file_size": 4689, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "2f9fa58d-29eb-4248-b6cb-52465ee67e91": {"node_ids": ["8c570dc5-7ecf-40a6-99ec-219181965149"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_making-data-ingestion-production-ready-a-langchain-powered-airbyte-destination_.txt", "file_name": "blog.langchain.dev_making-data-ingestion-production-ready-a-langchain-powered-airbyte-destination_.txt", "file_type": "text/plain", "file_size": 4144, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "93cdd84e-6b13-4405-adcc-134add3cbe11": {"node_ids": ["c48cc975-3ac0-4be5-83e4-c519dab13668"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_multion-x-langchain-powering-next-gen-web-automation-navigation-with-ai_.txt", "file_name": "blog.langchain.dev_multion-x-langchain-powering-next-gen-web-automation-navigation-with-ai_.txt", "file_type": "text/plain", "file_size": 3739, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "a2a18fb6-3415-4745-9f4b-b263304d32d4": {"node_ids": ["70ce7632-c91d-4431-a3f5-68397739189e", "4f40aae8-2ca5-4c8b-8e30-1eb25c595274", "b60a9dcf-19b3-4df1-ad5b-228c50b8032c"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_neo4j-x-langchain-new-vector-index_.txt", "file_name": "blog.langchain.dev_neo4j-x-langchain-new-vector-index_.txt", "file_type": "text/plain", "file_size": 9850, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "1eee1785-8b22-4586-bd15-a49a1e1c88cb": {"node_ids": ["697fc1ef-e443-472a-9121-97a2a620f624", "3238c40c-e598-4c35-b4bc-a5b780a73d63"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_neum-x-langchain_.txt", "file_name": "blog.langchain.dev_neum-x-langchain_.txt", "file_type": "text/plain", "file_size": 6284, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "fee773e4-ee3a-4447-bc94-b604cf9231b9": {"node_ids": ["eee5711e-eb8c-4895-a2a3-31315a11ae0f", "af026887-3667-4b25-a09b-a89a11e5ca0a"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_opaqueprompts-x-langchain-enhance-the-privacy-of-your-langchain-application-with-just-one-code-change_.txt", "file_name": "blog.langchain.dev_opaqueprompts-x-langchain-enhance-the-privacy-of-your-langchain-application-with-just-one-code-change_.txt", "file_type": "text/plain", "file_size": 7310, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "295d2839-53d5-44dc-a40b-4fdcff61c08d": {"node_ids": ["5bdf1930-b446-42d2-b806-4396d68a5c2c", "551b68f8-c29f-4abf-9b6e-ab446c536438", "8de519fa-e912-479f-936d-1e0df2cad1f9"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt", "file_name": "blog.langchain.dev_peering-into-the-soul-of-ai-decision-making-with-langsmith_.txt", "file_type": "text/plain", "file_size": 14087, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "05861ae7-c95f-4e12-aa50-ec6f2863cfa7": {"node_ids": ["d69a9195-cfb0-4605-8d1c-4cb506908f46"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_realchar-x-langsmith-ai-companions_.txt", "file_name": "blog.langchain.dev_realchar-x-langsmith-ai-companions_.txt", "file_type": "text/plain", "file_size": 3131, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "d4dbac9a-42a7-4e29-8602-358df8e42956": {"node_ids": ["21a243d7-fae5-49e5-8236-c36ad36971b5"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_streamlit-llm-hackathon-kickoff-and-projects-wed-love-to-see-2_.txt", "file_name": "blog.langchain.dev_streamlit-llm-hackathon-kickoff-and-projects-wed-love-to-see-2_.txt", "file_type": "text/plain", "file_size": 3128, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "1b35a746-2334-4af3-b700-9e7451d7e037": {"node_ids": ["4060ea8c-9755-4421-9c1c-e6e2f13ba1a3"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_student-hacker-in-residence-fall-23_.txt", "file_name": "blog.langchain.dev_student-hacker-in-residence-fall-23_.txt", "file_type": "text/plain", "file_size": 2237, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "c4650e72-2e50-44fa-8278-a4e922d08f99": {"node_ids": ["e7c31deb-e039-4a95-a056-61f085f667ae", "ab5ee918-c0b4-4cf7-8b11-d92b6c737281", "433da125-97f8-4ad6-9806-4592a1ae5936"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model_.txt", "file_name": "blog.langchain.dev_summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model_.txt", "file_type": "text/plain", "file_size": 10301, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "69c3976a-7737-4d6e-a73b-2ee14795a4d6": {"node_ids": ["93d5cda4-eb80-49f3-b3a3-7b709e8ad9cb", "ddd9c354-aaee-4172-85d5-49d8297500bc"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_syncing-data-sources-to-vector-stores_.txt", "file_name": "blog.langchain.dev_syncing-data-sources-to-vector-stores_.txt", "file_type": "text/plain", "file_size": 6318, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "bb70198e-5d74-49ba-9662-c7cb492efe26": {"node_ids": ["03ffeae0-4aac-4f98-b2dc-68d972f3bb7d"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_ted-ai-hackathon-kickoff_.txt", "file_name": "blog.langchain.dev_ted-ai-hackathon-kickoff_.txt", "file_type": "text/plain", "file_size": 2495, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "036cc20e-6afc-45ea-95aa-e2dc7a370631": {"node_ids": ["c6fcd670-40e9-49de-bc71-e4da4c99b2b1", "6b83140e-8002-480d-9201-fc1432699b27", "5f832ed9-9715-4e6c-a976-836c387b101a", "e64faa6d-6cfe-4fab-8fdb-7661991005b3", "d88b7964-bcd2-4799-879a-d5def701f526", "2bdf1086-5b5e-487f-af65-985cd564a3d8", "7a588886-6fd7-47b7-9c4f-bed2282189b8"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_name": "blog.langchain.dev_timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications_.txt", "file_type": "text/plain", "file_size": 29160, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "ca0e92d5-47c1-4dc2-b585-920412c51542": {"node_ids": ["cb4b951c-f4ea-4d8e-b3e4-0a3ca0e988db", "b430f89c-e810-4466-b7e9-8dfaf4cc8064"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_titantakeoff-x-langchain-supercharged-local-inference-for-llms-2_.txt", "file_name": "blog.langchain.dev_titantakeoff-x-langchain-supercharged-local-inference-for-llms-2_.txt", "file_type": "text/plain", "file_size": 4865, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "aff38af1-2b57-4a15-9ad4-916f9a964e9e": {"node_ids": ["1876db37-d180-46c0-8bc7-4397949ae4bd", "dae2bd5e-f99f-4fac-9202-d719e49a56f5", "0a9ef591-d1a1-492c-8fc5-36cc23356bce", "8075beab-b865-454f-b9ec-9d7e69c7757d"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_using-langsmith-to-support-fine-tuning-of-open-source-llms_.txt", "file_name": "blog.langchain.dev_using-langsmith-to-support-fine-tuning-of-open-source-llms_.txt", "file_type": "text/plain", "file_size": 13638, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "170a97eb-81bd-449d-be31-fdb5631aef90": {"node_ids": ["f458fd93-2978-449c-a835-48d81edb0d57"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_villagers-x-langsmith-simulating-multi-agent-social-networks_.txt", "file_name": "blog.langchain.dev_villagers-x-langsmith-simulating-multi-agent-social-networks_.txt", "file_type": "text/plain", "file_size": 2412, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "aec0222c-6689-4c17-843a-09c2360240a6": {"node_ids": ["76b4e9d6-cbad-4dfe-b089-76a8af22d836"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_week-of-8-21-langchain-release-notes_.txt", "file_name": "blog.langchain.dev_week-of-8-21-langchain-release-notes_.txt", "file_type": "text/plain", "file_size": 2935, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "6f6dae7e-5f00-4deb-958f-07a241f3289b": {"node_ids": ["81bdae04-6371-4ce8-bd3f-5ef4d3ee23ea"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_week-of-8-7_.txt", "file_name": "blog.langchain.dev_week-of-8-7_.txt", "file_type": "text/plain", "file_size": 3454, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "822deb9f-11f3-49cc-9315-71846cfc6731": {"node_ids": ["33fe22bc-688d-46a6-9ca5-d9f97976c601", "320fcc8a-c626-4a66-927a-b524902e31dc"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_week-of-9-18-langchain-release-notes_.txt", "file_name": "blog.langchain.dev_week-of-9-18-langchain-release-notes_.txt", "file_type": "text/plain", "file_size": 6125, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "04fb9861-1f52-4be8-ada0-e4da2c5dd23c": {"node_ids": ["d8977978-ee4d-4897-bebb-3eebf6a37e6f", "6726c641-6646-40a8-9de2-594a14a1f26c"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_xata-x-langchain-new-vector-store-and-memory-store-integrations_.txt", "file_name": "blog.langchain.dev_xata-x-langchain-new-vector-store-and-memory-store-integrations_.txt", "file_type": "text/plain", "file_size": 8357, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}, "8e97222e-ee39-45c2-b060-6e5ca5995f56": {"node_ids": ["bf29aa66-8795-4f15-974b-7bd09d40d29c", "49b17c5e-9050-4dc1-9ca5-bd1e1ebded20"], "metadata": {"file_path": "data/langchain_blog_posts/blog.langchain.dev_zep-x-langchain-slow-chatbots_.txt", "file_name": "blog.langchain.dev_zep-x-langchain-slow-chatbots_.txt", "file_type": "text/plain", "file_size": 8618, "creation_date": "2024-01-08", "last_modified_date": "2023-09-29", "last_accessed_date": "2024-01-08"}}}}
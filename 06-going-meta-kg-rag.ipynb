{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How KG improves RAG ?\n",
    "- Making the retrieval `structure-aware`\n",
    "- Enabling `context augmentation`\n",
    "- Enabling `fine-grained-access-control`\n",
    "- Combining `vector + graph search`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, logging, sys, os\n",
    "from pyvis.network import Network\n",
    "from IPython.display import display\n",
    "from llama_index.llms import AzureOpenAI\n",
    "from llama_index.llm_predictor import LLMPredictor\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index.graph_stores import Neo4jGraphStore\n",
    "from llama_index.vector_stores import Neo4jVectorStore\n",
    "from llama_index.text_splitter import TokenTextSplitter\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index.query_engine import KnowledgeGraphQueryEngine\n",
    "\n",
    "from llama_index import (\n",
    "                        StorageContext,\n",
    "                        VectorStoreIndex,\n",
    "                        KnowledgeGraphIndex,\n",
    "                        SimpleDirectoryReader, \n",
    "                        load_graph_from_storage,\n",
    "                        load_index_from_storage,\n",
    "                        ServiceContext,\n",
    "                        PromptHelper\n",
    "                        )\n",
    "\n",
    "logging.basicConfig(\n",
    "                    stream=sys.stdout, \n",
    "                    level=logging.INFO\n",
    "                    )\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cadentials.yaml') as f:\n",
    "    credentials = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_llm = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "llm=AzureOpenAI(\n",
    "                model=credentials['AZURE_ENGINE'],\n",
    "                api_key=credentials['AZURE_OPENAI_API_KEY'],\n",
    "                deployment_name=credentials['AZURE_DEPLOYMENT_ID'],\n",
    "                api_version=credentials['AZURE_OPENAI_API_VERSION'],\n",
    "                azure_endpoint=credentials['AZURE_OPENAI_API_BASE']\n",
    "                )\n",
    "chat_llm = LLMPredictor(llm)\n",
    "\n",
    "prompt_helper = PromptHelper(\n",
    "                            num_output=256,\n",
    "                            context_window=4096,\n",
    "                            chunk_overlap_ratio=0.1,\n",
    "                            chunk_size_limit=None\n",
    "                            )\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "                                separator=\" \",\n",
    "                                chunk_size=1024,\n",
    "                                chunk_overlap=20,\n",
    "                                backup_separators=[\"\\n\"]\n",
    "                                )\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "                                                text_splitter=text_splitter,\n",
    "                                                prompt_helper=prompt_helper,\n",
    "                                                embed_model=embedding_llm,\n",
    "                                                llm_predictor=chat_llm\n",
    "                                                )\n",
    "\n",
    "set_global_service_context(service_context)\n",
    "\n",
    "neo4j_db = Neo4jVectorStore(\n",
    "                            credentials['NEO4J_USERNAME'], \n",
    "                            credentials['NEO4J_PASSWORD'], \n",
    "                            credentials['NEO4J_URI'], \n",
    "                            384\n",
    "                            )\n",
    "\n",
    "neo4j_store = Neo4jGraphStore(\n",
    "                            username=credentials['NEO4J_USERNAME'],\n",
    "                            password=credentials['NEO4J_PASSWORD'],\n",
    "                            url=credentials['NEO4J_URI'],\n",
    "                            database='neo4j',\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"./data/new_papers\").load_data()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 01 : Graph Vector Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./db/06/method-01/vector/'):\n",
    "    vec_index = VectorStoreIndex.from_documents(\n",
    "                                                documents,\n",
    "                                                service_context = service_context\n",
    "                                                )\n",
    "    vec_index.storage_context.persist(persist_dir='./db/06/method-01/vector/')\n",
    "    print(\"Saving Vector Index ...\")\n",
    "else:\n",
    "    storage_context_vector = StorageContext.from_defaults(persist_dir='./db/06/method-01/vector/')\n",
    "    vec_index = load_index_from_storage(\n",
    "                                        storage_context=storage_context_vector)\n",
    "    print(\"Loading Vector Index ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./db/06/method-01/graph/'):\n",
    "    storage_context_graph = StorageContext.from_defaults(graph_store=neo4j_db)\n",
    "    \n",
    "    graph_index = VectorStoreIndex.from_documents(\n",
    "                                                documents, \n",
    "                                                storage_context=storage_context_graph\n",
    "                                                )\n",
    "    \n",
    "    graph_index.storage_context.persist(persist_dir='./db/06/method-01/graph/')\n",
    "    print(\"Saving Graph Index ...\")\n",
    "else:\n",
    "    storage_context_graph = StorageContext.from_defaults(\n",
    "                                                        graph_store=neo4j_db,\n",
    "                                                        persist_dir='./db/06/method-01/graph/'\n",
    "                                                        )\n",
    "    graph_index = load_index_from_storage(storage_context=storage_context_graph)\n",
    "    print(\"Loading Graph Index ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_vector = vec_index.as_query_engine()\n",
    "query_engine_graph = graph_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is ToolFormer ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_vector = str(query_engine_vector.query(query))\n",
    "response_graph = str(query_engine_graph.query(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"vector db response : {}\".format(response_vector))\n",
    "print(\"graph db response : {}\".format(response_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 02 : Text2Cypher (Knowledge Graph Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./db/06/method-02/'):\n",
    "    storage_context = StorageContext.from_defaults(graph_store=neo4j_store)\n",
    "    kg_index = KnowledgeGraphIndex.from_documents( \n",
    "                                            # tags=tags,\n",
    "                                            documents=documents,\n",
    "                                            max_triplets_per_chunk=10,\n",
    "                                            service_context=service_context,\n",
    "                                            storage_context=storage_context,\n",
    "                                            # space_name=space_name,\n",
    "                                            # edge_types=edge_types,\n",
    "                                            # rel_prop_names=rel_prop_names,\n",
    "                                            include_embeddings=True,\n",
    "                                            verbose=True\n",
    "                                            )\n",
    "    \n",
    "    kg_index.storage_context.persist(persist_dir='./db/06/method-02/')\n",
    "\n",
    "else:\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "                                                    graph_store=neo4j_store,\n",
    "                                                    persist_dir='./db/06/method-02/'\n",
    "                                                    )\n",
    "    kg_index = load_graph_from_storage(                                            # tags=tags,\n",
    "                                        documents=documents,\n",
    "                                        max_triplets_per_chunk=10,\n",
    "                                        service_context=service_context,\n",
    "                                        storage_context=storage_context,\n",
    "                                        # space_name=space_name,\n",
    "                                        # edge_types=edge_types,\n",
    "                                        # rel_prop_names=rel_prop_names,\n",
    "                                        include_embeddings=True,\n",
    "                                        verbose=True\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl2kg_query_engine = KnowledgeGraphQueryEngine(\n",
    "                                            storage_context=storage_context,\n",
    "                                            service_context=service_context,\n",
    "                                            llm=chat_llm,\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ngql SHOW HOSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, logging, sys, os\n",
    "from pyvis.network import Network\n",
    "from IPython.display import display\n",
    "from llama_index.llms import AzureOpenAI\n",
    "from llama_index.llm_predictor import LLMPredictor\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index.vector_stores import Neo4jVectorStore\n",
    "from llama_index.text_splitter import TokenTextSplitter\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index import (\n",
    "                        StorageContext,\n",
    "                        VectorStoreIndex,\n",
    "                        SimpleDirectoryReader, \n",
    "                        load_graph_from_storage,\n",
    "                        load_index_from_storage,\n",
    "                        ServiceContext,\n",
    "                        PromptHelper\n",
    "                        )\n",
    "\n",
    "logging.basicConfig(\n",
    "                    stream=sys.stdout, \n",
    "                    level=logging.INFO\n",
    "                    )\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cadentials.yaml') as f:\n",
    "    credentials = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/1zuu/miniforge3/envs/llamaindex/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMPredictor is deprecated, please use LLM instead.\n"
     ]
    }
   ],
   "source": [
    "embedding_llm = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "llm=AzureOpenAI(\n",
    "                model=credentials['AZURE_ENGINE'],\n",
    "                api_key=credentials['AZURE_OPENAI_API_KEY'],\n",
    "                deployment_name=credentials['AZURE_DEPLOYMENT_ID'],\n",
    "                api_version=credentials['AZURE_OPENAI_API_VERSION'],\n",
    "                azure_endpoint=credentials['AZURE_OPENAI_API_BASE']\n",
    "                )\n",
    "chat_llm = LLMPredictor(llm)\n",
    "\n",
    "prompt_helper = PromptHelper(\n",
    "                            num_output=256,\n",
    "                            context_window=4096,\n",
    "                            chunk_overlap_ratio=0.1,\n",
    "                            chunk_size_limit=None\n",
    "                            )\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "                                separator=\" \",\n",
    "                                chunk_size=1024,\n",
    "                                chunk_overlap=20,\n",
    "                                backup_separators=[\"\\n\"]\n",
    "                                )\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "                                                text_splitter=text_splitter,\n",
    "                                                prompt_helper=prompt_helper,\n",
    "                                                embed_model=embedding_llm,\n",
    "                                                llm_predictor=chat_llm\n",
    "                                                )\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Store Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "documents = SimpleDirectoryReader(\"./data/langchain_blog_posts\").load_data()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('./db/04/vector/'):\n",
    "    vec_index = VectorStoreIndex.from_documents(\n",
    "                                                documents,\n",
    "                                                service_context = service_context\n",
    "                                                )\n",
    "    vec_index.storage_context.persist(persist_dir='./db/04/vector/')\n",
    "else:\n",
    "    storage_context_vector = StorageContext.from_defaults(persist_dir='./db/04/vector/')\n",
    "    vec_index = load_index_from_storage(storage_context=storage_context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "neo4j_vector = Neo4jVectorStore(\n",
    "                                credentials['NEO4J_USERNAME'], \n",
    "                                credentials['NEO4J_PASSWORD'], \n",
    "                                credentials['NEO4J_URI'], \n",
    "                                384\n",
    "                                )\n",
    "\n",
    "if not os.path.exists('./db/04/graph/'):\n",
    "    storage_context_graph = StorageContext.from_defaults( graph_store=neo4j_vector)\n",
    "    \n",
    "    graph_index = VectorStoreIndex.from_documents(\n",
    "                                                documents, \n",
    "                                                storage_context=storage_context_graph\n",
    "                                                )\n",
    "    \n",
    "    graph_index.storage_context.persist(persist_dir='./db/04/graph/')\n",
    "    \n",
    "else:\n",
    "    storage_context_graph = StorageContext.from_defaults(\n",
    "                                                        graph_store=neo4j_vector,\n",
    "                                                        persist_dir='./db/04/graph/'\n",
    "                                                        )\n",
    "    graph_index = load_index_from_storage(storage_context=storage_context_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_vector = vec_index.as_query_engine()\n",
    "query_engine_graph = graph_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the difference between langchain and langsmith ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://doc-ocr-adl.openai.azure.com//openai/deployments/chat_model/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://doc-ocr-adl.openai.azure.com//openai/deployments/chat_model/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://doc-ocr-adl.openai.azure.com//openai/deployments/chat_model/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://doc-ocr-adl.openai.azure.com//openai/deployments/chat_model/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "response_vector = str(query_engine_vector.query(query))\n",
    "response_graph = query_engine_graph.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"vector db response : {}\".format())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
